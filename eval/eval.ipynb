{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text cleaning and similarity workflow\n",
    "# - Load `apt.txt`, `paper2code.txt`, and `protopy.txt`\n",
    "# - Strip whitespace/punctuation noise and normalize casing\n",
    "# - Tokenize cleaned strings and summarize token counts\n",
    "# - Compute cosine similarity across the token-count vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/eval\")\n",
    "SOURCE_FILES: Dict[str, Path] = {\n",
    "    \"apt\": BASE_DIR / \"apt.txt\",\n",
    "    \"paper2code\": BASE_DIR / \"paper2code.txt\",\n",
    "    \"protopy\": BASE_DIR / \"protopy.txt\",\n",
    "}\n",
    "\n",
    "_non_word_pattern = re.compile(r\"[^\\w]+\", re.UNICODE)\n",
    "_multi_space_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "def load_and_clean(path: Path) -> str:\n",
    "    \"\"\"Load text, lower-case it, drop punctuation/newlines, and collapse spaces.\"\"\"\n",
    "    raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    lowered = raw.lower()\n",
    "    # Replace any non-alphanumeric/underscore stretches with a single space\n",
    "    alnum_only = _non_word_pattern.sub(\" \", lowered)\n",
    "    collapsed = _multi_space_pattern.sub(\" \", alnum_only).strip()\n",
    "    return collapsed\n",
    "\n",
    "def tokenize(clean_text: str) -> List[str]:\n",
    "    \"\"\"Tokenize on single spaces (already normalized).\"\"\"\n",
    "    if not clean_text:\n",
    "        return []\n",
    "    return clean_text.split(\" \")\n",
    "\n",
    "def counter_to_vector(counter: Counter, vocab: List[str]) -> np.ndarray:\n",
    "    return np.array([counter.get(tok, 0) for tok in vocab], dtype=float)\n",
    "\n",
    "def cosine_similarity_matrix(vectors: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0  # avoid division by zero for empty docs\n",
    "    normalized = vectors / norms\n",
    "    return normalized @ normalized.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a974f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = {name: load_and_clean(path) for name, path in SOURCE_FILES.items()}\n",
    "\n",
    "cleaning_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"file\": name,\n",
    "        \"characters_after_cleaning\": len(text),\n",
    "        \"preview\": text[:140] + (\"â€¦\" if len(text) > 140 else \"\")\n",
    "    }\n",
    "    for name, text in cleaned_texts.items()\n",
    "])\n",
    "\n",
    "cleaning_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = {name: tokenize(text) for name, text in cleaned_texts.items()}\n",
    "token_counters = {name: Counter(tokens) for name, tokens in tokenized_texts.items()}\n",
    "\n",
    "token_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"file\": name,\n",
    "        \"token_count\": len(tokenized_texts[name]),\n",
    "        \"unique_tokens\": len(token_counters[name]),\n",
    "        \"sample_tokens\": tokenized_texts[name][:15],\n",
    "    }\n",
    "    for name in tokenized_texts\n",
    "])\n",
    "\n",
    "token_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted({token for tokens in tokenized_texts.values() for token in tokens})\n",
    "if not vocab:\n",
    "    raise ValueError(\"No tokens found after cleaning; cannot compute similarity.\")\n",
    "\n",
    "vectors = np.vstack([\n",
    "    counter_to_vector(token_counters[name], vocab) for name in tokenized_texts\n",
    "])\n",
    "cosine_matrix = cosine_similarity_matrix(vectors)\n",
    "\n",
    "similarity_df = pd.DataFrame(\n",
    "    cosine_matrix,\n",
    "    index=list(tokenized_texts.keys()),\n",
    "    columns=list(tokenized_texts.keys()),\n",
    ")\n",
    "\n",
    "pairwise_targets = [(\"apt\", \"paper2code\"), (\"apt\", \"protopy\")]\n",
    "pairwise_similarity = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"pair\": f\"{a} vs {b}\",\n",
    "            \"cosine_similarity\": float(similarity_df.loc[a, b]),\n",
    "        }\n",
    "        for a, b in pairwise_targets\n",
    "    ]\n",
    ")\n",
    "\n",
    "pairwise_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "222ad896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mermaid as md\n",
    "from mermaid.graph import Graph\n",
    "\n",
    "render = md.Mermaid(\"\"\"\n",
    "---\n",
    "config:\n",
    "  theme: neutral\n",
    "  look: handDrawn\n",
    "  layout: dagre\n",
    "title: Paper-to-Code System Overview\n",
    "---\n",
    "flowchart TB\n",
    " subgraph SFT[\"SFT Dataset\"]\n",
    "        F(\"â¬…ï¸ Input\")\n",
    "        G(\"âž¡ï¸ Output\")\n",
    "  end\n",
    "    A(\"ðŸ“„ Arxiv Papers\") --> B(\"ðŸ§¾ Structured<br>Papers (JSON)\")\n",
    "    B -- ðŸ¤– DeepSeek --> C(\"ðŸ“ˆ Paper with Model Imrprovment\")\n",
    "    C -- ðŸ¤– DeepSeek --> D[\"ðŸ“‘ Training Template <br> (JSON)\"]\n",
    "    C -- ðŸ” RegEx --> E[\"ðŸ§© Model Definition <br> Code\"]\n",
    "    D --> F\n",
    "    E --> G\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36a396f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg id=\"mermaidInkSvg\" width=\"100%\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 2412 512\" style=\"max-width: 512px;\" role=\"graphics-document document\" aria-roledescription=\"error\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><style xmlns=\"http://www.w3.org/1999/xhtml\">@import url(\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css\");</style><style xmlns=\"http://www.w3.org/1999/xhtml\">#mermaidInkSvg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;fill:#333;}@keyframes edge-animation-frame{from{stroke-dashoffset:0;}}@keyframes dash{to{stroke-dashoffset:0;}}#mermaidInkSvg .edge-animation-slow{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 50s linear infinite;stroke-linecap:round;}#mermaidInkSvg .edge-animation-fast{stroke-dasharray:9,5!important;stroke-dashoffset:900;animation:dash 20s linear infinite;stroke-linecap:round;}#mermaidInkSvg .error-icon{fill:#552222;}#mermaidInkSvg .error-text{fill:#552222;stroke:#552222;}#mermaidInkSvg .edge-thickness-normal{stroke-width:1px;}#mermaidInkSvg .edge-thickness-thick{stroke-width:3.5px;}#mermaidInkSvg .edge-pattern-solid{stroke-dasharray:0;}#mermaidInkSvg .edge-thickness-invisible{stroke-width:0;fill:none;}#mermaidInkSvg .edge-pattern-dashed{stroke-dasharray:3;}#mermaidInkSvg .edge-pattern-dotted{stroke-dasharray:2;}#mermaidInkSvg .marker{fill:#333333;stroke:#333333;}#mermaidInkSvg .marker.cross{stroke:#333333;}#mermaidInkSvg svg{font-family:\"trebuchet ms\",verdana,arial,sans-serif;font-size:16px;}#mermaidInkSvg p{margin:0;}#mermaidInkSvg :root{--mermaid-font-family:\"trebuchet ms\",verdana,arial,sans-serif;}</style><g/><g><path class=\"error-icon\" d=\"m411.313,123.313c6.25-6.25 6.25-16.375 0-22.625s-16.375-6.25-22.625,0l-32,32-9.375,9.375-20.688-20.688c-12.484-12.5-32.766-12.5-45.25,0l-16,16c-1.261,1.261-2.304,2.648-3.31,4.051-21.739-8.561-45.324-13.426-70.065-13.426-105.867,0-192,86.133-192,192s86.133,192 192,192 192-86.133 192-192c0-24.741-4.864-48.327-13.426-70.065 1.402-1.007 2.79-2.049 4.051-3.31l16-16c12.5-12.492 12.5-32.758 0-45.25l-20.688-20.688 9.375-9.375 32.001-31.999zm-219.313,100.687c-52.938,0-96,43.063-96,96 0,8.836-7.164,16-16,16s-16-7.164-16-16c0-70.578 57.422-128 128-128 8.836,0 16,7.164 16,16s-7.164,16-16,16z\"/><path class=\"error-icon\" d=\"m459.02,148.98c-6.25-6.25-16.375-6.25-22.625,0s-6.25,16.375 0,22.625l16,16c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688 6.25-6.25 6.25-16.375 0-22.625l-16.001-16z\"/><path class=\"error-icon\" d=\"m340.395,75.605c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688 6.25-6.25 6.25-16.375 0-22.625l-16-16c-6.25-6.25-16.375-6.25-22.625,0s-6.25,16.375 0,22.625l15.999,16z\"/><path class=\"error-icon\" d=\"m400,64c8.844,0 16-7.164 16-16v-32c0-8.836-7.156-16-16-16-8.844,0-16,7.164-16,16v32c0,8.836 7.156,16 16,16z\"/><path class=\"error-icon\" d=\"m496,96.586h-32c-8.844,0-16,7.164-16,16 0,8.836 7.156,16 16,16h32c8.844,0 16-7.164 16-16 0-8.836-7.156-16-16-16z\"/><path class=\"error-icon\" d=\"m436.98,75.605c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688l32-32c6.25-6.25 6.25-16.375 0-22.625s-16.375-6.25-22.625,0l-32,32c-6.251,6.25-6.251,16.375-0.001,22.625z\"/><text class=\"error-text\" x=\"1440\" y=\"250\" font-size=\"150px\" style=\"text-anchor: middle;\">Syntax error in text</text><text class=\"error-text\" x=\"1250\" y=\"400\" font-size=\"100px\" style=\"text-anchor: middle;\">mermaid version 11.12.1</text></g></svg>"
      ],
      "text/plain": [
       "<mermaid.__main__.Mermaid at 0x1241cfc20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
