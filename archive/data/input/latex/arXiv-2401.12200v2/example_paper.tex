    %%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\PassOptionsToPackage{dvipsnames,table}{xcolor}
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% Add my packages starting from here

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{natbib}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{url}
\usepackage{amsmath,amsfonts,amssymb}

\usepackage{multirow}
% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{pifont}
\usepackage{placeins}
\usepackage[normalem]{ulem}

\makeatletter
\patchcmd{\BR@backref}{\newblock}{\newblock(page~}{}{}
\patchcmd{\BR@backref}{\par}{)\par}{}{}
\makeatother

\useunder{\uline}{\ul}{}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newcommand{{\ourmethod}}{APT}
\newcommand{{\ourarch}}{APT adapter}
% \newcommand{\ourarchabbr}{$\mathcal{E}\text{Adapter}$}
\newcommand{\ourarchabbr}{APT adapter}
\newcommand{\tableindent}{~~~~}
\newcommand{\lmabbr}{LM}
\newcommand{\pruningabbr}{SP}
% \newcommand\todo[1]{\textit{\textcolor{purple}{[TODO] #1}}}

\newcommand{\draftonly}[1]{#1} 
% Uncomment for submission
% \renewcommand{\draftonly}[1]{}
\newcommand{\draftcomment}[3]{\draftonly{{\textcolor{#3}{[\small \textbf{\textsc{#2}: #1}]}}}}
\newcommand{\qq}[1]{\draftcomment{#1}{QC}{teal}}
\newcommand{\bowen}[1]{\draftcomment{#1}{BW}{magenta}}
\newcommand{\hanna}[1]{\draftcomment{#1}{HH}{orange}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference}

\begin{document}

\twocolumn[
\icmltitle{APT: Adaptive Pruning and Tuning Pretrained Language Models for \\Efficient Training and Inference}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Bowen Zhao}{xxx}
\icmlauthor{Hannaneh Hajishirzi}{xxx,yyy}
\icmlauthor{Qingqing Cao$^*$}{zzz}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{xxx}{University of Washington}
\icmlaffiliation{yyy}{Allen Institute for Artificial Intelligence}
\icmlaffiliation{zzz}{$^*$Apple, work done at the University of Washington}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Bowen Zhao}{bowen98@uw.edu}
\icmlcorrespondingauthor{Qingqing Cao}{qicao@apple.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Fine-tuning and inference with large Language Models~({\lmabbr}) are generally known to be expensive. 
Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of {\lmabbr} parameters but does not improve inference efficiency. Structured pruning improves {\lmabbr} inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce {\ourmethod} that adaptively {\it prunes} and {\it tunes} parameters for the {\lmabbr}s. 
At the early stage of fine-tuning, {\ourmethod} dynamically adds {\it salient} tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency.
Compared to baselines, our experiments show that {\ourmethod} maintains up to 98\% task performance when pruning 60\% of the parameters in RoBERTa and T5 models. APT also preserves 86.4\% of LLaMA models' performance with 70\% parameters remaining. Furthermore, {\ourmethod} speeds up LMs' fine-tuning by up to 8$\times$ and reduces large {\lmabbr}s' memory training footprint by up to 70\%. Our code and models are publicly available at \url{https://github.com/ROIM1998/APT}.
\end{abstract}


\input{sections/introduction}
\input{sections/background}
\input{sections/method}
\input{sections/experiment}
\input{sections/conclusion}


% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
This research was supported partly by NSF IIS-2044660, an Allen Investigator Distinguished award. We thank the members of the UW NLP group for their comments and feedback on this paper.

\input{sections/impact_statement}


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{official_custom}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{sections/appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
