@inproceedings{houlsby2019parameter,
 author = {Neil Houlsby and
Andrei Giurgiu and
Stanislaw Jastrzebski and
Bruna Morrone and
Quentin de Laroussilhe and
Andrea Gesmundo and
Mona Attariyan and
Sylvain Gelly},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/HoulsbyGJMLGAG19.bib},
 booktitle = {Proceedings of the 36th International Conference on Machine Learning,
{ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
 editor = {Kamalika Chaudhuri and
Ruslan Salakhutdinov},
 pages = {2790--2799},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Tue, 11 Jun 2019 01:00:00 +0200},
 title = {Parameter-Efficient Transfer Learning for {NLP}},
 volume = {97},
 year = {2019}
}

@inproceedings{hu_lora_2021,
 author = {Edward J. Hu and
Yelong Shen and
Phillip Wallis and
Zeyuan Allen{-}Zhu and
Yuanzhi Li and
Shean Wang and
Lu Wang and
Weizhu Chen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {LoRA: Low-Rank Adaptation of Large Language Models},
 year = {2022}
}

@inproceedings{xia_structured_2022,
 address = {Dublin, Ireland},
 author = {Xia, Mengzhou  and
Zhong, Zexuan  and
Chen, Danqi},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.107},
 pages = {1513--1528},
 publisher = {Association for Computational Linguistics},
 title = {Structured Pruning Learns Compact and Accurate Models},
 year = {2022}
}

@inproceedings{he_towards_2022,
 author = {Junxian He and
Chunting Zhou and
Xuezhe Ma and
Taylor Berg{-}Kirkpatrick and
Graham Neubig},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HeZMBN22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Towards a Unified View of Parameter-Efficient Transfer Learning},
 year = {2022}
}

@inproceedings{kwon_fast_2022,
 author = {Kwon, Woosuk and Kim, Sehoon and Mahoney, Michael W and Hassoun, Joseph and Keutzer, Kurt and Gholami, Amir},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24101--24116},
 publisher = {Curran Associates, Inc.},
 title = {A Fast Post-Training Pruning Framework for Transformers},
 volume = {35},
 year = {2022}
}

@inproceedings{lagunas_block_2021,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Lagunas, Fran{\c{c}}ois  and
Charlaix, Ella  and
Sanh, Victor  and
Rush, Alexander},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.829},
 pages = {10619--10629},
 publisher = {Association for Computational Linguistics},
 title = {Block Pruning For Faster Transformers},
 year = {2021}
}

@inproceedings{sanh_movement_2020,
 author = {Victor Sanh and
Thomas Wolf and
Alexander M. Rush},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/Sanh0R20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
 year = {2020}
}

@misc{hedegaard_structured_2022,
 author = {Hedegaard, Lukas and Alok, Aman and Jose, Juby and Iosifidis, Alexandros},
 journal = {ArXiv preprint},
 title = {Structured {Pruning} {Adapters}},
 volume = {abs/2211.10155},
 year = {2022}
}

@misc{santacroce_what_2023,
 author = {Santacroce, Michael and Wen, Zixin and Shen, Yelong and Li, Yuanzhi},
 journal = {ArXiv preprint},
 title = {What {Matters} {In} {The} {Structured} {Pruning} of {Generative} {Language} {Models}?},
 volume = {abs/2302.03773},
 year = {2023}
}

@inproceedings{shen_when_2022,
 author = {Shen, Maying and Molchanov, Pavlo and Yin, Hongxu and Alvarez, Jose M.},
 file = {Full Text PDF:C\:\\Users\\ROIM\\Zotero\\storage\\2Y6PNQNG\\Shen et al. - 2022 - When To Prune A Policy Towards Early Structural P.pdf:application/pdf},
 keywords = {pruning, structural pruning},
 language = {en},
 pages = {12247--12256},
 shorttitle = {When {To} {Prune}?},
 title = {When {To} {Prune}? {A} {Policy} {Towards} {Early} {Structural} {Pruning}},
 urldate = {2023-02-27},
 year = {2022}
}

@inproceedings{nova_gradient,
 abstract = {Large Language Models (LLMs) have achieved great success in solving difficult tasks across many domains, but such success comes with a high computation cost, and inference latency. As developers and third parties customize these models, the need to provide efficient inference has increased. Many efforts have attempted to reduce inference cost through model compression techniques such as pruning and distillation. However, these techniques either require labeled data, or are time-consuming as they require the compressed model to be retrained to regain accuracy. In this paper, we propose a gradient-free structured pruning framework that uses only unlabeled data. An evaluation on the GLUE and SQuAD benchmarks using BERTBASE and DistilBERT illustrates the effectiveness of the proposed approach. By only using the weights of the pre-trained model and unlabeled data, in a matter of a few minutes on a single GPU, up to 40\% of the original FLOP count can be reduced with less than a 4\% accuracy loss across all tasks considered.},
 articleno = {1097},
 author = {Nova, Azade and Dai, Hanjun and Schuurmans, Dale},
 booktitle = {Proceedings of the 40th International Conference on Machine Learning},
 location = {<conf-loc>, <city>Honolulu</city>, <state>Hawaii</state>, <country>USA</country>, </conf-loc>},
 numpages = {16},
 publisher = {JMLR.org},
 series = {ICML'23},
 title = {Gradient-free structured pruning with unlabeled data},
 year = {2023}
}

@inproceedings{sung2021training,
 author = {Yi{-}Lin Sung and
Varun Nair and
Colin Raffel},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/SungNR21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {24193--24205},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {Training Neural Networks with Fixed Sparse Masks},
 year = {2021}
}

@inproceedings{haidar2022rail,
 address = {Seattle, United States},
 author = {Haidar, Md Akmal  and
Anchuri, Nithin  and
Rezagholizadeh, Mehdi  and
Ghaddar, Abbas  and
Langlais, Philippe  and
Poupart, Pascal},
 booktitle = {Findings of the Association for Computational Linguistics: NAACL 2022},
 doi = {10.18653/v1/2022.findings-naacl.103},
 pages = {1389--1400},
 publisher = {Association for Computational Linguistics},
 title = {{RAIL}-{KD}: {RA}ndom Intermediate Layer Mapping for Knowledge Distillation},
 year = {2022}
}

@inproceedings{devlin-etal-2019-bert,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 year = {2019}
}

@article{liu2019roberta,
 author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
 journal = {ArXiv preprint},
 title = {Roberta: A robustly optimized bert pretraining approach},
 volume = {abs/1907.11692},
 year = {2019}
}

@article{raffel2020exploring,
 author = {Colin Raffel and
Noam Shazeer and
Adam Roberts and
Katherine Lee and
Sharan Narang and
Michael Matena and
Yanqi Zhou and
Wei Li and
Peter J. Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
 journal = {J. Mach. Learn. Res.},
 pages = {140:1--140:67},
 timestamp = {Fri, 05 Feb 2021 00:00:00 +0100},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer},
 volume = {21},
 year = {2020}
}

@inproceedings{zhang2023adaptive,
 author = {Qingru Zhang and Minshuo Chen and Alexander Bukharin and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
 booktitle = {The Eleventh International Conference on Learning Representations },
 title = {Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning },
 year = {2023}
}

@inproceedings{he-etal-2022-sparseadapter,
 address = {Abu Dhabi, United Arab Emirates},
 author = {He, Shwai  and
Ding, Liang  and
Dong, Daize  and
Zhang, Jeremy  and
Tao, Dacheng},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
 pages = {2184--2190},
 publisher = {Association for Computational Linguistics},
 title = {{S}parse{A}dapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters},
 year = {2022}
}

@inproceedings{guo-etal-2021-parameter,
 address = {Online},
 author = {Guo, Demi  and
Rush, Alexander  and
Kim, Yoon},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.378},
 pages = {4884--4896},
 publisher = {Association for Computational Linguistics},
 title = {Parameter-Efficient Transfer Learning with Diff Pruning},
 year = {2021}
}

@inproceedings{ben-zaken-etal-2022-bitfit,
 address = {Dublin, Ireland},
 author = {Ben Zaken, Elad  and
Goldberg, Yoav  and
Ravfogel, Shauli},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 doi = {10.18653/v1/2022.acl-short.1},
 pages = {1--9},
 publisher = {Association for Computational Linguistics},
 title = {{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
 year = {2022}
}

@inproceedings{jiao2020tinybert,
 address = {Online},
 author = {Jiao, Xiaoqi  and
Yin, Yichun  and
Shang, Lifeng  and
Jiang, Xin  and
Chen, Xiao  and
Li, Linlin  and
Wang, Fang  and
Liu, Qun},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 doi = {10.18653/v1/2020.findings-emnlp.372},
 pages = {4163--4174},
 publisher = {Association for Computational Linguistics},
 title = {{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding},
 year = {2020}
}

@inproceedings{vaswani2017attention,
 author = {Ashish Vaswani and
Noam Shazeer and
Niki Parmar and
Jakob Uszkoreit and
Llion Jones and
Aidan N. Gomez and
Lukasz Kaiser and
Illia Polosukhin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
 booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, {USA}},
 editor = {Isabelle Guyon and
Ulrike von Luxburg and
Samy Bengio and
Hanna M. Wallach and
Rob Fergus and
S. V. N. Vishwanathan and
Roman Garnett},
 pages = {5998--6008},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Attention is All you Need},
 year = {2017}
}

@inproceedings{louizoslearning,
 author = {Christos Louizos and
Max Welling and
Diederik P. Kingma},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/LouizosWK18.bib},
 booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Learning Sparse Neural Networks through L{\_}0 Regularization},
 year = {2018}
}

@inproceedings{lester-etal-2021-power,
 address = {Online and Punta Cana, Dominican Republic},
 author = {Lester, Brian  and
Al-Rfou, Rami  and
Constant, Noah},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.243},
 pages = {3045--3059},
 publisher = {Association for Computational Linguistics},
 title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
 year = {2021}
}

@inproceedings{li2021prefix,
 address = {Online},
 author = {Li, Xiang Lisa  and
Liang, Percy},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.353},
 pages = {4582--4597},
 publisher = {Association for Computational Linguistics},
 title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
 year = {2021}
}

@inproceedings{han2015learning,
 author = {Song Han and
Jeff Pool and
John Tran and
William J. Dally},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/HanPTD15.bib},
 booktitle = {Advances in Neural Information Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015, December 7-12, 2015,
Montreal, Quebec, Canada},
 editor = {Corinna Cortes and
Neil D. Lawrence and
Daniel D. Lee and
Masashi Sugiyama and
Roman Garnett},
 pages = {1135--1143},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Learning both Weights and Connections for Efficient Neural Network},
 year = {2015}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {ArXiv preprint},
 title = {Llama: Open and efficient foundation language models},
 volume = {abs/2302.13971},
 year = {2023}
}

@article{zhang2022opt,
 author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
 journal = {ArXiv preprint},
 title = {Opt: Open pre-trained transformer language models},
 volume = {abs/2205.01068},
 year = {2022}
}

@inproceedings{brown2020language,
 author = {Tom B. Brown and
Benjamin Mann and
Nick Ryder and
Melanie Subbiah and
Jared Kaplan and
Prafulla Dhariwal and
Arvind Neelakantan and
Pranav Shyam and
Girish Sastry and
Amanda Askell and
Sandhini Agarwal and
Ariel Herbert{-}Voss and
Gretchen Krueger and
Tom Henighan and
Rewon Child and
Aditya Ramesh and
Daniel M. Ziegler and
Jeffrey Wu and
Clemens Winter and
Christopher Hesse and
Mark Chen and
Eric Sigler and
Mateusz Litwin and
Scott Gray and
Benjamin Chess and
Jack Clark and
Christopher Berner and
Sam McCandlish and
Alec Radford and
Ilya Sutskever and
Dario Amodei},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
 booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual},
 editor = {Hugo Larochelle and
Marc'Aurelio Ranzato and
Raia Hadsell and
Maria{-}Florina Balcan and
Hsuan{-}Tien Lin},
 timestamp = {Tue, 19 Jan 2021 00:00:00 +0100},
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}

@inproceedings{wang-etal-2022-super,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Wang, Yizhong  and
Mishra, Swaroop  and
Alipoormolabashi, Pegah  and
Kordi, Yeganeh  and
Mirzaei, Amirreza  and
Naik, Atharva  and
Ashok, Arjun  and
Dhanasekaran, Arut Selvan  and
Arunkumar, Anjana  and
Stap, David  and
Pathak, Eshaan  and
Karamanolakis, Giannis  and
Lai, Haizhi  and
Purohit, Ishan  and
Mondal, Ishani  and
Anderson, Jacob  and
Kuznia, Kirby  and
Doshi, Krima  and
Pal, Kuntal Kumar  and
Patel, Maitreya  and
Moradshahi, Mehrad  and
Parmar, Mihir  and
Purohit, Mirali  and
Varshney, Neeraj  and
Kaza, Phani Rohitha  and
Verma, Pulkit  and
Puri, Ravsehaj Singh  and
Karia, Rushang  and
Doshi, Savan  and
Sampat, Shailaja Keyur  and
Mishra, Siddhartha  and
Reddy A, Sujan  and
Patro, Sumanta  and
Dixit, Tanay  and
Shen, Xudong},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {5085--5109},
 publisher = {Association for Computational Linguistics},
 title = {Super-{N}atural{I}nstructions: Generalization via Declarative Instructions on 1600+ {NLP} Tasks},
 year = {2022}
}

@inproceedings{mishra-etal-2022-cross,
 address = {Dublin, Ireland},
 author = {Mishra, Swaroop  and
Khashabi, Daniel  and
Baral, Chitta  and
Hajishirzi, Hannaneh},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.244},
 pages = {3470--3487},
 publisher = {Association for Computational Linguistics},
 title = {Cross-Task Generalization via Natural Language Crowdsourcing Instructions},
 year = {2022}
}

@inproceedings{wei2022finetuned,
 author = {Jason Wei and
Maarten Bosma and
Vincent Y. Zhao and
Kelvin Guu and
Adams Wei Yu and
Brian Lester and
Nan Du and
Andrew M. Dai and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/WeiBZGYLDDL22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Finetuned Language Models are Zero-Shot Learners},
 year = {2022}
}

@inproceedings{NEURIPS2022_5434be94,
 author = {Shen, Maying and Yin, Hongxu and Molchanov, Pavlo and Mao, Lei and Liu, Jianna and Alvarez, Jose M.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {12894--12908},
 publisher = {Curran Associates, Inc.},
 title = {Structural Pruning via Latency-Saliency Knapsack},
 volume = {35},
 year = {2022}
}

@article{ma2023llm,
 author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
 journal = {ArXiv preprint},
 title = {LLM-Pruner: On the Structural Pruning of Large Language Models},
 volume = {abs/2305.11627},
 year = {2023}
}

@article{zhao2023cpet,
 author = {Zhao, Weilin and Huang, Yuxiang and Han, Xu and Liu, Zhiyuan and Zhang, Zhengyan and Sun, Maosong},
 journal = {ArXiv preprint},
 title = {CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models},
 volume = {abs/2307.07705},
 year = {2023}
}

@article{sun2023simple,
 author = {Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
 journal = {ArXiv preprint},
 title = {A Simple and Effective Pruning Approach for Large Language Models},
 volume = {abs/2306.11695},
 year = {2023}
}

@inproceedings{Frantar2023SparseGPTML,
 abstract = {We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in <em>one-shot, without any retraining</em>, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.},
 author = {Frantar, Elias and Alistarh, Dan},
 booktitle = {Proceedings of the 40th International Conference on Machine Learning},
 editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
 pages = {10323--10337},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {{S}parse{GPT}: Massive Language Models Can be Accurately Pruned in One-Shot},
 volume = {202},
 year = {2023}
}

@inproceedings{karimi2021compacter,
 author = {Rabeeh Karimi Mahabadi and
James Henderson and
Sebastian Ruder},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/MahabadiHR21.bib},
 booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS 2021, December
6-14, 2021, virtual},
 editor = {Marc'Aurelio Ranzato and
Alina Beygelzimer and
Yann N. Dauphin and
Percy Liang and
Jennifer Wortman Vaughan},
 pages = {1022--1035},
 timestamp = {Tue, 03 May 2022 01:00:00 +0200},
 title = {Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
 year = {2021}
}

@inproceedings{ijcai2022p586,
 author = {Li, Yuchao and Luo, Fuli and Tan, Chuanqi and Wang, Mengdi and Huang, Songfang and Li, Shen and Bai, Junjie},
 booktitle = {Proceedings of the Thirty-First International Joint Conference on
Artificial Intelligence, {IJCAI-22}},
 doi = {10.24963/ijcai.2022/586},
 editor = {Lud De Raedt},
 note = {Main Track},
 pages = {4223--4229},
 publisher = {International Joint Conferences on Artificial Intelligence Organization},
 title = {Parameter-Efficient Sparsity for Large Language Models Fine-Tuning},
 year = {2022}
}

@article{zhang2023pruning,
 author = {Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
 journal = {ArXiv preprint},
 title = {Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning},
 volume = {abs/2305.18403},
 year = {2023}
}

@article{zhang2023increlora,
 author = {Zhang, Feiyu and Li, Liangzhi and Chen, Junhao and Jiang, Zhouqiang and Wang, Bowen and Qian, Yiming},
 journal = {ArXiv preprint},
 title = {IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning},
 volume = {abs/2308.12043},
 year = {2023}
}

@article{dettmers2023qlora,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 journal = {ArXiv preprint},
 title = {Qlora: Efficient finetuning of quantized llms},
 volume = {abs/2305.14314},
 year = {2023}
}

@misc{alpaca,
 author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
 howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
 journal = {GitHub repository},
 publisher = {GitHub},
 title = {Stanford Alpaca: An Instruction-following LLaMA model},
 year = {2023}
}

@inproceedings{hendrycks2021measuring,
 author = {Dan Hendrycks and
Collin Burns and
Steven Basart and
Andy Zou and
Mantas Mazeika and
Dawn Song and
Jacob Steinhardt},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/HendrycksBBZMSS21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Measuring Massive Multitask Language Understanding},
 year = {2021}
}

@inproceedings{wang2018glue,
 author = {Alex Wang and
Amanpreet Singh and
Julian Michael and
Felix Hill and
Omer Levy and
Samuel R. Bowman},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/WangSMHLB19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
Language Understanding},
 year = {2019}
}

@inproceedings{rajpurkar-etal-2018-know,
 address = {Melbourne, Australia},
 author = {Rajpurkar, Pranav  and
Jia, Robin  and
Liang, Percy},
 booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 doi = {10.18653/v1/P18-2124},
 pages = {784--789},
 publisher = {Association for Computational Linguistics},
 title = {Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}},
 year = {2018}
}

@inproceedings{Nallapati2016AbstractiveTS,
 address = {Berlin, Germany},
 author = {Nallapati, Ramesh  and
Zhou, Bowen  and
dos Santos, Cicero  and
Gulcehre, Caglar  and
Xiang, Bing},
 booktitle = {Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning},
 doi = {10.18653/v1/K16-1028},
 pages = {280--290},
 publisher = {Association for Computational Linguistics},
 title = {Abstractive Text Summarization using Sequence-to-sequence {RNN}s and Beyond},
 year = {2016}
}

@inproceedings{wang-etal-2022-finding-skill,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Wang, Xiaozhi  and
Wen, Kaiyue  and
Zhang, Zhengyan  and
Hou, Lei  and
Liu, Zhiyuan  and
Li, Juanzi},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {11132--11152},
 publisher = {Association for Computational Linguistics},
 title = {Finding Skill Neurons in Pre-trained Transformer-based Language Models},
 year = {2022}
}

@article{panigrahi2023task,
 author = {Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora, Sanjeev},
 journal = {ArXiv preprint},
 title = {Task-Specific Skill Localization in Fine-tuned Language Models},
 volume = {abs/2302.06600},
 year = {2023}
}

@inproceedings{Shen_2022_CVPR,
 author = {Shen, Maying and Molchanov, Pavlo and Yin, Hongxu and Alvarez, Jose M.},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {12247-12256},
 title = {When To Prune? A Policy Towards Early Structural Pruning},
 year = {2022}
}

@inproceedings{frankle2021pruning,
 author = {Jonathan Frankle and
Gintare Karolina Dziugaite and
Daniel Roy and
Michael Carbin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/FrankleD0C21.bib},
 booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
Virtual Event, Austria, May 3-7, 2021},
 publisher = {OpenReview.net},
 timestamp = {Wed, 23 Jun 2021 01:00:00 +0200},
 title = {Pruning Neural Networks at Initialization: Why Are We Missing the
Mark?},
 year = {2021}
}

@inproceedings{Han2015DeepCC,
 author = {Song Han and
Huizi Mao and
William J. Dally},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/HanMD15.bib},
 booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
 editor = {Yoshua Bengio and
Yann LeCun},
 timestamp = {Fri, 20 Nov 2020 00:00:00 +0100},
 title = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained
Quantization and Huffman Coding},
 year = {2016}
}

@inproceedings{LeCun1989OptimalBD,
 author = {Yann LeCun and John S. Denker and Sara A. Solla},
 booktitle = {NIPS},
 title = {Optimal Brain Damage},
 year = {1989}
}

@article{Hinton2015DistillingTK,
 author = {Geoffrey E. Hinton and
Oriol Vinyals and
Jeffrey Dean},
 journal = {ArXiv preprint},
 title = {Distilling the Knowledge in a Neural Network},
 volume = {abs/1503.02531},
 year = {2015}
}

@inproceedings{Xue2020mT5AM,
 address = {Online},
 author = {Xue, Linting  and
Constant, Noah  and
Roberts, Adam  and
Kale, Mihir  and
Al-Rfou, Rami  and
Siddhant, Aditya  and
Barua, Aditya  and
Raffel, Colin},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.41},
 pages = {483--498},
 publisher = {Association for Computational Linguistics},
 title = {m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
 year = {2021}
}

@inproceedings{bojar-etal-2016-findings,
 address = {Berlin, Germany},
 author = {Bojar, Ond{\v{r}}ej  and
Chatterjee, Rajen  and
Federmann, Christian  and
Graham, Yvette  and
Haddow, Barry  and
Huck, Matthias  and
Jimeno Yepes, Antonio  and
Koehn, Philipp  and
Logacheva, Varvara  and
Monz, Christof  and
Negri, Matteo  and
N{\'e}v{\'e}ol, Aur{\'e}lie  and
Neves, Mariana  and
Popel, Martin  and
Post, Matt  and
Rubino, Raphael  and
Scarton, Carolina  and
Specia, Lucia  and
Turchi, Marco  and
Verspoor, Karin  and
Zampieri, Marcos},
 booktitle = {Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
 doi = {10.18653/v1/W16-2301},
 pages = {131--198},
 publisher = {Association for Computational Linguistics},
 title = {Findings of the 2016 Conference on Machine Translation},
 year = {2016}
}

@inproceedings{pmlr-v202-xiao23c,
 abstract = {Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56$\times$ speedup and 2$\times$ memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.},
 author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
 booktitle = {Proceedings of the 40th International Conference on Machine Learning},
 editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
 pages = {38087--38099},
 pdf = {https://proceedings.mlr.press/v202/xiao23c/xiao23c.pdf},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 title = {{S}mooth{Q}uant: Accurate and Efficient Post-Training Quantization for Large Language Models},
 volume = {202},
 year = {2023}
}

@inproceedings{Dettmers2022LLMint88M,
 author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {30318--30332},
 publisher = {Curran Associates, Inc.},
 title = {GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
 volume = {35},
 year = {2022}
}

@inproceedings{Frantar2022GPTQAP,
 author = {Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
 booktitle = {The Eleventh International Conference on Learning Representations },
 title = {{OPTQ}: Accurate Quantization for Generative Pre-trained Transformers},
 year = {2023}
}

@inproceedings{ro-etal-2022-scaling,
 address = {Dublin, Ireland},
 author = {Ro, Jae  and
Breiner, Theresa  and
McConnaughey, Lara  and
Chen, Mingqing  and
Suresh, Ananda  and
Kumar, Shankar  and
Mathews, Rajiv},
 booktitle = {Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022)},
 doi = {10.18653/v1/2022.fl4nlp-1.2},
 pages = {6--20},
 publisher = {Association for Computational Linguistics},
 title = {Scaling Language Model Size in Cross-Device Federated Learning},
 year = {2022}
}

@article{liu2021federated,
 author = {Liu, Ming and Ho, Stella and Wang, Mengqi and Gao, Longxiang and Jin, Yuan and Zhang, He},
 journal = {ArXiv preprint},
 title = {Federated learning meets natural language processing: A survey},
 volume = {abs/2107.12603},
 year = {2021}
}

@article{wang2019federated,
 author = {Wang, Kangkang and Mathews, Rajiv and Kiddon, Chlo{\'e} and Eichner, Hubert and Beaufays, Fran{\c{c}}oise and Ramage, Daniel},
 journal = {ArXiv preprint},
 title = {Federated evaluation of on-device personalization},
 volume = {abs/1910.10252},
 year = {2019}
}

@inproceedings{jang2022towards,
 author = {Joel Jang and
Seonghyeon Ye and
Sohee Yang and
Joongbo Shin and
Janghoon Han and
Gyeonghun Kim and
Stanley Jungkyu Choi and
Minjoon Seo},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/JangYYSHKCS22.bib},
 booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
2022, Virtual Event, April 25-29, 2022},
 publisher = {OpenReview.net},
 timestamp = {Sat, 20 Aug 2022 01:00:00 +0200},
 title = {Towards Continual Knowledge Learning of Language Models},
 year = {2022}
}

@inproceedings{scialom-etal-2022-fine,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Scialom, Thomas  and
Chakrabarty, Tuhin  and
Muresan, Smaranda},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {6107--6122},
 publisher = {Association for Computational Linguistics},
 title = {Fine-tuned Language Models are Continual Learners},
 year = {2022}
}

@inproceedings{ke-etal-2022-continual,
 address = {Abu Dhabi, United Arab Emirates},
 author = {Ke, Zixuan  and
Lin, Haowei  and
Shao, Yijia  and
Xu, Hu  and
Shu, Lei  and
Liu, Bing},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {10205--10216},
 publisher = {Association for Computational Linguistics},
 title = {Continual Training of Language Models for Few-Shot Learning},
 year = {2022}
}

@inproceedings{Pfeiffer2020AdapterFusionNT,
 address = {Online},
 author = {Pfeiffer, Jonas  and
Kamath, Aishwarya  and
R{\"u}ckl{\'e}, Andreas  and
Cho, Kyunghyun  and
Gurevych, Iryna},
 booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
 doi = {10.18653/v1/2021.eacl-main.39},
 pages = {487--503},
 publisher = {Association for Computational Linguistics},
 title = {{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning},
 year = {2021}
}

@article{lialin2023scaling,
 author = {Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
 journal = {ArXiv preprint},
 title = {Scaling down to scale up: A guide to parameter-efficient fine-tuning},
 volume = {abs/2303.15647},
 year = {2023}
}

@inproceedings{micikevicius2018mixed,
 author = {Paulius Micikevicius and
Sharan Narang and
Jonah Alben and
Gregory F. Diamos and
Erich Elsen and
David Garc{\'{\i}}a and
Boris Ginsburg and
Michael Houston and
Oleksii Kuchaiev and
Ganesh Venkatesh and
Hao Wu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/MicikeviciusNAD18.bib},
 booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {Mixed Precision Training},
 year = {2018}
}

@inproceedings{frankle2018lottery,
 author = {Jonathan Frankle and
Michael Carbin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/iclr/FrankleC19.bib},
 booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
New Orleans, LA, USA, May 6-9, 2019},
 publisher = {OpenReview.net},
 timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
 title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
 year = {2019}
}

@inproceedings{xu2021rethinking,
 address = {Online},
 author = {Xu, Dongkuan  and
Yen, Ian En-Hsu  and
Zhao, Jinxi  and
Xiao, Zhibin},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.188},
 pages = {2376--2382},
 publisher = {Association for Computational Linguistics},
 title = {Rethinking Network Pruning {--} under the Pre-train and Fine-tune Paradigm},
 year = {2021}
}

@misc{alpaca_eval,
 author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
 howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}},
 journal = {GitHub repository},
 publisher = {GitHub},
 title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
 year = {2023}
}

@article{zhang2023emergent,
 author = {Zhang, Zhengyan and Zeng, Zhiyuan and Lin, Yankai and Xiao, Chaojun and Wang, Xiaozhi and Han, Xu and Liu, Zhiyuan and Xie, Ruobing and Sun, Maosong and Zhou, Jie},
 journal = {ArXiv preprint},
 title = {Emergent Modularity in Pre-trained Transformers},
 volume = {abs/2305.18390},
 year = {2023}
}

@article{lin2023awq,
 author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
 journal = {ArXiv preprint},
 title = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
 volume = {abs/2306.00978},
 year = {2023}
}

@misc{eval-harness,
 author = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
 doi = {10.5281/zenodo.10256836},
 publisher = {Zenodo},
 title = {A framework for few-shot language model evaluation},
 version = {v0.4.0},
 year = {2023}
}

@inproceedings{lin-etal-2022-truthfulqa,
 address = {Dublin, Ireland},
 author = {Lin, Stephanie  and
Hilton, Jacob  and
Evans, Owain},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.229},
 pages = {3214--3252},
 publisher = {Association for Computational Linguistics},
 title = {{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods},
 year = {2022}
}

@inproceedings{zellers-etal-2019-hellaswag,
 address = {Florence, Italy},
 author = {Zellers, Rowan  and
Holtzman, Ari  and
Bisk, Yonatan  and
Farhadi, Ali  and
Choi, Yejin},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1472},
 pages = {4791--4800},
 publisher = {Association for Computational Linguistics},
 title = {{H}ella{S}wag: Can a Machine Really Finish Your Sentence?},
 year = {2019}
}

@article{clark2018think,
 author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
 journal = {ArXiv preprint},
 title = {Think you have solved question answering? try arc, the ai2 reasoning challenge},
 volume = {abs/1803.05457},
 year = {2018}
}

@article{10.1145/3352020.3352024,
 abstract = {Researchers have proposed hardware, software, and algorithmic optimizations to improve the computational performance of deep learning. While some of these optimizations perform the same operations faster (e.g., increasing GPU clock speed), many others modify the semantics of the training procedure (e.g., reduced precision), and can impact the final model's accuracy on unseen data. Due to a lack of standard evaluation criteria that considers these trade-offs, it is difficult to directly compare these optimizations. To address this problem, we recently introduced DAWNBENCH, a benchmark competition focused on end-to-end training time to achieve near-state-of-the-art accuracy on an unseen dataset-a combined metric called time-to-accuracy (TTA). In this work, we analyze the entries from DAWNBENCH, which received optimized submissions from multiple industrial groups, to investigate the behavior of TTA as a metric as well as trends in the best-performing entries. We show that TTA has a low coefficient of variation and that models optimized for TTA generalize nearly as well as those trained using standard methods. Additionally, even though DAWNBENCH entries were able to train ImageNet models in under 3 minutes, we find they still underutilize hardware capabilities such as Tensor Cores. Furthermore, we find that distributed entries can spend more than half of their time on communication. We show similar findings with entries to the MLPERF v0.5 benchmark.},
 address = {New York, NY, USA},
 author = {Coleman, Cody and Kang, Daniel and Narayanan, Deepak and Nardi, Luigi and Zhao, Tian and Zhang, Jian and Bailis, Peter and Olukotun, Kunle and R\'{e}, Chris and Zaharia, Matei},
 doi = {10.1145/3352020.3352024},
 issn = {0163-5980},
 issue_date = {July 2019},
 journal = {SIGOPS Oper. Syst. Rev.},
 number = {1},
 numpages = {12},
 pages = {14â€“25},
 publisher = {Association for Computing Machinery},
 title = {Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark},
 volume = {53},
 year = {2019}
}

@article{ding2023parameter,
 author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
 journal = {Nature Machine Intelligence},
 number = {3},
 pages = {220--235},
 publisher = {Nature Publishing Group UK London},
 title = {Parameter-efficient fine-tuning of large-scale pre-trained language models},
 volume = {5},
 year = {2023}
}

@article{kaplan2020scaling,
 author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
 journal = {ArXiv preprint},
 title = {Scaling laws for neural language models},
 volume = {abs/2001.08361},
 year = {2020}
}

@article{xu2023qa,
 author = {Xu, Yuhui and Xie, Lingxi and Gu, Xiaotao and Chen, Xin and Chang, Heng and Zhang, Hengheng and Chen, Zhensu and Zhang, Xiaopeng and Tian, Qi},
 journal = {ArXiv preprint},
 title = {QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models},
 volume = {abs/2309.14717},
 year = {2023}
}
