\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & \underline{0.67} & 0.70 & 1.18 & 3.67 & 13.22 & 50.44 & - & - & - & - \\
\textbf{Megatron} & 0.74 & \underline{0.65} & 1.23 & 3.80 & 13.21 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 3.93 & 7.01 & 13.15 & 25.89 & 52.09 & 105.00 & 215.13 & - & - & - \\
\textbf{Local Attention} & 1.09 & 1.27 & 1.99 & 5.38 & 18.32 & 36.77 & 73.67 & 147.29 & 296.35 & - \\
\textbf{Linformer} & 1.31 & 1.25 & 1.30 & \underline{1.29} & \underline{3.20} & \underline{6.10} & \underline{11.93} & \underline{23.39} & \underline{46.72} & \underline{100.52} \\
\textbf{Smyrf} & 2.98 & 4.23 & 7.78 & 15.12 & 29.96 & 59.45 & 118.60 & 237.02 & - & - \\
\textbf{LSformer} & 3.03 & 3.05 & 4.26 & 10.70 & 30.77 & 60.15 & 118.33 & 234.94 & - & - \\
\hline
\textbf{Block Sparse} & 2.39 & 2.40 & 3.31 & 5.02 & 12.25 & 35.94 & - & - & - & - \\
\textbf{Longformer} & 2.36 & 2.34 & 2.38 & 2.94 & 9.83 & 21.35 & 58.12 & - & - & - \\
\textbf{BigBird} & 2.35 & 2.35 & 2.37 & 3.25 & 10.36 & 22.57 & 60.63 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.31} & \textbf{0.31} & \textbf{0.73} & 2.29 & 7.64 & 30.09 & 118.50 & 470.51 & 1876.08 & 7492.85 \\
\textbf{Block-Sparse \sysname} & 0.74 & 0.77 & \underline{0.82} & \textbf{0.88} & \textbf{1.71} & \textbf{3.21} & \textbf{6.56} & \textbf{12.60} & \textbf{24.93} & \textbf{50.39} \\
\bottomrule
\end{tabular}
\label{tab:combined}
\end{table}