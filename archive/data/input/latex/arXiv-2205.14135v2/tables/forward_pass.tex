\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & \underline{0.21} & \underline{0.22} & 0.43 & 1.27 & 4.32 & 16.47 & 67.77 & - & - & - \\
\textbf{Megatron} & 0.24 & 0.26 & \underline{0.42} & 1.33 & 4.28 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 1.77 & 2.82 & 5.01 & 9.74 & 20.03 & 41.11 & 87.39 & 192.40 & - & - \\
\textbf{Local Attention} & 0.48 & 0.57 & 0.80 & 1.90 & 5.76 & 11.56 & 23.13 & 46.65 & 94.74 & - \\
\textbf{Linformer} & 0.46 & 0.36 & 0.45 & \textbf{0.50} & \underline{1.09} & \underline{2.09} & \underline{4.01} & \underline{7.90} & \underline{15.70} & \underline{35.40} \\
\textbf{Smyrf} & 1.94 & 1.96 & 3.01 & 5.69 & 11.26 & 22.23 & 44.21 & 88.22 & - & - \\
\textbf{LSformer} & 1.21 & 1.34 & 1.34 & 3.31 & 11.01 & 21.71 & 43.27 & 86.32 & 172.85 & - \\
\hline
\textbf{Block Sparse} & 0.96 & 1.04 & 1.66 & 2.16 & 5.41 & 16.15 & - & - & - & - \\
\textbf{Longformer} & 0.99 & 0.98 & 0.99 & 1.56 & 4.79 & 11.07 & 32.98 & - & - & - \\
\textbf{BigBird} & 0.96 & 1.02 & 1.02 & 1.48 & 5.05 & 11.59 & 34.16 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.08} & \textbf{0.09} & \textbf{0.18} & 0.68 & 2.40 & 8.42 & 33.54 & 134.03 & 535.95 & 2147.05 \\
\textbf{Block-Sparse \sysname} & 0.56 & 0.52 & 0.63 & \underline{0.65} & \textbf{0.61} & \textbf{0.96} & \textbf{1.69} & \textbf{3.02} & \textbf{5.69} & \textbf{11.77} \\
\bottomrule
\end{tabular}
\label{tab:forward_pass}
\end{table}