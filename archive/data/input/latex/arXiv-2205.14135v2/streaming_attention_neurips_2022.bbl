\begin{thebibliography}{94}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aggarwal and Vitter(1988)]{aggarwal1988input}
Alok Aggarwal and S~Vitter, Jeffrey.
\newblock The input/output complexity of sorting and related problems.
\newblock \emph{Communications of the ACM}, 31\penalty0 (9):\penalty0
  1116--1127, 1988.

\bibitem[Bello(2021)]{bello2021lambdanetworks}
Irwan Bello.
\newblock Lambda{N}etworks: Modeling long-range interactions without attention.
\newblock \emph{arXiv preprint arXiv:2102.08602}, 2021.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Blackford et~al.(2002)Blackford, Petitet, Pozo, Remington, Whaley,
  Demmel, Dongarra, Duff, Hammarling, Henry, et~al.]{blackford2002updated}
L~Susan Blackford, Antoine Petitet, Roldan Pozo, Karin Remington, R~Clint
  Whaley, James Demmel, Jack Dongarra, Iain Duff, Sven Hammarling, Greg Henry,
  et~al.
\newblock An updated set of basic linear algebra subprograms (blas).
\newblock \emph{ACM Transactions on Mathematical Software}, 28\penalty0
  (2):\penalty0 135--151, 2002.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chalkidis et~al.(2019)Chalkidis, Androutsopoulos, and
  Aletras]{chalkidis-etal-2019-neural}
Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras.
\newblock Neural legal judgment prediction in {E}nglish.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 4317--4323, Florence, Italy, 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1424}.
\newblock URL \url{https://www.aclweb.org/anthology/P19-1424}.

\bibitem[Chalkidis et~al.(2021)Chalkidis, Fergadiotis, Tsarapatsanis, Aletras,
  Androutsopoulos, and Malakasiotis]{chalkidis-et-al-2021-ecthr}
Ilias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapatsanis, Nikolaos Aletras,
  Ion Androutsopoulos, and Prodromos Malakasiotis.
\newblock Paragraph-level rationale extraction through regularization: A case
  study on european court of human rights cases.
\newblock In \emph{Proceedings of the Annual Conference of the North American
  Chapter of the Association for Computational Linguistics}, Mexico City,
  Mexico, 2021. Association for Computational Linguistics.

\bibitem[Charlier et~al.(2021)Charlier, Feydy, Glaunès, Collin, and
  Durif]{charlier2021kernel}
Benjamin Charlier, Jean Feydy, Joan~Alexis Glaunès, François-David Collin,
  and Ghislain Durif.
\newblock Kernel operations on the gpu, with autodiff, without memory
  overflows.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (74):\penalty0 1--6, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-275.html}.

\bibitem[Chen et~al.(2021)Chen, Dao, Winsor, Song, Rudra, and
  Ré]{scatterbrain}
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré.
\newblock Scatterbrain: Unifying sparse and low-rank attention.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Dai et~al.(2022)Dai, Chalkidis, Darkner, and
  Elliott]{dai2022revisiting}
Xiang Dai, Ilias Chalkidis, Sune Darkner, and Desmond Elliott.
\newblock Revisiting transformer-based models for long document classification.
\newblock \emph{arXiv preprint arXiv:2204.06683}, 2022.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G Carbonell, Quoc Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 2978--2988, 2019.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and R{\'e}]{dao2019learning}
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R{\'e}.
\newblock Learning fast algorithms for linear transforms using butterfly
  factorizations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Dao et~al.(2020)Dao, Sohoni, Gu, Eichhorn, Blonder, Leszczynski,
  Rudra, and R{\'e}]{dao2020kaleidoscope}
Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan
  Leszczynski, Atri Rudra, and Christopher R{\'e}.
\newblock Kaleidoscope: An efficient, learnable representation for all
  structured linear maps.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Dao et~al.(2022{\natexlab{a}})Dao, Chen, Liang, Yang, Song, Rudra, and
  R{\'e}]{dao2021pixelated}
Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and
  Christopher R{\'e}.
\newblock Pixelated butterfly: Simple and efficient sparse training for neural
  network models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022{\natexlab{a}}.

\bibitem[Dao et~al.(2022{\natexlab{b}})Dao, Chen, Sohoni, Desai, Poli, Grogan,
  Liu, Rao, Rudra, and R{\'e}]{dao2022monarch}
Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan,
  Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R{\'e}.
\newblock Monarch: Expressive structured matrices for efficient and accurate
  training.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2022{\natexlab{b}}.

\bibitem[Daras et~al.(2020)Daras, Kitaev, Odena, and Dimakis]{daras2020smyrf}
Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros~G Dimakis.
\newblock Smyrf-efficient attention using asymmetric clustering.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6476--6489, 2020.

\bibitem[De~Sa et~al.(2018)De~Sa, Gu, Puttagunta, R{\'e}, and
  Rudra]{desa2018two}
Christopher De~Sa, Albert Gu, Rohan Puttagunta, Christopher R{\'e}, and Atri
  Rudra.
\newblock A two-pronged progress in structured dense matrix vector
  multiplication.
\newblock In \emph{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1060--1079. SIAM, 2018.

\bibitem[Denning(1968)]{denning1968working}
Peter~J Denning.
\newblock The working set model for program behavior.
\newblock \emph{Communications of the ACM}, 11\penalty0 (5):\penalty0 323--333,
  1968.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock 2019.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno~Jialin Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock \emph{arXiv preprint arXiv:1705.07565}, 2017.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Eidelman and Gohberg(1999)]{eidelman1999new}
Y~Eidelman and I~Gohberg.
\newblock On a new class of structured matrices.
\newblock \emph{Integral Equations and Operator Theory}, 34\penalty0
  (3):\penalty0 293--324, 1999.

\bibitem[Feydy et~al.(2020)Feydy, Glaun{\`e}s, Charlier, and
  Bronstein]{feydy2020fast}
Jean Feydy, Joan Glaun{\`e}s, Benjamin Charlier, and Michael Bronstein.
\newblock Fast geometric learning with symbolic matrices.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Flum and Grohe(2006)]{flum2006parameterized}
J{\"o}rg Flum and Martin Grohe.
\newblock \emph{Parameterized Complexity Theory}.
\newblock Springer, 2006.

\bibitem[Frankle and Carbin(2018)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2019stabilizing}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock Stabilizing the lottery ticket hypothesis.
\newblock \emph{arXiv preprint arXiv:1903.01611}, 2019.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3259--3269. PMLR, 2020.

\bibitem[Goel et~al.(2022)Goel, Gu, Donahue, and R{\'e}]{goel2022s}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'e}.
\newblock It's raw! audio generation with state-space models.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Gokaslan et~al.(2019)Gokaslan, Cohen, Ellie, and
  Tellex]{Gokaslan2019OpenWeb}
Aaron Gokaslan, Vanya Cohen, Pavlick Ellie, and Stefanie Tellex.
\newblock Openwebtext corpus, 2019.

\bibitem[Gray et~al.(1997)Gray, Chaudhuri, Bosworth, Layman, Reichart,
  Venkatrao, Pellow, and Pirahesh]{gray1997data}
Jim Gray, Surajit Chaudhuri, Adam Bosworth, Andrew Layman, Don Reichart, Murali
  Venkatrao, Frank Pellow, and Hamid Pirahesh.
\newblock Data cube: A relational aggregation operator generalizing group-by,
  cross-tab, and sub-totals.
\newblock \emph{Data mining and knowledge discovery}, 1\penalty0 (1):\penalty0
  29--53, 1997.

\bibitem[Griewank and Walther(2008)]{griewank2008evaluating}
Andreas Griewank and Andrea Walther.
\newblock \emph{Evaluating derivatives: principles and techniques of
  algorithmic differentiation}.
\newblock SIAM, 2008.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2020.

\bibitem[Gu et~al.(2021)Gu, Johnson, Goel, Saab, Dao, Rudra, and
  R{\'e}]{gu2021combining}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and
  Christopher R{\'e}.
\newblock Combining recurrent, convolutional, and continuous-time models with
  linear state space layers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Gu et~al.(2022)Gu, Goel, and R\'e]{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R\'e.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2022.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William~J Dally.
\newblock Learning both weights and connections for efficient neural networks.
\newblock \emph{arXiv preprint arXiv:1506.02626}, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Hennessy and Patterson(2003)]{hennessy2003memory}
John Hennessy and David Patterson.
\newblock Memory hierarchy design.
\newblock \emph{Computer Architecture: A Quantitative Approach}, pages
  390--525, 2003.

\bibitem[Hooker(2020)]{hooker2020hardware}
Sara Hooker.
\newblock The hardware lottery.
\newblock \emph{arXiv preprint arXiv:2009.06489}, 2020.

\bibitem[Hua et~al.(2022)Hua, Dai, Liu, and Le]{hua2022transformer}
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc~V Le.
\newblock Transformer quality in linear time.
\newblock \emph{arXiv preprint arXiv:2202.10447}, 2022.

\bibitem[Ivanov et~al.(2021)Ivanov, Dryden, Ben-Nun, Li, and
  Hoefler]{ivanov2021data}
Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.
\newblock Data movement is all you need: A case study on optimizing
  transformers.
\newblock \emph{Proceedings of Machine Learning and Systems}, 3:\penalty0
  711--732, 2021.

\bibitem[Jia and Van~Sandt(2021)]{jia2021dissecting}
Zhe Jia and Peter Van~Sandt.
\newblock Dissecting the {A}mpere {GPU} architecture via microbenchmarking.
\newblock {GPU} Technology Conference, 2021.

\bibitem[Jia et~al.(2018)Jia, Maggioni, Staiger, and
  Scarpazza]{jia2018dissecting}
Zhe Jia, Marco Maggioni, Benjamin Staiger, and Daniele~P Scarpazza.
\newblock Dissecting the nvidia {V}olta {GPU} architecture via
  microbenchmarking.
\newblock \emph{arXiv preprint arXiv:1804.06826}, 2018.

\bibitem[Jia et~al.(2019)Jia, Tillman, Maggioni, and
  Scarpazza]{jia2019dissecting}
Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele~Paolo Scarpazza.
\newblock Dissecting the graphcore {IPU} architecture via microbenchmarking.
\newblock \emph{arXiv preprint arXiv:1912.03413}, 2019.

\bibitem[Johnson et~al.(2016)Johnson, Pollard, Shen, Lehman, Feng, Ghassemi,
  Moody, Szolovits, Anthony~Celi, and Mark]{johnson2016mimic}
Alistair~EW Johnson, Tom~J Pollard, Lu~Shen, Li-wei~H Lehman, Mengling Feng,
  Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony~Celi, and
  Roger~G Mark.
\newblock Mimic-iii, a freely accessible critical care database.
\newblock \emph{Scientific data}, 3\penalty0 (1):\penalty0 1--9, 2016.

\bibitem[Jouppi et~al.(2017)Jouppi, Young, Patil, Patterson, Agrawal, Bajwa,
  Bates, Bhatia, Boden, Borchers, et~al.]{jouppi2017datacenter}
Norman~P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
  Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al~Borchers, et~al.
\newblock In-datacenter performance analysis of a tensor processing unit.
\newblock In \emph{Proceedings of the 44th annual international symposium on
  computer architecture}, pages 1--12, 2017.

\bibitem[Kailath et~al.(1979)Kailath, Kung, and Morf]{kailath1979displacement}
Thomas Kailath, Sun-Yuan Kung, and Martin Morf.
\newblock Displacement ranks of matrices and linear equations.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 68\penalty0
  (2):\penalty0 395--407, 1979.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In \emph{The International Conference on Machine Learning ({ICML})},
  2020.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite {BEDRT} for self-supervised learning of language
  representations.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[Li et~al.(2020)Li, Liu, Liu, Sun, You, Yang, Luan, Gan, Yang, and
  Qian]{li2020deep}
Mingzhen Li, Yi~Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi
  Luan, Lin Gan, Guangwen Yang, and Depei Qian.
\newblock The deep learning compiler: A comprehensive survey.
\newblock \emph{IEEE Transactions on Parallel and Distributed Systems},
  32\penalty0 (3):\penalty0 708--727, 2020.

\bibitem[Likhosherstov et~al.(2020)Likhosherstov, Choromanski, Davis, Song, and
  Weller]{likhosherstov2020sub}
Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, and
  Adrian Weller.
\newblock Sub-linear memory: How to make performers slim.
\newblock \emph{arXiv preprint arXiv:2012.11346}, 2020.

\bibitem[Lin et~al.(2017)Lin, Rao, Lu, and Zhou]{NIPS2017_a51fb975}
Ji~Lin, Yongming Rao, Jiwen Lu, and Jie Zhou.
\newblock Runtime neural pruning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Ma et~al.(2021)Ma, Kong, Wang, Zhou, May, Ma, and
  Zettlemoyer]{ma2021luna}
Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and
  Luke Zettlemoyer.
\newblock Luna: Linear unified nested attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mattson et~al.(2020)Mattson, Cheng, Diamos, Coleman, Micikevicius,
  Patterson, Tang, Wei, Bailis, Bittorf, et~al.]{mattson2020mlperf}
Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius
  Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor
  Bittorf, et~al.
\newblock Mlperf training benchmark.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  336--349, 2020.

\bibitem[McSherry et~al.(2015)McSherry, Isard, and
  Murray]{mcsherry2015scalability}
Frank McSherry, Michael Isard, and Derek~G Murray.
\newblock Scalability! but at what $\{$COST$\}$?
\newblock In \emph{15th Workshop on Hot Topics in Operating Systems (HotOS
  XV)}, 2015.

\bibitem[Milakov and Gimelshein(2018)]{milakov2018online}
Maxim Milakov and Natalia Gimelshein.
\newblock Online normalizer calculation for softmax.
\newblock \emph{arXiv preprint arXiv:1805.02867}, 2018.

\bibitem[NVIDIA(2017)]{nvidia2017nvidia}
NVIDIA.
\newblock Nvidia {T}esla {V}100 {GPU} architecture, 2017.

\bibitem[NVIDIA(2020)]{nvidia2020nvidia}
NVIDIA.
\newblock Nvidia {A}100 tensor core {GPU} architecture, 2020.

\bibitem[NVIDIA(2022)]{nvidia2022nvidia}
NVIDIA.
\newblock Nvidia {H}100 tensor core {GPU} architecture, 2022.

\bibitem[Parker(1995)]{parker1995random}
D~Stott Parker.
\newblock Random butterfly transformations with applications in computational
  linear algebra.
\newblock 1995.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Rabe and Staats(2021)]{rabe2021self}
Markus~N Rabe and Charles Staats.
\newblock Self-attention does not need $ {O} (n^2) $ memory.
\newblock \emph{arXiv preprint arXiv:2112.05682}, 2021.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rae and Razavi(2020)]{rae-razavi-2020-transformers}
Jack Rae and Ali Razavi.
\newblock Do transformers need deep long-range memory?
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, Online, July 2020. Association for
  Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.acl-main.672}.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, and
  Lillicrap]{rae2019compressive}
Jack~W Rae, Anna Potapenko, Siddhant~M Jayakumar, and Timothy~P Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2020.

\bibitem[Ragan-Kelley et~al.(2013)Ragan-Kelley, Barnes, Adams, Paris, Durand,
  and Amarasinghe]{ragan2013halide}
Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Fr{\'e}do
  Durand, and Saman Amarasinghe.
\newblock Halide: a language and compiler for optimizing parallelism, locality,
  and recomputation in image processing pipelines.
\newblock \emph{Acm Sigplan Notices}, 48\penalty0 (6):\penalty0 519--530, 2013.

\bibitem[Ramakrishnan et~al.(2003)Ramakrishnan, Gehrke, and
  Gehrke]{ramakrishnan2003database}
Raghu Ramakrishnan, Johannes Gehrke, and Johannes Gehrke.
\newblock \emph{Database management systems}, volume~3.
\newblock McGraw-Hill New York, 2003.

\bibitem[Recht and R{\'e}(2013)]{recht2013parallel}
Benjamin Recht and Christopher R{\'e}.
\newblock Parallel stochastic gradient algorithms for large-scale matrix
  completion.
\newblock \emph{Mathematical Programming Computation}, 5\penalty0 (2):\penalty0
  201--226, 2013.

\bibitem[Ren et~al.(2021)Ren, Dai, Dai, Yang, Leskovec, Schuurmans, and
  Dai]{ren2021combiner}
Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale
  Schuurmans, and Bo~Dai.
\newblock Combiner: Full attention transformer with sparse computation cost.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Roy et~al.(2021)Roy, Saffar, Vaswani, and Grangier]{roy2021efficient}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 53--68, 2021.

\bibitem[Sabne(2020)]{sabne2020xla}
Amit Sabne.
\newblock {XLA}: Compiling machine learning for peak performance.
\newblock 2020.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{sanh2020movement}
Victor Sanh, Thomas Wolf, and Alexander~M Rush.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock \emph{arXiv preprint arXiv:2005.07683}, 2020.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-{LM}: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Sindhwani et~al.(2015)Sindhwani, Sainath, and
  Kumar]{sindhwani2015structured}
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar.
\newblock Structured transforms for small-footprint deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3088--3096, 2015.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and
  Joulin]{sukhbaatar2019adaptive}
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for
  Computational Linguistics}, 2019.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Bahri, and
  Metzler]{tay2020efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Ma, Dong, Huang, Zhang, and
  Wei]{wang2022deepnet}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
\newblock Deepnet: Scaling transformers to 1,000 layers.
\newblock \emph{arXiv preprint arXiv:2203.00555}, 2022.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Williams et~al.(2009)Williams, Waterman, and
  Patterson]{williams2009roofline}
Samuel Williams, Andrew Waterman, and David Patterson.
\newblock Roofline: an insightful visual performance model for multicore
  architectures.
\newblock \emph{Communications of the ACM}, 52\penalty0 (4):\penalty0 65--76,
  2009.

\bibitem[Wolf and Lam(1991)]{wolf1991data}
Michael~E Wolf and Monica~S Lam.
\newblock A data locality optimizing algorithm.
\newblock In \emph{Proceedings of the ACM SIGPLAN 1991 conference on
  Programming language design and implementation}, pages 30--44, 1991.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Woodruff(2004)]{woodruff2004optimal}
David~P Woodruff.
\newblock Optimal space lower bounds for all frequency moments.
\newblock In \emph{SODA}, volume~4, pages 167--175. Citeseer, 2004.

\bibitem[Wu et~al.(2019)Wu, Fan, Baevski, Dauphin, and Auli]{wu2019pay}
Felix Wu, Angela Fan, Alexei Baevski, Yann~N Dauphin, and Michael Auli.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2019.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and
  Singh]{xiong2021nystromformer}
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung,
  Yin Li, and Vikas Singh.
\newblock Nystr{\"o}mformer: A nyst{\"o}m-based algorithm for approximating
  self-attention.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence. AAAI Conference on Artificial Intelligence}, volume~35, page
  14138, 2021.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Wang, Yu, Shi, Jiang, Tay, Feng, and
  Yan]{yuan2021tokens}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
  Francis~EH Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 558--567, 2021.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020bigbird}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zhai et~al.(2021)Zhai, Talbott, Srivastava, Huang, Goh, Zhang, and
  Susskind]{zhai2021attention}
Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh,
  Ruixiang Zhang, and Josh Susskind.
\newblock An attention free transformer.
\newblock \emph{arXiv preprint arXiv:2105.14103}, 2021.

\bibitem[Zhu et~al.(2021)Zhu, Ping, Xiao, Shoeybi, Goldstein, Anandkumar, and
  Catanzaro]{zhu2021long}
Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima
  Anandkumar, and Bryan Catanzaro.
\newblock Long-short transformer: Efficient transformers for language and
  vision.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\end{thebibliography}
