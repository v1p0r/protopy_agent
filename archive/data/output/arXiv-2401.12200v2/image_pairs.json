{"0": {"image": "figures/APT_arch.pdf", "caption": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{figures/APT_arch.pdf}\n    \\caption{{\\ourmethod} adaptively identifies pruning and tuning parameters via {\\ourarchabbr}s during fine-tuning with little cost. {\\ourmethod} gradually prunes {\\lmabbr} parameters with binary pruning masks learned from our lightweight outlier-aware salience scoring function for training and inference efficiency. {\\ourmethod} also adds tuning parameters in salient layers in {\\lmabbr} fine-tuning through increasing dynamic ranks in {\\ourarchabbr}s for performance recovery.}\n    \\label{fig:elastic-adapter}\n\\vspace{-15pt}\n\\end{figure*}"}, "1": {"image": "figures/teaser_figure_revised.pdf", "caption": "\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figures/teaser_figure_revised.pdf}\n\\caption{{\\ourmethod} provides both training and inference efficiency benefits by pruning and tuning pretrained LM parameters adaptively via the \\textbf{{\\ourmethod} adapter}. We dynamically adjust (add/reduce) {\\ourmethod} adapter input/output dimensions and the rank ($r_{\\text{apt}}$). Reducing adapter dimensions prunes frozen parameters, making training and inference faster and more memory-efficient. Adding adapter ranks helps recover the pruned {\\lmabbr}'s task performance. In contrast, existing adapters like LoRA allow efficient training but do not provide inference efficiency since the model size is not reduced.}\n    \\label{fig:teaser}\n    \\vspace{-15pt}\n\\end{figure}"}, "2": {"image": "figures/pruning_tradeoff.pdf", "caption": "\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.8\\columnwidth]{figures/pruning_tradeoff.pdf}\n    \\caption{Task performance v.s. relative inference efficiency on RoBERTa, T5, and LLaMA-2 7B models with {\\ourmethod} and baselines.}\n    \\label{fig:prune-tradeoff}\n    \\vspace{-10pt}\n\\end{figure}"}, "3": {"image": "figures/plot_tradeoff_scatter.pdf", "caption": "\\begin{figure*}[t!]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/plot_tradeoff_scatter.pdf}\n    \\caption{The performance-efficiency tradeoff of {\\ourmethod} compared to baseline methods. All metrics are normalized using LoRA tuning w/o pruning as the baseline. The circular dots with vertical axes on the left indicate training speed v.s. performance, with their sizes denoting the peak training memory usage. The squared dots with axes on the right indicate inference speedup v.s. performance, with sizes denoting inference memory usage.}\n    \\label{fig:perf-efficiency-tradeoff}\n    \\vspace{-10pt}\n\\end{figure*}"}, "4": {"image": "figures/tuning_tradeoff.pdf", "caption": "\\begin{figure}[ht!]\n  \\centering\n  \n  \n  \n  \n  \n  \\begin{minipage}[b]{\\textwidth}\n    \\subfloat[Comparison of different initial ranks of LoRA layers pruning with {\\ourmethod} on RoBERTa with SST2 task accuracy, relative training peak memory and speed to 97\\% fine-tuning accuracy to the fine-tuning model.]{\n    \\includegraphics[width=0.45\\columnwidth]{figures/tuning_tradeoff.pdf}\n    \\label{fig:rank-tradeoff}\n    }\n    \\hfill\n    \\subfloat[Training initial sparsity trade-off with 30\\% target sparsity model's relative performances to the LoRA-tuned LLaMA2-7B and 13B models.]{\n        \\includegraphics[width=0.45\\columnwidth]{figures/llama_init_density.pdf}\n        \\label{fig:llama-init-tradeoff}\n    }\n  \\end{minipage}"}}