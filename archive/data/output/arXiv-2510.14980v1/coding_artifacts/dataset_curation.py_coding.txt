```python
# dataset/curation.py
import json
import os
import time
import logging
from typing import List, Dict, Any, Optional, Tuple
from utils.config import Config
from utils.logger import Logger
from agent.single_agent import SingleAgent
from representation.construction_tree import ConstructionTree
from env.besiegefield import BesiegeFieldSimulator
from reward.calculator import RewardCalculator
from utils.validator import JSONValidator
from env.block_registry import BlockRegistry


class DatasetCurator:
    """
    Curates the cold-start dataset for RL fine-tuning by generating and filtering
    machine designs from Gemini 2.5 Pro using the SingleAgent workflow.
    Produces exactly 9,984 valid (prompt, CoT, construction_tree) triples as specified in the paper.
    """

    def __init__(self, config: Config = None):
        """
        Initialize the DatasetCurator with configuration and required components.
        
        Args:
            config (Config, optional): Configuration loader. If None, loads from "config.yaml"
        """
        self.config = config or Config("config.yaml")
        self.logger = Logger(__name__)
        
        # Load configuration values
        self.prompt_file = self.config.get("dataset.prompt_file")
        self.output_file = self.config.get("dataset.cold_start_dataset_path")
        self.num_prompts = self.config.get("dataset.num_test_prompts", 100)
        self.generation_per_prompt = 250
        self.simulation_duration = self.config.get("simulation.duration_seconds", 5.0)
        self.state_log_interval = self.config.get("simulation.state_log_interval", 0.2)
        self.collision_threshold = self.config.get("simulation.collision_threshold", 0.01)
        self.catapult_height_threshold = self.config.get("simulation.catapult_height_threshold", 3.0)
        self.min_displacement_threshold = 0.01  # Minimum displacement for physics-driven functionality
        
        # Validate configuration
        if not isinstance(self.prompt_file, str) or not os.path.exists(self.prompt_file):
            raise FileNotFoundError(f"Prompt file not found: {self.prompt_file}")
        if not isinstance(self.output_file, str):
            raise ValueError("dataset.cold_start_dataset_path must be a string")
        if not isinstance(self.generation_per_prompt, int) or self.generation_per_prompt <= 0:
            raise ValueError("generation_per_prompt must be a positive integer")
        if not isinstance(self.num_prompts, int) or self.num_prompts <= 0:
            raise ValueError("dataset.num_test_prompts must be a positive integer")
        if not isinstance(self.simulation_duration, (int, float)) or self.simulation_duration <= 0:
            raise ValueError("simulation.duration_seconds must be a positive number")
        if not isinstance(self.state_log_interval, (int, float)) or self.state_log_interval <= 0:
            raise ValueError("simulation.state_log_interval must be a positive number")
        if not isinstance(self.collision_threshold, (int, float)) or self.collision_threshold <= 0:
            raise ValueError("simulation.collision_threshold must be a positive number")
        if not isinstance(self.min_displacement_threshold, (int, float)) or self.min_displacement_threshold < 0:
            raise ValueError("min_displacement_threshold must be a non-negative number")
            
        # Initialize components
        self.block_registry = BlockRegistry()
        self.json_validator = JSONValidator()
        
        # Initialize SingleAgent with Gemini 2.5 Pro (as specified in paper)
        # Note: In a real implementation, this would use an actual API call to Gemini
        # For this reproduction, we assume SingleAgent is properly implemented to interface with Gemini
        self.single_agent = SingleAgent(
            llm_model="gemini-2.5-pro",
            prompt_template=None  # Use default template from SingleAgent
        )
        
        # Initialize simulation environment for validation
        # Use a minimal block list (27 blocks) as specified in paper
        self.simulator = BesiegeFieldSimulator(
            block_list=list(self.block_registry._valid_block_names),
            physics_config={
                "duration_seconds": self.simulation_duration,
                "state_log_interval": self.state_log_interval,
                "gravity": 9.81,
                "collision_threshold": self.collision_threshold,
                "catapult_height_threshold": self.catapult_height_threshold
            }
        )
        
        # Initialize reward calculator for physics-driven functionality check
        self.reward_calculator = RewardCalculator(
            task="car",  # Will be overridden per prompt in compute()
            catapult_height_threshold=self.catapult_height_threshold
        )
        
        # Ensure output directory exists
        output_dir = os.path.dirname(self.output_file)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        # Initialize counters
        self.total_generated = 0
        self.file_valid = 0
        self.spatial_valid = 0
        self.physics_valid = 0
        self.rejected_reasons = {
            "file_invalid": 0,
            "spatial_collision": 0,
            "no_physics_functionality": 0,
            "other": 0
        }

    def _load_prompts(self) -> List[str]:
        """
        Load prompts from the JSONL file specified in config.
        
        Returns:
            List[str]: List of prompt strings
            
        Raises:
            ValueError: If prompts file is malformed or doesn't contain exactly 100 prompts
        """
        prompts = []
        with open(self.prompt_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    prompt_obj = json.loads(line)
                    if not isinstance(prompt_obj, dict) or "prompt" not in prompt_obj:
                        raise ValueError(f"Line {line_num}: Each line must be a JSON object with 'prompt' key")
                    prompt_str = prompt_obj["prompt"]
                    if not isinstance(prompt_str, str) or not prompt_str.strip():
                        raise ValueError(f"Line {line_num}: 'prompt' must be a non-empty string")
                    prompts.append(prompt_str.strip())
                except json.JSONDecodeError as e:
                    raise ValueError(f"Line {line_num}: Invalid JSON format: {str(e)}")
                except Exception as e:
                    raise ValueError(f"Line {line_num}: Unexpected error: {str(e)}")
                    
        if len(prompts) != self.num_prompts:
            raise ValueError(f"Expected {self.num_prompts} prompts, but found {len(prompts)} in {self.prompt_file}")
            
        self.logger.info(f"Loaded {len(prompts)} prompts from {self.prompt_file}")
        return prompts

    def _is_physics_driven(self, state_log: List[Dict[str, Any]], task: str) -> bool:
        """
        Determine if a machine exhibits physics-driven functionality (not a statue).
        Uses displacement and velocity thresholds to detect meaningful motion.
        
        Args:
            state_log (List[Dict[str, Any]]): Simulation state log from BesiegeFieldSimulator
            task (str): Task type ("car" or "catapult")
            
        Returns:
            bool: True if machine exhibits physics-driven functionality, False otherwise
        """
        if not state_log or len(state_log) == 0:
            return False
            
        # Check if any block has non-zero velocity in any timestep
        has_motion = False
        for timestep in state_log:
            for block in timestep.get("blocks", []):
                velocity = block.get("velocity", [0.0, 0.0, 0.0])
                angular_velocity = block.get("angular_velocity", [0.0, 0.0, 0.0])
                
                # Check if any velocity component exceeds threshold
                if any(abs(v) > 0.001 for v in velocity) or any(abs(av) > 0.001 for av in angular_velocity):
                    has_motion = True
                    break
                    
            if has_motion:
                break
                
        if not has_motion:
            return False
            
        # For car task: check root block displacement
        if task == "car":
            root_blocks = [timestep for timestep in state_log if any(
                block.get("type") == "Starting Block" for block in timestep.get("blocks", [])
            )]
            
            if len(root_blocks) == 0:
                return False
                
            # Get initial and final positions
            initial_pos = root_blocks[0]["blocks"][0]["position"]
            final_pos = root_blocks[-1]["blocks"][0]["position"]
            
            # Calculate displacement along x-axis (designated forward direction)
            displacement = abs(final_pos[0] - initial_pos[0])
            
            # Must have minimum displacement to be considered functional
            return displacement >= self.min_displacement_threshold
            
        # For catapult task: check boulder displacement
        elif task == "catapult":
            boulder_blocks = [timestep for timestep in state_log if any(
                block.get("type") == "Boulder" for block in timestep.get("blocks", [])
            )]
            
            if len(boulder_blocks) == 0:
                return False
                
            # Get initial and final positions of boulder
            initial_pos = boulder_blocks[0]["blocks"][0]["position"]
            final_pos = boulder_blocks[-1]["blocks"][0]["position"]
            
            # Calculate displacement along x-axis
            displacement = abs(final_pos[0] - initial_pos[0])
            
            # Check if boulder reached significant height (not just lifted slightly)
            max_height = max(
                block.get("position", [0, 0, 0])[1] 
                for timestep in state_log 
                for block in timestep.get("blocks", []) 
                if block.get("type") == "Boulder"
            )
            
            # Must have minimum displacement AND reasonable height to be considered functional
            return displacement >= self.min_displacement_threshold and max_height >= 0.1
            
        return False

    def _simulate_and_validate(self, tree: ConstructionTree, task: str) -> Tuple[bool, bool, List[Dict[str, Any]], str]:
        """
        Simulate a machine design and validate its validity and functionality.
        
        Args:
            tree (ConstructionTree): Validated construction tree
            task (str): Task type ("car" or "catapult")
            
        Returns:
            Tuple[bool, bool, List[Dict[str, Any]], str]: 
                (spatial_valid: bool, physics_driven: bool, state_log: List[Dict], error_msg: str)
        """
        try:
            # Build machine in simulator
            build_success = self.simulator.build_from_tree(tree)
            if not build_success:
                return False, False, [], "Failed to build machine in simulator"
                
            # Check for self-collision
            has_collision = not self.simulator.check_self_collision()
            if has_collision:
                return False, False, [], "Self-collision detected before simulation"
                
            # Run simulation
            self.simulator.simulate()
            state_log = self.simulator.get_state_log()
            
            # Check if machine is intact throughout simulation
            intact = True
            for timestep in state_log:
                for block in timestep.get("blocks", []):
                    if block.get("integrity", 1.0) < 0.1:
                        intact = False
                        break
                if not intact:
                    break
                    
            if not intact:
                return True, False, state_log, "Machine broke during simulation"
                
            # Check physics-driven functionality
            physics_driven = self._is_physics_driven(state_log, task)
            
            return True, physics_driven, state_log, ""
            
        except Exception as e:
            return False, False, [], f"Simulation error: {str(e)}"

    def generate_and_filter(self) -> List[Dict[str, Any]]:
        """
        Generate 250 machine designs per prompt using Gemini 2.5 Pro and filter for validity.
        Returns exactly 9,984 valid (prompt, CoT, machine) triples.
        
        Returns:
            List[Dict[str, Any]]: List of valid samples, each with keys: "prompt", "cot", "machine"
        """
        self.logger.info("Starting dataset curation process")
        start_time = time.time()
        
        # Load prompts
        prompts = self._load_prompts()
        
        valid_samples = []
        
        # Process each prompt
        for prompt_idx, prompt in enumerate(prompts, 1):
            self.logger.info(f"Processing prompt {prompt_idx}/{len(prompts)}: '{prompt[:50]}...'")

            # Generate 250 samples per prompt
            for gen_idx in range(self.generation_per_prompt):
                self.total_generated += 1
                
                # Generate machine design using SingleAgent
                try:
                    full_response, tree, success = self.single_agent.generate(prompt)
                except Exception as e:
                    self.logger.error(f"SingleAgent failed for prompt {prompt_idx}, generation {gen_idx + 1}: {str(e)}")
                    self.rejected_reasons["other"] += 1
                    continue
                    
                # Track file validity
                if not success:
                    self.rejected_reasons["file_invalid"] += 1
                    continue
                    
                self.file_valid += 1
                
                # Validate construction tree structure
                try:
                    is_valid, error_msg = tree.validate()
                    if not is_valid:
                        self.rejected_reasons["file_invalid"] += 1
                        self.logger.debug(f"File invalid for prompt {prompt_idx}, gen {gen_idx + 1}: {error_msg}")
                        continue
                except Exception as e:
                    self.rejected_reasons["file_invalid"] += 1
                    self.logger.debug(f"Tree validation failed for prompt {prompt_idx}, gen {gen_idx + 1}: {str(e)}")
                    continue
                    
                # Simulate and validate spatial and physics validity
                # Determine task type from prompt content (heuristic)
                task = "catapult" if any(keyword in prompt.lower() for keyword in ["throw", "boulder", "launch", "catapult"]) else "car"
                
                spatial_valid, physics_driven, state_log, error_msg = self._simulate_and_validate(tree, task)
                
                if not spatial_valid:
                    self.rejected_reasons["spatial_collision"] += 1
                    self.logger.debug(f"Spatial invalid for prompt {prompt_idx}, gen {gen_idx + 1}: {error_msg}")
                    continue
                    
                self.spatial_valid += 1
                
                if not physics_driven:
                    self.rejected_reasons["no_physics_functionality"] += 1
                    self.logger.debug(f"No physics functionality for prompt {prompt_idx}, gen {gen_idx + 1}: {error_msg}")
                    continue
                    
                self.physics_valid += 1
                
                # Extract CoT from full response
                # The CoT is everything before the JSON array
                # We need to find the first occurrence of '[' and take everything before it
                # This is based on the SingleAgent's output format: CoT in triple backticks followed by JSON
                json_start = full_response.find('```json')
                if json_start == -1:
                    json_start = full_response.find('```')
                    
                if json_start != -1:
                    cot = full_response[:json_start].strip()
                else:
                    # Fallback: if no code block, use entire response
                    cot = full_response.strip()
                    
                # Clean CoT: remove any leading/trailing whitespace and empty lines
                cot_lines = [line.strip() for line in cot.split('\n') if line.strip()]
                cot = '\n'.join(cot_lines)
                
                # Extract machine as JSON list
                machine_json = tree.to_json()
                
                # Create sample
                sample = {
                    "prompt": prompt,
                    "cot": cot,
                    "machine": machine_json
                }
                
                valid_samples.append(sample)
                
                # Log progress
                if len(valid_samples) % 100 == 0:
                    self.logger.info(f"Valid samples collected: {len(valid_samples)}/9984")
                    
                # Early termination if we have enough samples
                if len(valid_samples) >= 9984:
                    break
                    
            # Early termination if we have enough samples
            if len(valid_samples) >= 9984:
                break
                
        # Final validation
        if len(valid_samples) < 9984:
            self.logger.warning(f"Only {len(valid_samples)} valid samples generated, expected 9984")
        else:
            # Truncate to exactly 9984
            valid_samples = valid_samples[:9984]
            self.logger.info(f"Truncated to exactly 9984 valid samples")
            
        # Log summary
        end_time = time.time()
        self.logger.info(f"Dataset curation completed in {end_time - start_time:.2f} seconds")
        self.logger.info(f"Total generated: {self.total_generated}")
        self.logger.info(f"File valid: {self.file_valid}")
        self.logger.info(f"Spatial valid: {self.spatial_valid}")
        self.logger.info(f"Physics driven: {self.physics_valid}")
        self.logger.info(f"Rejected reasons: {self.rejected_reasons}")
        
        # Write to output file
        self._write_output(valid_samples)
        
        return valid_samples

    def _write_output(self, samples: List[Dict[str, Any]]) -> None:
        """
        Write the valid samples to the output file in JSONL format.
        
        Args:
            samples (List[Dict[str, Any]]): List of valid samples
        """
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                for sample in samples:
                    f.write(json.dumps(sample, ensure_ascii=False) + '\n')
                    
            self.logger.info(f"Successfully wrote {len(samples)} samples to {self.output_file}")
            
            # Verify output file
            with open(self.output_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if len(lines) != len(samples):
                    raise ValueError(f"Output file has {len(lines)} lines, expected {len(samples)}")
                    
            self.logger.info(f"Verified output file: {len(lines)} lines written")
            
        except Exception as e:
            self.logger.error(f"Failed to write output file {self.output_file}: {str(e)}")
            raise

    def validate_output(self) -> bool:
        """
        Validate the output file by re-reading and checking structure.
        
        Returns:
            bool: True if output is valid, False otherwise
        """
        try:
            with open(self.output_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            if len(lines) != 9984:
                self.logger.error(f"Output file has {len(lines)} lines, expected 9984")
                return False
                
            for i, line in enumerate(lines):
                try:
                    sample = json.loads(line.strip())
                    if not isinstance(sample, dict):
                        self.logger.error(f"Line {i+1}: Not a JSON object")
                        return False
                    if "prompt" not in sample or "cot" not in sample or "machine" not in sample:
                        self.logger.error(f"Line {i+1}: Missing required keys")
                        return False
                    if not isinstance(sample["prompt"], str) or not sample["prompt"].strip():
                        self.logger.error(f"Line {i+1}: Invalid prompt")
                        return False
                    if not isinstance(sample["cot"], str) or not sample["cot"].strip():
                        self.logger.error(f"Line {i+1}: Invalid CoT")
                        return False
                    if not isinstance(sample["machine"], list):
                        self.logger.error(f"Line {i+1}: Machine must be a list")
                        return False
                    # Validate machine structure
                    if len(sample["machine"]) == 0:
                        self.logger.error(f"Line {i+1}: Machine has no blocks")
                        return False
                    # Validate first block is Starting Block with id=0
                    if sample["machine"][0].get("type") != "Starting Block" or sample["machine"][0].get("id") != 0:
                        self.logger.error(f"Line {i+1}: First block must be Starting Block with id=0")
                        return False
                except json.JSONDecodeError:
                    self.logger.error(f"Line {i+1}: Invalid JSON")
                    return False
                    
            self.logger.info("Output file validated successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Output validation failed: {str(e)}")
            return False

```