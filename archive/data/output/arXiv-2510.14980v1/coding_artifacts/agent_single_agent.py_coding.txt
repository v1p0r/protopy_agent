```python
# agent/single_agent.py
import json
import re
from typing import Optional, List, Dict, Any
from utils.config import Config
from utils.logger import Logger
from env.block_registry import BlockRegistry
from representation.construction_tree import ConstructionTree
from utils.validator import JSONValidator


class SingleAgent:
    """
    Single-agent for compositional machine design: generates a machine design in one shot
    from a natural language task prompt using Chain-of-Thought (CoT) reasoning.
    Implements the baseline agent workflow described in the paper.
    """

    def __init__(self, llm_model: str, prompt_template: Optional[str] = None):
        """
        Initialize the SingleAgent with LLM model identifier and optional custom prompt template.
        
        Args:
            llm_model (str): Identifier for the LLM to use (e.g., "gemini-2.5-pro", "qwen-2.5-14b")
            prompt_template (str, optional): Custom prompt template. If None, uses default template from paper.
        """
        self.llm_model = llm_model
        self.logger = Logger(__name__)
        self.config = Config("config.yaml")
        
        # Get block list from registry (27 blocks as per paper)
        self.block_registry = BlockRegistry()
        self.block_list = sorted(list(self.block_registry._valid_block_names))
        
        # Default prompt template as specified in paper's "Single-agent setting"
        self.prompt_template = prompt_template or self._build_default_prompt_template()
        
        # LLM API parameters (hardcoded per paper's cold-start use case)
        self.max_output_length = self.config.get("model.max_output_length", 1168)
        self.temperature = 0.0  # Deterministic generation for dataset curation
        self.top_p = 1.0        # Full sampling space
        
        # Validate template contains required placeholders
        if "{task}" not in self.prompt_template:
            raise ValueError("Prompt template must contain '{task}' placeholder for task injection")
        if "{block_list}" not in self.prompt_template:
            raise ValueError("Prompt template must contain '{block_list}' placeholder for block list injection")

    def _build_default_prompt_template(self) -> str:
        """
        Build the default prompt template as specified in the paper's "Single-agent setting".
        This template enforces Chain-of-Thought reasoning and strict JSON output format.
        
        Returns:
            str: Formatted prompt template with placeholders
        """
        # Get block list as comma-separated string
        block_list_str = ", ".join([f'"{block}"' for block in self.block_list])
        
        return f"""You are an expert mechanical designer. Your task is to build a machine using only the following 27 blocks: {block_list_str}.

Construction Rules:
- Start with the "Starting Block" (ID=0). Every other block must be attached to an existing block.
- Each block has exactly one attachable face, except "Spring", which can attach to two blocks.
- Blocks cannot be scaled or rotated after attachment.
- Output must be a JSON list of block dictionaries in construction order.
- Each block must include: "type", "id", "parent", "face_id". For "Spring", use "parent_a", "parent_b", "face_id_a", "face_id_b".
- First block must be: {{"type": "Starting Block", "id": 0, "parent": null, "face_id": null}}.
- Do not include any text outside the JSON list.

Task: {{task}}

First, reason step-by-step about the design in a Chain-of-Thought (CoT) inside triple backticks:
```
Step 1: I need a base... Step 2: Attach wheels for mobility... Step 3: Add a lever to launch...
```
Then output the JSON construction tree below, with no other text:
"""

    def generate(self, task: str) -> Optional[ConstructionTree]:
        """
        Generate a machine design from a natural language task description.
        
        Steps:
        1. Format prompt with task and block list
        2. Call LLM API to generate response with CoT and JSON
        3. Extract and validate JSON output
        4. Return ConstructionTree if valid, None otherwise
        
        Args:
            task (str): Natural language task description (e.g., "Build a machine to throw a boulder as far as possible")
            
        Returns:
            ConstructionTree or None: Valid machine design or None if generation/validation failed
        """
        if not isinstance(task, str) or not task.strip():
            self.logger.error("SingleAgent.generate: Task must be a non-empty string")
            return None

        # Format prompt with task and block list
        prompt = self.prompt_template.format(task=task, block_list=", ".join(self.block_list))
        
        # Call LLM API (simulated here; in practice, this would call Gemini/Qwen API)
        try:
            llm_response = self._call_llm_api(prompt)
        except Exception as e:
            self.logger.error(f"SingleAgent.generate: Failed to call LLM API for task '{task[:50]}...': {str(e)}")
            return None
        
        # Extract JSON from LLM response
        try:
            json_str = self._extract_json_from_response(llm_response)
        except Exception as e:
            self.logger.error(f"SingleAgent.generate: Failed to extract JSON from LLM response for task '{task[:50]}...': {str(e)}")
            return None
        
        # Validate JSON structure
        is_valid, error_msg = JSONValidator().validate_construction_tree(json_str)
        if not is_valid:
            self.logger.error(f"SingleAgent.generate: Invalid JSON structure for task '{task[:50]}...': {error_msg}")
            return None
        
        # Parse JSON into list of dictionaries
        try:
            json_data = json.loads(json_str)
        except json.JSONDecodeError as e:
            self.logger.error(f"SingleAgent.generate: Failed to parse JSON for task '{task[:50]}...': {str(e)}")
            return None
        
        # Validate that json_data is a list
        if not isinstance(json_data, list):
            self.logger.error(f"SingleAgent.generate: JSON output must be a list, got {type(json_data).__name__}")
            return None
        
        # Create and validate ConstructionTree
        try:
            machine = ConstructionTree(json_data, self.block_registry)
        except Exception as e:
            self.logger.error(f"SingleAgent.generate: Failed to instantiate ConstructionTree for task '{task[:50]}...': {str(e)}")
            return None
        
        # Validate ConstructionTree structure
        is_tree_valid, tree_error = machine.validate()
        if not is_tree_valid:
            self.logger.error(f"SingleAgent.generate: ConstructionTree validation failed for task '{task[:50]}...': {tree_error}")
            return None
        
        # Log successful generation
        self.logger.debug(f"SingleAgent.generate: Successfully generated machine for task: {task[:100]}...")
        return machine

    def _call_llm_api(self, prompt: str) -> str:
        """
        Simulate calling an LLM API (in practice, this would interface with Gemini, Qwen, etc.).
        For reproduction, this is a placeholder - actual implementation would use external API.
        
        Args:
            prompt (str): Formatted prompt with task and block list
            
        Returns:
            str: LLM response text containing CoT and JSON output
            
        Note: In real implementation, this would use:
            - Google AI API for Gemini 2.5 Pro
            - Hugging Face API for Qwen2.5-14B-Instruct
            - With appropriate authentication and rate limiting
        """
        # This is a placeholder for the actual LLM API call.
        # In production, this would be replaced with actual API calls.
        # For testing and reproduction, we could use a mock or a local model.
        # Since the paper uses Gemini 2.5 Pro for cold-start, we assume it's available.
        
        # For the purpose of this reproduction, we'll return a mock response
        # that matches the expected format. In a real system, this would be replaced.
        
        # IMPORTANT: This is a placeholder. In a real implementation, you would:
        # 1. Use the correct LLM model (e.g., Gemini 2.5 Pro via Google AI Platform)
        # 2. Set max_tokens=self.max_output_length
        # 3. Set temperature=self.temperature
        # 4. Set top_p=self.top_p
        # 5. Handle API keys and rate limiting
        
        # Example mock response that matches paper's format
        # This is just for testing - in reality, this would be the LLM's actual output
        return f"""Here is my design for the task: {prompt.split("Task: ")[1].split("\n")[0]}
```
Step 1: I need a base structure for the machine.
Step 2: I'll attach four wheels for mobility - two on each side.
Step 3: I'll add a lever arm connected to a boulder to create a catapult.
Step 4: The lever will be attached to a rotating block to provide the force.
```
[{"type": "Starting Block", "id": 0, "parent": null, "face_id": null}, {"type": "Powered Wheel", "id": 1, "parent": 0, "face_id": 4}, {"type": "Powered Wheel", "id": 2, "parent": 0, "face_id": 5}, {"type": "Powered Wheel", "id": 3, "parent": 0, "face_id": 2}, {"type": "Powered Wheel", "id": 4, "parent": 0, "face_id": 3}, {"type": "Rotating Block", "id": 5, "parent": 0, "face_id": 0}, {"type": "Wooden Block", "id": 6, "parent": 5, "face_id": 0}, {"type": "Boulder", "id": 7, "parent": 6, "face_id": 4}]
"""

    def _extract_json_from_response(self, response: str) -> str:
        """
        Extract the JSON construction tree from LLM response.
        Looks for the first JSON array after the CoT block (triple backticks).
        
        Args:
            response (str): Raw LLM response text
            
        Returns:
            str: Extracted JSON string
            
        Raises:
            ValueError: If JSON cannot be extracted
        """
        # Find the CoT block (text between triple backticks)
        cot_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
        cot_matches = re.findall(cot_pattern, response)
        
        # If we found a CoT block, remove it from response and look for JSON after
        if cot_matches:
            # Remove the CoT block from response
            response_without_cot = re.sub(cot_pattern, '', response)
        else:
            # No CoT block found, use entire response
            response_without_cot = response
        
        # Look for JSON array at the end of the response
        # Pattern for JSON array: starts with [ and ends with ]
        json_pattern = r'(\[.*\])'
        json_matches = re.findall(json_pattern, response_without_cot, re.DOTALL)
        
        if not json_matches:
            raise ValueError("No JSON array found in LLM response")
        
        # Take the first (and presumably only) JSON array
        json_str = json_matches[0].strip()
        
        # Validate that it's a complete JSON array
        if not json_str.startswith('[') or not json_str.endswith(']'):
            raise ValueError("Extracted content is not a valid JSON array")
        
        return json_str

```