### **Logic Analysis: dataset/loader.py â€” DatasetLoader Class**

---

#### **1. Purpose and Role in System**
The `DatasetLoader` class is a **critical data access layer** that bridges the curated cold-start dataset (generated by `DatasetCurator`) and downstream components: `RLTrainer` (for finetuning) and `EvaluationMetrics` (for benchmarking). Its sole responsibility is to **load, validate, and partition** the 9,984 valid (prompt, CoT, machine) triples into **training** (9,884 samples) and **test** (100 samples) splits, ensuring **strict non-overlap** as required by the paperâ€™s experimental protocol.

This module does **not** generate data â€” it only reads and structures it. It must be **robust, deterministic, and schema-aware**, since any corruption or misalignment will invalidate RL training and evaluation results.

---

#### **2. Input Data Format (JSONL Schema)**
The input file (`cold_start_dataset_path` from `config.yaml`) is a **JSON Lines (JSONL)** file, where each line is a standalone JSON object with the following **strict schema**:

```json
{
  "prompt": "Build a machine to throw a boulder as far as possible",
  "cot": "First, I need a base platform... then attach a lever to a container...",
  "machine": [
    {
      "type": "Starting Block",
      "id": 0,
      "parent": null,
      "face_id": null
    },
    {
      "type": "Wooden Block",
      "id": 1,
      "parent": 0,
      "face_id": 0
    },
    {
      "type": "Powered Wheel",
      "id": 2,
      "parent": 1,
      "face_id": 1
    },
    ...
  ]
}
```

**Schema Constraints (from paper + design):**
- `"prompt"`: string, natural language task description.
- `"cot"`: string, chain-of-thought reasoning (from Gemini 2.5 Pro).
- `"machine"`: list of dictionaries, **must conform to `ConstructionTree` schema** (validated by `JSONValidator`).
  - Each block has `"type"`, `"id"` (0-indexed, unique, sequential), `"parent"` (int or null), `"face_id"` (int).
  - For Spring: `"parent_a"`, `"parent_b"`, `"face_id_a"`, `"face_id_b"` are used instead of `"parent"`/`"face_id"`.
  - `"id"` must match list index (i.e., `machine[i]["id"] == i`).
  - First block must be `"Starting Block"` with `"id": 0`, `"parent": null`.
  - No cycles, no invalid parent IDs, no missing required fields.

> âœ… **Critical**: The `"machine"` field is **not** raw XML or global coordinates â€” it is already in **ConstructionTree format**, as produced by `DatasetCurator` using Gemini 2.5 Pro and validated by `JSONValidator`.

---

#### **3. Output Data Structure**
The class returns a **tuple of three lists**:

```python
train_data: List[dict]  # 9,884 samples
val_data: List[dict]    # Optional, not specified in paper â†’ use empty list or None
test_data: List[dict]   # 100 samples
```

Each dictionary in the list has the **exact same structure** as the input JSONL line:

```python
{
  "prompt": str,
  "cot": str,
  "machine": list[dict]  # ConstructionTree format
}
```

> âš ï¸ **Important**: The `"machine"` field is **not converted** into a `ConstructionTree` object here. It remains as a **raw JSON list of dicts**.  
> Why? Because `RLTrainer` and `EvaluationMetrics` will **re-parse** it using `ConstructionTree` only when needed (e.g., for validation or simulation). This avoids unnecessary object creation during loading and keeps memory usage minimal.

However, **schema validation must occur at load time** to catch corrupted samples early.

---

#### **4. Splitting Logic (Train/Test Partitioning)**
- **Total samples**: 9,984 (from `config.dataset.cold_start_dataset_path`)
- **Test set size**: 100 (explicitly defined in `config.dataset.num_test_prompts`)
- **Train set size**: 9,984 - 100 = 9,884

**Splitting Strategy**:
- **Deterministic**: Use **fixed seed** (e.g., `seed=42`) to ensure reproducibility across runs.
- **Stratified?** Not required. The paper does not mention stratification by task type (car/catapult). All 100 test prompts are held-out from the original 100 prompts used in curation.  
  â†’ Therefore, **test set must correspond to 100 unique prompts** that were **not used** in generating the 9,984 training samples.
- **Implementation**:  
  The 9,984 samples are grouped by their source prompt (each original prompt generated 250 samples â†’ 100 prompts Ã— 250 = 25,000 â†’ filtered to 9,984).  
  â†’ To ensure **no overlap**, we must **exclude all samples from the 100 test prompts** from the training set.

**How to achieve this?**
- The `DatasetCurator` must have **annotated each sample** with its source prompt ID (e.g., `"prompt_id": 0` to `99`).
- If this annotation is **not present**, the paper implies that the **100 test prompts are disjoint** from the 100 cold-start prompts.  
  â†’ So we assume:  
  > The 9,984 training samples were generated from 100 prompts, and the 100 test prompts are a **completely separate set** of 100 prompts (not used in curation).

**Therefore, the splitting is not by sample count, but by prompt source**:

> â— **Assumption Clarification**:  
> The paper states:  
> *â€œWe curated 100 design objectives... Using this prompt dataset, we generate 250 machines per prompt... We obtain 9,984 machines...â€*  
> and later:  
> *â€œEvaluate on 100 test prompts (held-out from training set)â€*  

â†’ This implies the **100 test prompts are distinct from the 100 used for curation**.  
â†’ So the 9,984 training samples are **all** from the 100 curated prompts.  
â†’ The test set is **100 new prompts**, **not** sampled from the 9,984.

**Thus, the `DatasetLoader` does NOT split the 9,984 samples into train/test**.  
Instead:

- **Train**: Load all 9,984 samples from `cold_start_dataset_path`.
- **Test**: Load 100 new prompts from `test_prompts_path` (which contains **only prompts**, **not** machines).

> ðŸš¨ **CRITICAL INFERRED DESIGN DECISION**:  
> The test set **does not contain machine generations**.  
> During evaluation, the `SingleAgent`, `IterativeEditing`, etc., will **generate** machines from these 100 test prompts on-the-fly.  
> Therefore, `test_prompts_path` contains:
> ```json
> {"prompt": "Build a car to cross a curved track"}
> {"prompt": "Design a catapult to launch over a 5m wall"}
> ...
> ```

**Hence, `DatasetLoader.load_train_val_test()` returns:**
```python
train_data: list[dict]  # 9,984 samples: {prompt, cot, machine}
val_data: list[dict]    # [] (not used, paper doesnâ€™t specify)
test_prompts: list[str] # 100 strings: only prompts, no machines
```

> âœ… **This matches the paperâ€™s evaluation protocol**:  
> â€œRun all agent types on 100 test promptsâ€ â†’ agents generate machines from prompts â†’ simulate â†’ compute metrics.

**Therefore, the output of `DatasetLoader` is not three lists of full triples, but:**
- `train_data`: 9,984 full triples (for cold-start finetuning)
- `test_prompts`: 100 raw prompts (for agent inference during evaluation)

> ðŸ“Œ **Why?**  
> Because RL finetuning uses the **prompt + CoT + machine** triples to align LLM reasoning.  
> But evaluation uses **only prompts** to test generalization â€” agents must generate machines from scratch.

---

#### **5. Dependencies and Integration**
- **Dependencies**:
  - `construction_tree.py`: Only for **schema validation** of `"machine"` field in training data.  
    â†’ Use `JSONValidator.validate_construction_tree()` to ensure each `machine` list is valid before loading.
  - `utils/config.py`: To read paths (`cold_start_dataset_path`, `test_prompts_path`) and `num_test_prompts`.
  - `utils/logger.py`: Log loading stats (e.g., â€œLoaded 9984 training samples, 100 test promptsâ€).

- **Integration Points**:
  - **RLTrainer.cold_start_finetune()**: Calls `DatasetLoader.load_train_val_test()` â†’ gets `train_data` â†’ feeds to QOFT finetuning.
  - **EvaluationMetrics.compute_all()**: Called from `main.py` â†’ receives `test_prompts` â†’ passes to agents â†’ collects generated machines â†’ computes metrics.
  - **No direct use of `ConstructionTree` object**: Only JSON parsing and validation.

---

#### **6. Error Handling and Validation**
The class must **fail fast and clearly** on invalid data:

| Scenario | Action |
|--------|--------|
| File not found (`cold_start_dataset_path`) | Raise `FileNotFoundError` with path |
| File empty or not JSONL | Raise `ValueError` |
| Line is not valid JSON | Skip + log warning + count |
| `"machine"` field missing or not list | Skip + log warning |
| `"machine"` fails `JSONValidator.validate_construction_tree()` | Skip + log error (critical) |
| `"prompt"` missing or empty | Skip + log warning |
| Total training samples â‰  9,984 | Log warning + proceed (paper allows filtering) |
| `test_prompts_path` has â‰  100 prompts | Raise `ValueError` â€” must be exactly 100 |

> âœ… **Validation Policy**:  
> - **Training data**: Strict schema validation. Invalid samples are **discarded** (paper filtered to 9,984, so we assume the file is pre-cleaned).  
> - **Test prompts**: Only validate that itâ€™s a list of 100 non-empty strings.

---

#### **7. Performance and Memory Considerations**
- **File size**: 9,984 samples Ã— ~500 tokens each â†’ ~5â€“10 MB total â†’ fits in RAM.
- **Loading**: Use **generator + streaming** (`jsonlines.open()`) to avoid loading entire file into memory at once.
- **Processing**: Validate each sample on-the-fly during iteration.
- **Return**: Return lists (not generators) because `RLTrainer` needs random access during epoching.

---

#### **8. Configuration Usage**
- **Paths**:
  - `config.dataset.cold_start_dataset_path` â†’ input for training data
  - `config.dataset.test_prompts_path` â†’ input for test prompts
  - `config.dataset.num_test_prompts` â†’ validate test set size (must be 100)

> âœ… **All values are pulled from `config.yaml`** â€” no hardcoding.

---

#### **9. Edge Cases and Assumptions**
| Edge Case | Resolution |
|---------|------------|
| **Test prompts file contains machines?** | Raise error. Must be **only prompts**. Paper implies agents generate machines. |
| **Train file has fewer than 9,984 samples?** | Log warning. Proceed â€” paper says â€œwe obtained 9,984â€, but filtering may have removed some. |
| **Prompt in test set appears in train set?** | Impossible by design â€” test prompts are **disjoint** from curation prompts. |
| **Malformed `machine` with duplicate IDs?** | `JSONValidator` will catch and reject. |
| **Missing `"id"` field in machine?** | Reject â€” required by `ConstructionTree` schema. |
| **`"parent"` refers to non-existent ID?** | Reject â€” `JSONValidator` checks parent ID validity. |

---

#### **10. Summary: Expected Behavior**
```python
loader = DatasetLoader(config.dataset.cold_start_dataset_path, config.dataset.test_prompts_path)
train_data, _, test_prompts = loader.load_train_val_test()

len(train_data) == 9984  # âœ…
len(test_prompts) == 100 # âœ…
all(isinstance(p, str) and len(p) > 0 for p in test_prompts)  # âœ…
all(isinstance(m, list) and len(m) > 0 for d in train_data for m in [d["machine"]])  # âœ…
all(JSONValidator.validate_construction_tree(m)[0] for d in train_data for m in [d["machine"]])  # âœ…
```

> ðŸ”’ **The DatasetLoader is a gatekeeper**: It ensures that only **clean, validated, correctly partitioned** data flows into the system.  
> Any failure here cascades into **invalid RL training** or **unreproducible evaluation**.

---

#### **11. Implementation Notes for Code**
- Use `jsonlines` library to stream-read JSONL.
- Use `config.get()` to retrieve paths and num_test_prompts.
- Use `JSONValidator.validate_construction_tree()` from `utils/validator.py` for each machine.
- Log every skipped sample (for debugging).
- Use `random.seed(42)` if shuffling is needed (though not required).
- Return `val_data = []` (paper doesnâ€™t use validation set).
- Do **not** convert `"machine"` to `ConstructionTree` objects â€” keep as JSON list.

---

âœ… **Final Logic Summary for `DatasetLoader`**:
> **Load 9,984 (prompt, cot, machine) triples for training** â€” validate machine schema.  
> **Load 100 pure prompts for testing** â€” validate they are strings.  
> **Return train_data (list of dicts), empty val_data, test_prompts (list of strings)**.  
> **All configuration and validation driven by config.yaml and ConstructionTree schema.**  
> **Zero assumptions beyond paper â€” strict adherence to disjoint prompt sets.**