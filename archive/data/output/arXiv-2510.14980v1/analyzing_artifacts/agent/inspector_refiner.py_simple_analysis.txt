### **Logic Analysis: `agent/inspector_refiner.py` — InspectorRefiner Class**

---

#### **1. Role and Purpose in the System**
The `InspectorRefiner` class is a critical component of the **iterative editing workflow**, acting as a *self-critic agent* that bridges *abstract reasoning* and *concrete refinement*. Its function is twofold:

1. **Abstract Critique (Pre-Simulation)**:  
   Before any simulation occurs, the InspectorRefiner evaluates the draft machine design (a `ConstructionTree`) against the **high-level functional requirements** and **design principles** implied by the task. This is a *symbolic, reasoning-based critique* — analogous to a human engineer reviewing a blueprint for logical consistency, missing components, or structural flaws *before* building it.

2. **Feedback-Driven Refinement (Post-Simulation)**:  
   After the `ActiveEnvQuerier` simulates the machine and returns feedback (minimal + selective), the InspectorRefiner *invokes the Refiner* to generate revised designs that correct the identified failures. This step is *data-driven*, leveraging physical simulation outcomes.

> **Key Insight**: The dual-stage structure (critique → refinement) mirrors human design iteration: *think first, then test*. This reduces wasteful simulation calls and improves sample efficiency — a critical design choice in RLVR where simulation is expensive.

---

#### **2. Input and Output Contract**
- **Input**:
  - `design: ConstructionTree` — the machine design generated by the `Designer` (initial draft).
  - `feedback: dict` — the output from `ActiveEnvQuerier.get_minimal_feedback()` + `selective_query()`. Contains:
    - Task-specific metrics (e.g., boulder max height, root displacement)
    - Block-specific state logs (position, velocity, integrity) for queried blocks
    - Boolean flags (e.g., `boulder_max_height_gt_3m`, `any_block_broken`)

- **Output**:
  - `list[ConstructionTree]` — a list of **≥3 revised machine designs**, generated via LLM sampling. These are candidate revisions for the next MCTS rollout.

> **Note**: The `feedback` dict may be `None` during the *first critique* (pre-simulation), triggering only the abstract critique phase.

---

#### **3. Internal Workflow (Step-by-Step Logic)**

##### **Step 1: Abstract Critique (No Simulation)**
- **Trigger**: When `feedback is None` or `feedback == {}` (first pass in iterative loop).
- **Action**:
  - Construct a **prompt template** that includes:
    - Original task description
    - List of 27 blocks (from `config.yaml` → `tasks.*.validity_constraints`)
    - Attachment rules (from paper: one attachable face per block, Spring exception)
    - The draft `ConstructionTree` serialized as JSON
    - Explicit instruction:  
      > “Analyze this machine design abstractly. Does it satisfy the functional goal? Are there missing components? Are parts connected logically? Identify potential flaws without simulating physics. List 3–5 critical issues.”
  - Call the LLM (`self.llm_model`) with this prompt.
  - Parse the LLM response as a **structured critique** (e.g., JSON list of issues):
    ```json
    [
      "Missing counterweight: catapult arm will not swing with sufficient torque.",
      "Boulder is not secured in container — likely to fall out during launch.",
      "Powered wheel is not connected to axle — no propulsion possible."
    ]
    ```
- **Output**: A **textual critique summary** to be fed into Step 2.

##### **Step 2: Refinement via Refiner (With Simulation Feedback)**
- **Trigger**: When `feedback` is non-empty (after simulation).
- **Action**:
  - Construct a **refinement prompt** that includes:
    - Original task
    - Original design (as JSON)
    - Critique summary from Step 1 (if available)
    - **Simulation feedback** (formatted as natural language):
      > “Simulation results: Boulder reached 2.1m max height (threshold: 3m). Container block broke at t=1.4s. Root block moved 1.8m. Powered cog rotated but did not transfer torque to axle.”
    - Explicit instruction:
      > “Based on the above critique and simulation feedback, generate 5 revised versions of the machine. Each revision should fix at least one issue. Return only a list of valid construction trees in JSON format, one per item.”
  - Call `self.refiner.refine(design=design, feedback=feedback)` — which internally samples from LLM with `temperature=0.7`, `top_p=0.95` (per `config.yaml`).
  - The `Refiner` returns `list[ConstructionTree]` (≥3 candidates).

##### **Step 3: Output Filtering and Validation**
- **Validation**:
  - Filter out any candidate in the returned list that fails `ConstructionTree.validate()`.
  - Log invalid candidates via `Logger.debug()` for debugging.
- **Output Guarantee**:
  - Return **at least 3 valid** `ConstructionTree` objects.
  - If fewer than 3 valid candidates are generated, **repeat the refinement prompt with increased temperature (e.g., 0.8)** or **augment with random perturbations** (e.g., reposition last-added block) — a fallback heuristic to ensure MCTS has sufficient branching.

> **Rationale**: MCTS requires sufficient candidate diversity. A failed refinement loop (0 valid candidates) would collapse the search tree. This safeguard ensures robustness.

---

#### **4. Integration with Dependencies**

| Dependency | Role | How InspectorRefiner Uses It |
|----------|------|------------------------------|
| **`Refiner`** | Generates candidate revisions | Invoked in Step 2 to produce revised designs. Uses same LLM model. |
| **`ActiveEnvQuerier`** | Provides simulation feedback | Its output (`feedback: dict`) is passed directly to the refinement prompt. |
| **`ConstructionTree`** | Machine representation | Input and output type. Must be validated before/after refinement. |
| **`Config`** | Hyperparameters | Retrieves `temperature`, `top_p`, `candidates_per_round` from `config.yaml` → agent section. |
| **`Logger`** | Debugging & auditing | Logs critique summaries, invalid candidates, refinement attempts. |

> **Critical Design Note**: The `InspectorRefiner` **does not simulate**. It **delegates simulation** to `ActiveEnvQuerier`. This separation of concerns ensures modularity and allows `Refiner` to be reused in other contexts (e.g., hierarchical design).

---

#### **5. Critical Logic Decisions (Aligned with Paper)**

| Paper Requirement | Implementation Decision | Justification |
|-------------------|-------------------------|---------------|
| **“inspector agent which does self-critic before running any environment simulation”** | Implement **abstract critique phase** with no simulation | Matches “Effect of feedback-free self-critic” (Table TABREF100): improves performance for strong models like Gemini. |
| **“refiner generates multiple candidate revisions at each step”** | Use `Refiner` to generate ≥3 candidates via LLM sampling | Paper mandates multiple candidates for MCTS. Temperature=0.7 ensures diversity without chaos. |
| **“selectively extract feedback on specific blocks”** | Pass full `feedback` dict from `ActiveEnvQuerier` to refinement prompt | Enables LLM to reason about *why* a block broke (e.g., “Container broke at t=1.4s → weak joint”) — critical for precision. |
| **“hierarchical design is more structured and principled, yields lower variance”** | Abstract critique enforces principled design *before* simulation | Reduces random exploration → aligns with lower variance observed in hierarchical design. |
| **“LLMs fail to faithfully translate CoT into geometric design”** | Feed critique + feedback as *context*, not just CoT | Forces LLM to reconcile *abstract reasoning* with *physical outcome* — directly addresses the “CoT-machine correspondence” failure mode. |
| **“Pass@k advantage estimator”** | Ensure ≥3 valid candidates per iteration | Required for MCTS node expansion. 5 candidates per round (config) → 15–20 valid candidates over 10 rounds → sufficient for Pass@64. |

---

#### **6. Edge Cases and Error Handling**

| Scenario | Handling Strategy |
|--------|------------------|
| **LLM returns malformed JSON in critique** | Use `json.loads()` with try-except. If invalid, return generic critique: “Failed to parse critique. Proceeding with simulation feedback only.” |
| **No valid candidates after refinement** | Re-run refinement with `temperature=0.8`, `top_p=0.98`. If still fails, return 3 copies of original design with minor random perturbations (e.g., shift one block’s face_id by ±1). |
| **Feedback contains unknown block IDs** | Use `BlockRegistry.is_valid_block_type()` to validate block names in feedback. Log warning and ignore unknown blocks. |
| **Critique is empty or generic** | If critique contains only “Looks good” or “No issues”, trigger a **forced query**: ask Refiner to “Identify one potential failure mode even if design seems correct.” |
| **Simulation feedback contradicts critique** | Log discrepancy. Allow LLM to resolve: “Critique said ‘missing counterweight’, but simulation shows boulder reached 4m. Reconcile.” |

---

#### **7. Configuration Dependencies (config.yaml)**

| Key | Usage | Source |
|-----|-------|--------|
| `agent.temperature` | LLM sampling temperature for refinement | `self.refiner.refine(..., temperature=config.get("agent.temperature"))` |
| `agent.top_p` | Top-p sampling for refinement | Same as above |
| `agent.candidates_per_round` | Minimum number of candidates to generate | Used to validate output: `len(candidates) >= config.get("agent.candidates_per_round")` |
| `model.max_output_length` | Ensures LLM output fits context window | Passed to LLM tokenizer when constructing prompt |
| `simulation.catapult_height_threshold` | Used in feedback formatting | “Boulder height: {value}m (threshold: {threshold}m)” |

> **Important**: All values are pulled via `Config.get()` — **no hardcoding**.

---

#### **8. Performance and Efficiency Considerations**

- **Latency**: The abstract critique adds ~1–2 seconds per iteration (LLM call). This is acceptable given the 48-hour RL training budget and the 10-iteration MCTS loop.
- **Parallelization**: `InspectorRefiner` is **not parallelized** — it runs sequentially within each MCTS node. However, its output (candidates) is passed to `ParallelSimulator` for batch simulation.
- **Memory**: Only the JSON representation of `ConstructionTree` is passed — no large state tensors. Lightweight.
- **Scalability**: Can be reused in hierarchical design (MetaDesigner → Designer → InspectorRefiner → Refiner) if extended.

---

#### **9. Validation Against Paper Observations**

| Paper Observation | How InspectorRefiner Addresses It |
|------------------|----------------------------------|
| **“LLMs often fail to faithfully translate high-level plans into geometric designs”** | Abstract critique forces alignment between *intent* and *structure* before simulation. |
| **“Edit histories are helpful in decreasing failure attempts”** | Critique + feedback history is preserved in prompt → LLM learns from prior mistakes. |
| **“Meta-designer provides abstract blueprint → improves performance”** | InspectorRefiner acts as a *local* meta-designer per iteration — reinforces high-level structure. |
| **“Pass@k is more effective than Pass@1”** | Generates 5 candidates per node → enables selection of best among many → feeds into Pass@64 rollout. |
| **“Feedback improves performance”** | Combines *abstract* and *physical* feedback → superior to feedback-only or critique-only baselines. |

---

#### **10. Summary: Core Design Philosophy**

> **The InspectorRefiner is not a critic or a builder — it is a *translator*.**  
> It translates:  
> - *Human intent* → *Abstract design principles* (via critique),  
> - *Physical failure* → *Concrete revision instructions* (via feedback),  
> - And then *orchestrates* the Refiner to bridge the gap.

This mirrors how expert engineers work:  
> “I see you built a catapult, but you forgot the counterweight. And the container broke? Then the pivot is too weak. Fix both.”

By **decoupling critique from simulation**, the system achieves **higher sample efficiency**, **better interpretability**, and **stronger alignment with human design cognition** — directly addressing the paper’s key insight: *the bottleneck is not computation, but reasoning fidelity*.

---

#### **Next Steps for Implementation**

1. Implement `Refiner` class (already designed) to handle LLM sampling.
2. Build prompt templates as string templates in `agent/inspector_refiner.py` using `string.Template`.
3. Integrate `Config` to fetch `temperature`, `top_p`, `candidates_per_round`.
4. Implement fallback logic for zero valid candidates.
5. Add logging for critique summaries and refinement attempts.
6. Unit test with mock `Refiner` and `ActiveEnvQuerier` returning canned feedback.

This module is the **cognitive engine** of the iterative workflow — its quality determines whether the system discovers novel, functional designs, or merely tweaks failures.