{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f729a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from utils import *\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "# importlib.reload(utils)\n",
    "\n",
    "DASHSCOPE_API_KEY = \"sk-a15dd69d27524486a759b7ea0e706a87\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038a3ee",
   "metadata": {},
   "source": [
    "# 0_preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "431f5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the image pairs to a json file\n",
    "\n",
    "input_latex_path = \"/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/input/latex/arXiv-2510.14980v1\"\n",
    "output_path = \"/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output\"\n",
    "paper_name = os.path.basename(input_latex_path)\n",
    "\n",
    "pattern = r\"\\\\includegraphics(?:\\[.*?\\])?\\{(.*?)\\}\"\n",
    "\n",
    "image_pairs = {}\n",
    "\n",
    "# read the latex fileso\n",
    "for index, latexfile in enumerate(extract_raw_figures(input_latex_path)):\n",
    "    \n",
    "    image_path_line = [line for line in latexfile['snippet'].splitlines() if \"\\\\includegraphics\" in line][0]\n",
    "    \n",
    "    filename = re.findall(pattern, image_path_line)\n",
    "\n",
    "    image_pairs[index] = {\"image\": filename[-1], \"caption\": latexfile['snippet']}\n",
    "    # print(type(i))\n",
    "    # caption_line = [\n",
    "    #     line for line in i['snippet'].splitlines()\n",
    "    #     if re.search(r'\\\\caption(?:\\s*\\[[^\\]]*\\])?\\s*\\{', line)\n",
    "    # ][0]\n",
    "\n",
    "    # print(type(caption_line))\n",
    "    # papers[file].append(( \\\n",
    "    #     os.path.join(input_path,file, extract_figure_name_and_caption(image_path_line, caption_line)[0]), \\\n",
    "    #     extract_figure_name_and_caption(image_path_line, caption_line)[1]\n",
    "    # ))\n",
    "\n",
    "full_path = os.path.join(output_path, paper_name)\n",
    "os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "with open(full_path + '/' + 'image_pairs.json', 'w',encoding=\"utf-8\") as f:\n",
    "    json.dump(image_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38100ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(image_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fa2a7",
   "metadata": {},
   "source": [
    "# 1_image_agent.py\n",
    "## a. filter the image pairs based on the caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4526a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/v1p0r/miniconda3/envs/vllm_agent/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from openai import OpenAI\n",
    "import json\n",
    "model_name = \"qwen3-vl-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4ab0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image pairs from preprocess.py\n",
    "with open(full_path + '/' + 'image_pairs.json', \"r\", encoding=\"utf-8\") as f:\n",
    "    image_pairs = json.load(f)\n",
    "\n",
    "payload = json.dumps(image_pairs, ensure_ascii=False, separators=(\",\", \":\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230ed11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the OpenAI competiable client for qwen3-vl-flash\n",
    "client = OpenAI(\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope-intl.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "def api_call(model_name, prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "            model=model_name, \n",
    "            messages=prompt\n",
    "    )\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c65af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_prompt = [{'role': 'system', 'content': \"\"\"\n",
    "You are a strict figure classifier for a research paper.\n",
    "\n",
    "You receive a JSON object mapping string IDs to:\n",
    "{\n",
    "    \"0\": {\n",
    "        \"image\": \"path or filename\",\n",
    "        \"caption\": \"LaTeX figure environment including caption\"\n",
    "    },\n",
    "    \"1\": {\n",
    "        \"image\": \"path or filename\",\n",
    "        \"caption\": \"LaTeX figure environment including caption\"\n",
    "    },\n",
    "    \n",
    "}\n",
    "\n",
    "Your job is to select ONLY figures that are:\n",
    "- Identify only figures that depict **model architectures**, **pipelines**, or **system workflows**.\n",
    "- Exclude all others, including metric/performance plots, ablations, task illustrations, examples, galleries, datasets, failure cases, and editor screenshots.\n",
    "- For each selected figure, extract the **human-readable caption text** (remove LaTeX commands like \\\\begin, \\\\end, \\\\vspace, \\\\label, \\\\includegraphics, etc.).\n",
    "- Return that caption in plain, readable English — no LaTeX, no comments.\n",
    "\n",
    "You MUST EXCLUDE figures that are primarily:\n",
    "- Metrics, curves, score plots, RL training curves\n",
    "- Performance comparisons, ablations, Best@N, validity rates, reward curves\n",
    "- Qualitative examples, galleries of results, synthesized samples\n",
    "- Task/environment illustrations, dataset examples, failure cases, case studies\n",
    "- Editor UI screenshots, block catalogs, low-level component lists without a full system pipeline\n",
    "\n",
    "Use ONLY the provided `caption` text (and, if helpful, filename hints like 'RL_Metrics').\n",
    "\n",
    "Do not include explanations, comments, or extra fields.\n",
    "\"\"\"},\n",
    "\n",
    "\n",
    "{'role': 'user', 'content': f\"\"\"\n",
    "\n",
    "Below is a JSON object containing figure data from a research paper.\n",
    "\n",
    "{payload}\n",
    "\n",
    "\n",
    "Output format:\n",
    "Return **strictly valid JSON** of this exact format:\n",
    "```json\n",
    "{{\n",
    "    \n",
    "    \"id\": \"clear caption text\",\n",
    "    \"4\": \"clear caption text\",\n",
    "    \"8\": \"clear caption text\"\n",
    "    \n",
    "  \n",
    "}}\n",
    "```\n",
    "\n",
    "\n",
    "\"\"\"}\n",
    "]\n",
    "\n",
    "response = api_call(model_name, filtering_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f262c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = json.loads(response.model_dump_json())\n",
    "\n",
    "# print(json.dumps(response_json, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6120db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5': 'Our agentic machine design workflow.', '2': 'Demonstration of the default XML representation and our construction tree representation. Parent block info is in blue and child info is in red.'}\n"
     ]
    }
   ],
   "source": [
    "output_json = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "filtered_image_pairs = json.loads(output_json.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "\n",
    "print(filtered_image_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b61b8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(Path(os.path.join(output_path, paper_name,os.path.basename(filename))).with_suffix('.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a021ec8",
   "metadata": {},
   "source": [
    "## b. extract the information from the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "652e3920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output/arXiv-2510.14980v1/besiegefield_pipeline_compressed.png\n",
      "✅ Saved: /Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output/arXiv-2510.14980v1/xml_intro_v2_cropped.png\n"
     ]
    }
   ],
   "source": [
    "# image to png to b64\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "for key, caption in filtered_image_pairs.items():\n",
    "    filtered_image_pairs[key] = {}\n",
    "\n",
    "    filename = os.path.join(input_latex_path, image_pairs[key][\"image\"])\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        png_output_path = Path(os.path.join(output_path, paper_name,os.path.basename(filename))).with_suffix('.png')\n",
    "        subprocess.run(['bash','/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/pdf2png.sh', filename, png_output_path])\n",
    "\n",
    "    filtered_image_pairs[key] = {\"image\": image_pairs[key][\"image\"], \\\n",
    "        \"caption\": caption, \\\n",
    "        \"b64\": image_to_base64(png_output_path)\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e91b6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for key, image_pair in filtered_image_pairs.items():\n",
    "    caption = image_pair[\"caption\"]\n",
    "    image_b64 = image_pair[\"b64\"]\n",
    "    extraction_prompt = [{'role': 'system', 'content': '''\n",
    "    You are an expert vision-language model specialized in understanding figures from computer science and machine learning papers.\n",
    "\n",
    "    Given:\n",
    "    1. A caption describing a figure.\n",
    "    2. The figure image itself.\n",
    "\n",
    "    Your tasks:\n",
    "\n",
    "    1. Classify the figure into exactly ONE of the following categories:\n",
    "    - \"model_architecture\": diagrams that show layers, blocks, components of a model or network.\n",
    "    - \"pipeline\": diagrams that show a sequence of processing steps or modules for data or experiments (ingest → preprocess → train → eval, etc.).\n",
    "    - \"system_workflow\": diagrams that show interactions between multiple components, agents, services, or tools in a larger system (e.g., multi-agent system, platform, or end-to-end toolchain).\n",
    "    - \"other\": if none of the above clearly apply.\n",
    "\n",
    "    2. Extract structured information from the figure that would help RECONSTRUCT the model, pipeline, or system in code.\n",
    "    Be as concrete and implementation-oriented as possible:\n",
    "    - List key components/blocks/modules with concise names and roles.\n",
    "    - List edges / data flows (source → target, what is passed).\n",
    "    - List stages or steps in order, if applicable.\n",
    "    - Include inputs and outputs (data types, files, models, agents, APIs, etc.).\n",
    "    - Include any loops, conditions, or control flow if visible.\n",
    "    - Prefer information grounded in the VISUAL content; use the caption only as supporting context.\n",
    "    - Do NOT invent components that are not clearly implied by the figure.\n",
    "\n",
    "    3. Output MUST be valid JSON only. No extra text.\n",
    "\n",
    "    JSON schema (strict):\n",
    "\n",
    "    {\n",
    "    \"category\": \"model_architecture\" # \"pipeline\" or \"system_workflow\" or \"other\",\n",
    "    \"extracted\": {\n",
    "        \"components\": [\n",
    "        {\n",
    "            \"name\": \"string\",\n",
    "            \"type\": \"string\",\n",
    "            \"role\": \"string\"\n",
    "        }\n",
    "        ],\n",
    "        \"flows\": [\n",
    "        {\n",
    "            \"source\": \"string\",\n",
    "            \"target\": \"string\",\n",
    "            \"data\": \"string\"\n",
    "        }\n",
    "        ],\n",
    "        \"stages\": [\n",
    "        {\n",
    "            \"name\": \"string\",\n",
    "            \"description\": \"string\"\n",
    "        }\n",
    "        ],\n",
    "        \"inputs\": [\"string\"],\n",
    "        \"outputs\": [\"string\"]\n",
    "    }\n",
    "    }\n",
    "    '''},{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Analyze the following figure and respond in the required JSON schema.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"Caption: {caption}\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{image_b64}\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "    response = api_call(model_name, extraction_prompt)\n",
    "    output_json = json.loads(response.model_dump_json())\n",
    "    content = output_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "    image_list.append(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47dff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_path}/{paper_name}/image_extracted_information.json', 'w') as f:\n",
    "        json.dump(content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80459512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # response.model_dump_json()\n",
    "# x =json.loads(response.model_dump_json())\n",
    "# # response = api_call(model_name, extraction_prompt)\n",
    "# # parse:\n",
    "# content = response.model_dump_json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "707ebc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "826a708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4598cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = json.loads(content)[\"choices\"][0][\"message\"][\"content\"]\n",
    "# with open(f'{output_path}/image_extracted_information.json', 'w') as f:\n",
    "#     json.dump(x, f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
