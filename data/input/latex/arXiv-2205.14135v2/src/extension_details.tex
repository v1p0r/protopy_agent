\section{Extension Details}
\label{sec:extension_details}

\subsection{Block-sparse \sysname}
\label{subsec:block_sparse_details}

We describe the full block-sparse \sysname algorithm
in~\cref{alg:blocksparse_stream_attn}.
The algorithm is identical to~\cref{alg:fwd_full}, except that we skip zero blocks.
\iftoggle{arxiv}{
\begin{algorithm}[h]
} {
\begin{algorithm}[h]
}
  \caption{\small\label{alg:blocksparse_stream_attn}Block-Sparse \sysname Forward Pass}
  \begin{algorithmic}[1]
    \REQUIRE Matrices $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$ in HBM, on-chip SRAM of
    size $M$, softmax scaling constant $\tau \in \mathbb{R}$, masking function
    $\textsc{mask}$, dropout probability $p_\mathrm{drop}$, block sizes
    $B_c = \left \lceil \frac{M}{4d} \right\rceil, B_r = \min\left( \left \lceil \frac{M}{4d} \right\rceil, d\right)$, block sparsity mask $M \in \{ 0, 1 \}^{N/B_r \times N/B_c}$..
    \STATE Initialize the pseudo-random number generator state ${\cal R}$ and save to HBM.
    \STATE Initialize $\vO = (0)_{N \times d} \in \mathbb{R}^{N \times d}, \ell = (0)_N \in \mathbb{R}^{N}, m = (-\infty)_N \in \mathbb{R}^{N}$ in HBM.
    \STATE Divide $\vQ$ into $T_r = \left\lceil\frac{N}{B_r} \right\rceil$ blocks $\vQ_1, \dots, \vQ_{T_r}$ of size $B_r \times d$ each,
    and divide $\vK, \vV$ in to $T_c = \left\lceil \frac{N}{B_c} \right\rceil$ blocks $\vK_1, \dots, \vK_{T_c}$ and
    $\vV_1, \dots, \vV_{T_c}$, of size $B_c \times d$ each.
    \STATE Divide $\vO$ into $T_r$ blocks $\vO_i, \dots, \vO_{T_r}$ of size
    $B_r \times d$ each, divide $\ell$ into $T_r$ blocks $\ell_i, \dots, \ell_{T_r}$ of size
    $B_r$ each, divide $m$ into $T_r$ blocks $m_1, \dots, m_{T_r}$ of size $B_r$ each.
    \FOR{$1 \le j \le T_c$}
      \STATE Load $\vK_j, \vV_j$ from HBM to on-chip SRAM.
      \FOR{$1 \le i \le T_r$}
        \IF{$M_{ij} \neq 0$}
        \STATE Load $\vQ_i, \vO_i, \ell_i, m_i$ from HBM to on-chip SRAM.
        \STATE On chip, compute $\vS_{ij} = \tau \vQ_i \vK_j^T \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute $\vS_{ij}^{\mathrm{masked}} = \textsc{mask}(\vS_{ij})$.
        \STATE On chip, compute $\tilde{m}_{ij} = \mathrm{rowmax}(\vS_{ij}^{\mathrm{masked}}) \in \mathbb{R}^{B_r}$, $\tilde{\vP}_{ij} = \exp(\vS_{ij}^{\mathrm{masked}} - \tilde{m}_{ij}) \in \mathbb{R}^{B_r \times B_c}$ (pointwise),
        $\tilde{\ell}_{ij} = \mathrm{row sum}(\tilde{\vP}_{ij}) \in \mathbb{R}^{B_r}$.
        \STATE On chip, compute $m_i^{\mathrm{new}} = \max(m_i, \tilde{m}_{ij}) \in \mathbb{R}^{B_r}$, $\ell_i^{\mathrm{new}} = e^{m_i - m_i^{\mathrm{new}}} \ell_i + e^{\tilde{m}_{ij} - m_i^{\mathrm{new}}} \tilde{\ell}_{ij} \in \mathbb{R}^{B_r}$.
        \STATE On chip, compute $\tilde{\vP}_{ij}^{\mathrm{dropped}} = \mathrm{dropout}(\tilde{\vP}_{ij}, p_\mathrm{drop})$.
        \STATE Write $\vO_i \leftarrow \diag(\ell_i^{\mathrm{new}})^{-1}(\diag(\ell_i) e^{m_i - m_i^{\mathrm{new}}} \vO_i + e^{\tilde{m}_{ij} - m_i^{\mathrm{new}}}\tilde{\vP}_{ij}^{\mathrm{dropped}} \vV_j)$
        to HBM.
        \STATE Write $\ell_i \leftarrow \ell_i^{\mathrm{new}}$, $m_i \leftarrow m_i^{\mathrm{new}}$ to HBM.
        \ENDIF
      \ENDFOR
    \ENDFOR
    \STATE Return $\vO, \ell, m, {\cal R}$.
  \end{algorithmic}
\end{algorithm}

We prove the IO-complexity of block-sparse \sysname.
\begin{proof}[Proof of \cref{thm:io_complexity_blocksparse}]
  The proof is very similar to the proof of~\cref{thm:io_complexity}.
  For the block-sparse case, notice that we only need to load blocks
  corresponding to nonzero blocks.
  As a result, the number of HBM accesses are scaled by $s$, the
  fraction of nonzero blocks in the block-sparsity mask.
  However, for small values of $s$, we would still need to write the result
  $\vO \in \mathbb{R}^{N \times d}$.
  Therefore the number of HBM accesses is
  \begin{equation*}
    \Theta \left( Nd + \frac{N^2 d^2}{M} s \right).
  \end{equation*}

\end{proof}

\subsection{Potential Extensions}

We discuss here a few potential extensions of the IO-aware approach to speed up
deep learning training.

\textbf{Multi-GPU Attention.}
Large language models are trained on hundreds or thousands of GPUs, and one
typically splits the attention computation between 4-8 GPUs on the same
node~\citep{shoeybi2019megatron}.
This introduces another level of memory hierarchy: beside GPU SRAM and GPU HBM,
we also have the HBM of other GPUs.
For very long sequences, the different GPUs on the same node can cooperate to
compute attention by taking into account the asymmetry of different levels of
memory hierarchy.

\textbf{Sparse MLP layers.}
Typical dense MLP layers are compute-bound and not memory-bound.
To improve their efficiency, MLP layers with sparse weight matrices can be
used~\citep{dao2021pixelated}.
However, many sparse MLP layers are instead memory-bound, and their speedup is
often not proportional to the sparsity.
We believe that an IO-aware implementation can alleviate this issue and realize
the benefits of sparsity.
We are excited about future work in this direction, to reduce the computational
requirement of large models and improve their wall-block runtime.

\textbf{Kernel machine learning.}
Our approach in \sysname relies on the fact that the $N \times N$ attention matrix is
a function of a low-rank matrix $\vQ \vK^\top$ (of rank $d \ll N$).
As a result, we can repeatedly load the inputs $\vQ, \vK$ and recompute the
block of the attention matrix that we need, significantly reducing HBM access.
As similar scenario happens in kernel machine learning: each element $K_{ij}$ of the
$N \times N$ kernel matrix $\vK$ is a function of two vectors of size $d \ll N$, as it
measures the similarity between two datapoints $x_i$ and $x_j$.
The KeOps library~\citep{feydy2020fast,charlier2021kernel} is a successful example of how
reducing memory reads/writes can speed up kernel operations.
We hope that this will motivate kernel methods that focus more on reducing IOs
instead of just FLOPs.

