\iftoggle{icmlworkshop}{
\section{\sysname: Efficient Attention with Tiling \& Recomputation}
}{
\section{\sysname: Algorithm, Analysis, and Extensions}
}
\label{sec:algo}

We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate matrices for the backward pass.
This yields an attention algorithm that is both memory efficient and faster
in wall-clock time.
We analyze its IO complexity, showing that our method requires
much fewer HBM accesses compared to standard attention.
We further show that \sysname can serve as a useful primitive by extending it to handle block-sparse attention.

\iftoggle{icmlworkshop}{
\cref{sec:background} contains background on GPU architecture and the performance characteristics of common deep learning operations.
We note that attention is memory-bound: its runtime is bottlenecked by the time taken by HBM reads/writes.
}{}
We focus here on the forward pass for ease of exposition; \cref{sec:algo_details} contains details
for the backward.

\iftoggle{icmlworkshop}{}{
\subsection{An Efficient Attention Algorithm With Tiling and Recomputation}
\label{sec:implementation}
}


Given the inputs $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$ in HBM, we aim to compute the attention output $\vO \in \mathbb{R}^{N \times d}$ and write it to HBM.
Our goal is to reduce the amount of HBM accesses (to sub-quadratic in $N$).

We apply two established techniques (tiling, recomputation) to overcome the
technical challenge of computing exact attention in sub-quadratic HBM accesses.
We describe this in~\cref{alg:stream_attn}.
The main idea is that we split the inputs $\vQ, \vK, \vV$ into blocks,
load them from slow HBM to fast SRAM, then compute the attention output with
respect to those blocks.
By scaling the output of each block by the right normalization factor before
adding them up, we get the correct result at the end.

\textbf{Tiling.}
We compute attention by blocks.
Softmax couples columns of $\vK$,
so we decompose the large softmax
with scaling~\citep{milakov2018online, kitaev2020reformer, rabe2021self}.
\iftoggle{icmlworkshop}{}{
For numerical stability, the softmax of vector $x \in \mathbb{R}^{B}$ is computed as:
\begin{equation*}
  m(x) \defeq \max_i\ \ x_i, \quad
  f(x) \defeq \begin{bmatrix} e^{x_1 - m(x)} & \hdots & e^{x_B - m(x)} \end{bmatrix}, \quad
  \ell(x) \defeq \sum_i f(x)_i, \quad
  \softmax(x) \defeq \frac{f(x)}{\ell(x)}.
\end{equation*}
For vectors $x^{(1)}, x^{(2)} \in \mathbb{R}^{B}$, we can decompose the softmax of the concatenated $x = \begin{bmatrix} x^{(1)} \ x^{(2)} \end{bmatrix} \in \mathbb{R}^{2B}$ as:
\begin{align*}
  &m(x) = m(\begin{bmatrix} x^{(1)} \ x^{(2)} \end{bmatrix}) = \max(m(x^{(1)}), m(x^{(2)})), \quad
  f(x) = \begin{bmatrix} e^{m(x^{(1)}) - m(x)} f(x^{(1)}) & e^{m(x^{(2)}) - m(x)} f(x^{(2)}) \end{bmatrix}, \\
  &\ell(x) = \ell(\begin{bmatrix} x^{(1)} \ x^{(2)} \end{bmatrix}) = e^{m(x^{(1)}) - m(x)}\ell (x^{(1)}) + e^{m(x^{(2)}) - m(x)} \ell(x^{(2)}), \quad
  \softmax(x) = \frac{f(x)}{\ell(x)}.
\end{align*}
Therefore if we keep track of some extra statistics ($m(x), \ell(x)$), we can compute softmax one block at a time.\footnote{This style of aggregation is called \emph{algebraic aggregation}~\citep{gray1997data}.}
}
We thus split the inputs $\vQ, \vK, \vV$ into blocks (\cref{alg:stream_attn} line \ref{alg:stream_attn_split_qkv}), compute the softmax values along with extra statistics (\cref{alg:stream_attn} line \ref{alg:stream_attn_statistics}), and combine the results (\cref{alg:stream_attn} line \ref{alg:stream_attn_aggregate}).

\textbf{Recomputation.}
One of our goals is to not store $O(N^2)$ intermediate values for the backward
pass.
The backward pass typically requires the matrices
$\vS, \vP \in \mathbb{R}^{N \times N}$ to compute the gradients with respect to $\vQ, \vK, \vV$.
However, by storing the output $\vO$ and the softmax normalization statistics $(m, \ell)$, we can
recompute the attention matrix $\vS$ and $\vP$ easily in the backward pass from blocks of $\vQ, \vK, \vV$ in SRAM.
This can be seen as a form of selective gradient checkpointing~\citep{griewank2008evaluating, chen2016training}.
While gradient checkpointing has been suggested to reduce the maximum amount of memory required~\citep{rabe2021self}, all implementations (that we know off) have to trade speed for memory.
In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to reduced HBM accesses (\cref{fig:micros}).
The full backward pass description is in~\cref{sec:algo_details}.


\textbf{Implementation details: Kernel fusion.}
Tiling enables us to implement our algorithm in one CUDA kernel, loading input from HBM,
performing all the computation steps (matrix multiply, softmax, optionally
masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout in~\cref{sec:algo_details}).
This avoids repeatedly reading and writing of inputs and outputs from and to HBM.


\vspace{-0.5em}
\begin{algorithm}[H]
  \iftoggle{icmlworkshop}{
  \small
  }{}
  \caption{\small\label{alg:stream_attn}\sysname}
  \begin{algorithmic}[1]
    \REQUIRE Matrices $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$ in HBM, on-chip SRAM of
    size $M$.
    \STATE Set block sizes $B_c = \left\lceil \frac{M}{4d} \right\rceil, B_r = \min \left( \left\lceil \frac{M}{4d} \right\rceil , d \right)$.
    \STATE \label{alg:stream_attn_init} Initialize $\vO = (0)_{N \times d} \in \mathbb{R}^{N \times d}, \ell = (0)_N \in \mathbb{R}^{N}, m = (-\infty)_N \in \mathbb{R}^{N}$ in HBM.
    \STATE \label{alg:stream_attn_split_qkv} Divide $\vQ$ into $T_r = \left\lceil\frac{N}{B_r} \right\rceil$ blocks $\vQ_1, \dots, \vQ_{T_r}$ of size $B_r \times d$ each,
    and divide $\vK, \vV$ in to $T_c = \left\lceil \frac{N}{B_c} \right\rceil$ blocks $\vK_1, \dots, \vK_{T_c}$ and
    $\vV_1, \dots, \vV_{T_c}$, of size $B_c \times d$ each.
    \STATE Divide $\vO$ into $T_r$ blocks $\vO_i, \dots, \vO_{T_r}$ of size
    $B_r \times d$ each, divide $\ell$ into $T_r$ blocks $\ell_i, \dots, \ell_{T_r}$ of size
    $B_r$ each, divide $m$ into $T_r$ blocks $m_1, \dots, m_{T_r}$ of size $B_r$ each.
    \FOR{$1 \le j \le T_c$} \label{alg:stream_attn_outer_loop}
      \STATE \label{alg:stream_attn_load_kv} Load $\vK_j, \vV_j$ from HBM to on-chip SRAM.
      \FOR{$1 \le i \le T_r$}
        \STATE \label{alg:stream_attn_load_qo} Load $\vQ_i, \vO_i, \ell_i, m_i$ from HBM to on-chip SRAM.
        \STATE \label{alg:stream_attn_qk} On chip, compute $\vS_{ij} = \vQ_i \vK_j^T \in \mathbb{R}^{B_r \times B_c}$.
        \STATE \label{alg:stream_attn_statistics} On chip, compute $\tilde{m}_{ij} = \mathrm{rowmax}(\vS_{ij}) \in \mathbb{R}^{B_r}$, $\tilde{\vP}_{ij} = \exp(\vS_{ij} - \tilde{m}_{ij}) \in \mathbb{R}^{B_r \times B_c}$ (pointwise),
        $\tilde{\ell}_{ij} = \mathrm{row sum}(\tilde{\vP}_{ij}) \in \mathbb{R}^{B_r}$.
        \STATE On chip, compute $m_i^{\mathrm{new}} = \max(m_i, \tilde{m}_{ij}) \in \mathbb{R}^{B_r}$, $\ell_i^{\mathrm{new}} = e^{m_i - m_i^{\mathrm{new}}} \ell_i + e^{\tilde{m}_{ij} - m_i^{\mathrm{new}}} \tilde{\ell}_{ij} \in \mathbb{R}^{B_r}$.
        \STATE \label{alg:stream_attn_aggregate} Write $\vO_i \leftarrow \diag(\ell_i^{\mathrm{new}})^{-1}(\diag(\ell_i) e^{m_i - m_i^{\mathrm{new}}} \vO_i + e^{\tilde{m}_{ij} - m_i^{\mathrm{new}}}\tilde{\vP}_{ij} \vV_j)$
        to HBM.
        \STATE Write $\ell_i \leftarrow \ell_i^{\mathrm{new}}$, $m_i \leftarrow m_i^{\mathrm{new}}$ to HBM.
      \ENDFOR
    \ENDFOR
    \STATE Return $\vO$.
  \end{algorithmic}
\end{algorithm}
\vspace{-0.5em}

We show \sysname's correctness, runtime, and memory requirement (proof in~\cref{sec:proofs}).
\begin{theorem}
  \label{thm:correctness}
  \cref{alg:stream_attn} returns $\vO = \softmax(\vQ\vK^\top)\vV$ with $O(N^2d)$ FLOPs and
  requires $O(N)$ additional memory beyond inputs and output.
\end{theorem}

\iftoggle{icmlworkshop}{
In~\cref{sec:theory}, we analyze the IO-complexity of \sysname, proving that it
requires fewer HBM accesses than standard attention, and the complexity is
optimal for a range of SRAM size $M$.
We discuss various extensions to \sysname in~\cref{sec:blocks_sparse,sec:extension_details}.
}{}

