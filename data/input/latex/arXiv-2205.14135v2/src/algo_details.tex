\section{Algorithm Details}
\label{sec:algo_details}

We first derive the forward and backward passes of attention and show that
they can be computed in a memory-efficient manner (requiring extra memory linear
instead of quadratic in the sequence length).
Though they reduce the amount of extra memory required, naively they still incur
quadratic HBM accesses, resulting in slower execution speed.
We describe the \sysname algorithm to implement both the forward and the
backward passes on GPUs that reduces HBM accesses, leading to both faster
runtime and smaller memory footprint.


\subsection{Memory-efficient forward pass}
\label{sec:forward}

The main challenge in making attention memory-efficient is the softmax that
couples the columns of $\vK$ (and columns of $\vV$).
Our approach is to compute the softmax normalization constant separately to
decouple the columns.
This technique~\citep{milakov2018online} has been used in the
literature~\citep{kitaev2020reformer,rabe2021self} to show that attention
computation does not need quadratic \emph{extra} memory (though the number of
HBM accesses is still quadratic, resulting in slow run-time).

For simplicity, we omit here the max-shifting step during softmax.
The full algorithm in~\cref{sec:algo_fwd_full} contains all the steps.

Recall that given input sequences $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$, we want to
compute the attention output $\vO \in \mathbb{R}^{N \times d}$:
\begin{equation*}
  \vS = \vQ \vK^\top \in \mathbb{R}^{N \times N}, \quad \vP = \softmax(\vS) \in \mathbb{R}^{N \times N}, \quad \vO = \vP\vV \in \mathbb{R}^{N \times d}.
\end{equation*}

We have that $S_{ij} = q_i^T k_j$ where $q_i$ and $k_j$ are the $i$-th and
$j$-th columns of $\vQ$ and $\vK$ respectively.
Define the normalization constants of softmax:
\begin{equation}
  \label{eq:L_i}
  L_i = \sum_{j} e^{q_i^T k_j}.
\end{equation}
Let $v_j$ be the $j$-th column of $\vV$, then the $i$-th columns of the output is
\begin{equation}
  \label{eq:forward_oi}
  o_i = P_{i:} \vV = \sum_{j} P_{ij} v_j = \sum_{j} \frac{e^{q_i^T k_j}}{L_i} v_j.
\end{equation}

We see that once $L_i$ is computed, we can compute $o_i$ without extra memory
by repeatedly summing $\frac{e^{q_i^T k_j}}{L_i} v_j$.
Therefore the forward pass can be computed with $O(n)$ extra memory:
\begin{enumerate}
  \item Compute $L_i$ for all $i$ according to \cref{eq:L_i}, which takes $O(n)$
  extra memory.
  \item Compute $o_i$ for all $i$ according to \cref{eq:forward_oi}, which takes
  $O(d)$ extra memory.
\end{enumerate}

\subsection{Memory-efficient backward pass}
\label{sec:backward}

We derive the backward pass of attention and show that it can also be computed
with linear memory.
\citet{rabe2021self} suggests that the backward pass can be done without
quadratic extra memory by applying gradient checkpointing to the
memory-efficient forward pass.
We instead derive the backward pass explicitly and show how it can be computed
in a memory-efficient manner.

Suppose that there is a scalar loss function $\phi$, and let the output gradient
be $\vdO \in \mathbb{R}^{n \times d}$ (where $\vdO$ denotes
$\frac{\partial \phi}{\partial \vO}$).
We want to compute the input gradients $\vdQ, \vdK, \vdV \in \mathbb{R}^{n \times d}$
(where $\vdQ, \vdK, \vdV$ denote
$\frac{\partial \phi}{\partial \vQ}, \frac{\partial \phi}{\partial \vK}, \frac{\partial \phi}{\partial \vV}$
respectively).

The gradient $\vdV$ is easy to see.
Applying reverse-mode autodiff by hand (aka the chain rule), we obtain (in
matrix notation) $\vdV = \vP^T \vdO$.
Thus:
\begin{equation}
  \label{eq:dv}
  dv_j = \sum_{i} P_{ij} do_i = \sum_{i} \frac{e^{q_i^T k_j}}{L_i} do_i.
\end{equation}
Since we already computed $L_i$, $dv_j$ can be computed without extra memory by
repeated summing.

The gradients $\vdQ$ and $\vdK$ are a little more complicated.
We go through the gradients $\vdP$ and $\vdS$ first.
From \cref{eq:forward_oi}, we have that $\vdP = \vdO \vV^T$, and so:
\begin{equation*}
  dP_{ij} = do_i^T v_j.
\end{equation*}

Recall that $P_{i:} = \softmax(S_{i:})$.
Using the fact that the Jacobian of $y = \softmax(x)$ is $\diag(y) - y y^T$, we
have that
\begin{equation*}
  dS_{i:} = (\diag(P_{i:}) - P_{i:} P_{i:}^T) dP_{i:} = P_{i:} \circ dP_{i:} - (P_{i:}^T dP_{i:}) P_{i:},
\end{equation*}
where $\circ$ denotes pointwise multiplication.

Define
\begin{equation}
  \label{eq:D_i}
  D_{i} = P_{i:}^T dP_{i:} = \sum_{j} \frac{e^{q_i^T k_j}}{L_i} do_i^T v_j = do_i^T \sum_{j} \frac{e^{q_i^\top k_j}}{L_i} v_j = do_i^T o_i,
\end{equation}
then
\begin{equation*}
  dS_{i:} = P_{i:} \circ dP_{i:} - D_i P_{i:}.
\end{equation*}
Hence
\begin{equation*}
  dS_{ij} = P_{ij} dP_{ij} - D_i P_{ij} = P_{ij} (dP_{ij} - D_i).
\end{equation*}

Now we can get the gradients $\vdQ$ and $\vdK$.
Recall that $S_{ij} = q_i^T k_j$, so
\begin{equation}
  \label{eq:dq}
  dq_i = \sum_{j} dS_{ij} k_j = \sum_{j} P_{ij} (dP_{ij} - D_i) k_j = \sum_{j} \frac{e^{q_i^T k_j}}{L_i} (do_i^T v_j - D_i) k_j.
\end{equation}
Similarly,
\begin{equation}
  \label{eq:dk}
  dk_j = \sum_{i} dS_{ij} q_i = \sum_{i} P_{ij} (dP_{ij} - D_i) q_i = \sum_{i} \frac{e^{q_i^T k_j}}{L_i} (do_i^T v_j - D_i) q_i.
\end{equation}

Therefore the backward pass can also be computed with $O(n)$ extra memory:
\begin{enumerate}
  \item Compute $dv_j$ for all $j$ according to \cref{eq:dv}, which takes
  $O(d)$ extra memory.
  \item Compute $D_i$ for all $i$ according to \cref{eq:D_i}, which takes $O(n)$
  extra memory.
  \item Compute $dq_i$ for all $i$ according to \cref{eq:dq}, which takes
  $O(d)$ extra memory.
  \item Compute $dk_j$ for all $j$ according to \cref{eq:dk}, which takes
  $O(d)$ extra memory.
\end{enumerate}

\subsection{\sysname: Forward Pass}
\label{sec:algo_fwd_full}

We describe the full details of \sysname forward pass.
Given input sequences $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$, we want to
compute the attention output $\vO \in \mathbb{R}^{N \times d}$:
\begin{align*}
  &\vS = \tau \vQ \vK^\top \in \mathbb{R}^{N \times N}, \quad
  \vS^{\mathrm{masked}} = \textsc{mask}(S) \in \mathbb{R}^{N \times N}, \quad
  \vP = \softmax(\vS^{\mathrm{masked}}) \in \mathbb{R}^{N \times N}, \\
  &\vP^{\mathrm{dropped}} = \mathrm{dropout}(\vP, p_\mathrm{drop}), \quad
  \vO = \vP^{\mathrm{dropped}}\vV \in \mathbb{R}^{N \times d},
\end{align*}
where $\tau \in \mathbb{R}$ is some softmax scaling (typically $\frac{1}{\sqrt{d}}$),
$\textsc{mask}$ is some masking function that sets some entries of the input to
$-\infty$ and keep other entries the same (e.g., key padding mask when sequences
in the batch don't have the same lengths and are padded), and
$\mathrm{dropout}(x, p)$ applies dropout to $x$ elementwise (i.e., output $\frac{x}{1 - p}$
with probability $1 - p$ and output 0 with probability $p$ for each element $x$).

\iftoggle{icmlworkshop}{
For numerical stability, the softmax of vector $x \in \mathbb{R}^{B}$ is computed as:
\begin{equation*}
  m(x) \defeq \max_i\ \ x_i, \quad
  f(x) \defeq \begin{bmatrix} e^{x_1 - m(x)} & \hdots & e^{x_B - m(x)} \end{bmatrix}, \quad
  \ell(x) \defeq \sum_i f(x)_i, \quad
  \softmax(x) \defeq \frac{f(x)}{\ell(x)}.
\end{equation*}
For vectors $x^{(1)}, x^{(2)} \in \mathbb{R}^{B}$, we can decompose the softmax of the concatenated $x = \begin{bmatrix} x^{(1)} \ x^{(2)} \end{bmatrix} \in \mathbb{R}^{2B}$ as:
\begin{align*}
  &m(x) = m(\begin{bmatrix} x^{(1)} \ x^{(2)} \end{bmatrix}) = \max(m(x^{(1)}), m(x^{(2)})), \quad
  f(x) = \begin{bmatrix} e^{m(x^{(1)}) - m(x)} f(x^{(1)}) & e^{m(x^{(2)}) - m(x)} f(x^{(2)}) \end{bmatrix}, \\
  &\ell(x) = \ell(\begin{bmatrix} x^{(1)} \ x^{(2)} \end{bmatrix}) = e^{m(x^{(1)}) - m(x)}\ell (x^{(1)}) + e^{m(x^{(2)}) - m(x)} \ell(x^{(2)}), \quad
  \softmax(x) = \frac{f(x)}{\ell(x)}.
\end{align*}
}{}


The full algorithm is in~\cref{alg:fwd_full}.
We save the output $\vO$, the softmax statistics $\ell$ and $m$, and the pseudo-random
number generator state ${\cal R}$ for the backward pass.
\begin{algorithm}[H]
  \caption{\small\label{alg:fwd_full}\sysname Forward Pass}
  \begin{algorithmic}[1]
    \REQUIRE Matrices $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$ in HBM, on-chip SRAM of
    size $M$, softmax scaling constant $\tau \in \mathbb{R}$, masking function
    $\textsc{mask}$, dropout probability $p_\mathrm{drop}$.
    \STATE Initialize the pseudo-random number generator state ${\cal R}$ and save to HBM.
    \STATE Set block sizes $B_c = \left\lceil \frac{M}{4d} \right\rceil, B_r = \min \left( \left\lceil \frac{M}{4d} \right\rceil , d \right)$.
    \STATE Initialize $\vO = (0)_{N \times d} \in \mathbb{R}^{N \times d}, \ell = (0)_N \in \mathbb{R}^{N}, m = (-\infty)_N \in \mathbb{R}^{N}$ in HBM.
    \STATE Divide $\vQ$ into $T_r = \left\lceil\frac{N}{B_r} \right\rceil$ blocks $\vQ_1, \dots, \vQ_{T_r}$ of size $B_r \times d$ each,
    and divide $\vK, \vV$ in to $T_c = \left\lceil \frac{N}{B_c} \right\rceil$ blocks $\vK_1, \dots, \vK_{T_c}$ and
    $\vV_1, \dots, \vV_{T_c}$, of size $B_c \times d$ each.
    \STATE Divide $\vO$ into $T_r$ blocks $\vO_i, \dots, \vO_{T_r}$ of size
    $B_r \times d$ each, divide $\ell$ into $T_r$ blocks $\ell_i, \dots, \ell_{T_r}$ of size
    $B_r$ each, divide $m$ into $T_r$ blocks $m_1, \dots, m_{T_r}$ of size $B_r$ each.
    \FOR{$1 \le j \le T_c$}
      \STATE Load $\vK_j, \vV_j$ from HBM to on-chip SRAM.
      \FOR{$1 \le i \le T_r$}
        \STATE Load $\vQ_i, \vO_i, \ell_i, m_i$ from HBM to on-chip SRAM.
        \STATE On chip, compute $\vS_{ij} = \tau \vQ_i \vK_j^T \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute $\vS_{ij}^{\mathrm{masked}} = \textsc{mask}(\vS_{ij})$.
        \STATE On chip, compute $\tilde{m}_{ij} = \mathrm{rowmax}(\vS_{ij}^{\mathrm{masked}}) \in \mathbb{R}^{B_r}$, $\tilde{\vP}_{ij} = \exp(\vS_{ij}^{\mathrm{masked}} - \tilde{m}_{ij}) \in \mathbb{R}^{B_r \times B_c}$ (pointwise),
        $\tilde{\ell}_{ij} = \mathrm{row sum}(\tilde{\vP}_{ij}) \in \mathbb{R}^{B_r}$.
        \STATE On chip, compute $m_i^{\mathrm{new}} = \max(m_i, \tilde{m}_{ij}) \in \mathbb{R}^{B_r}$, $\ell_i^{\mathrm{new}} = e^{m_i - m_i^{\mathrm{new}}} \ell_i + e^{\tilde{m}_{ij} - m_i^{\mathrm{new}}} \tilde{\ell}_{ij} \in \mathbb{R}^{B_r}$.
        \STATE On chip, compute $\tilde{\vP}_{ij}^{\mathrm{dropped}} = \mathrm{dropout}(\tilde{\vP}_{ij}, p_\mathrm{drop})$.
        \STATE Write $\vO_i \leftarrow \diag(\ell_i^{\mathrm{new}})^{-1}(\diag(\ell_i) e^{m_i - m_i^{\mathrm{new}}} \vO_i + e^{\tilde{m}_{ij} - m_i^{\mathrm{new}}}\tilde{\vP}_{ij}^{\mathrm{dropped}} \vV_j)$
        to HBM.
        \STATE Write $\ell_i \leftarrow \ell_i^{\mathrm{new}}$, $m_i \leftarrow m_i^{\mathrm{new}}$ to HBM.
      \ENDFOR
    \ENDFOR
    \STATE Return $\vO, \ell, m, {\cal R}$.
  \end{algorithmic}
\end{algorithm}

\subsection{\sysname: Backward Pass}
\label{sec:algo_bwd_full}

We describe the full details of \sysname backward pass.
Given input sequences $\vQ, \vK, \vV \in \mathbb{R}^{N \times d}$, the output $\vO \in \mathbb{R}^{N \times d}$,
and the output gradient $\vdO$, we want to
compute the input gradients $\vdQ, \vdK, \vdV \in \mathbb{R}^{N \times d}$.

We first describe the standard attention backward pass in~\cref{alg:standard_attn_bwd} for completeness.
\begin{algorithm}[H]
  \caption{\small\label{alg:standard_attn_bwd}Standard Attention Backward Pass}
  \begin{algorithmic}[1]
    \REQUIRE Matrices $\vQ, \vK, \vV, \vdO \in \mathbb{R}^{N \times d}$, $\vP \in \mathbb{R}^{N \times N}$ in HBM.
    \STATE Load $\vP, \vdO$ by blocks from HBM, compute
    $\vdV = \vP^\top \vdO \in \mathbb{R}^{N \times d}$, write $\vdV$ to HBM.
    \STATE Load $\vdO, \vV$ by blocks from HBM, compute
    $\vdP = \vdO \vV^\top \in \mathbb{R}^{N \times N}$, write $\vdP$ to HBM.
    \STATE Read $\vP, \vdP$ from HBM, compute $\vdS \in \mathbb{R}^{N \times N}$ where
    $dS_{ij} = P_{ij} (dP_{ij} - \sum_l P_{il} dP_{il})$, write $\vdS$ to
    HBM.
    \STATE Load $\vdS$ and $\vK$ by blocks from HBM, compute $\vdQ = \vdS\vK$,
    write $\vdQ$ to HBM.
    \STATE Load $\vdS$ and $\vQ$ by blocks from HBM, compute $\vdK = \vdS^\top\vQ$, write $\vdK$ to
    HBM.
    \STATE Return $\vdQ, \vdK, \vdV$.
  \end{algorithmic}
\end{algorithm}

We now make two observations about \sysname backward pass:
\begin{enumerate}
  \item We do not need to store the dropout mask of size $O(N^2)$ from the
  forward pass.
  Instead, we can save the pseudo-random number generator states from
  the forward pass and re-generate the dropout mask in the backward pass.
  This allows us to only use $O(N)$ extra memory.
  \item When computing the softmax gradient, we use~\cref{eq:D_i} to compute
  $D_i = P_{i:}^\top dP_{i:}$ without reducing over $P_{i:}$ and $dP_{i:}$ of size
  $N$ (they might not fit into SRAM).
  Instead we can rewrite $D_i = do_i^\top o_i$ and compute the dot product between
  vectors of size $d$.
\end{enumerate}

The full \sysname backward pass algorithm is in~\cref{alg:bwd_full}.
Conceptually it is just a block version of the derivation
in~\cref{sec:backward}.

\iftoggle{arxiv}{
\begin{algorithm}[H]
} {
\begin{algorithm}[h]
}
  \caption{\small\label{alg:bwd_full}\sysname Backward Pass}
  \begin{algorithmic}[1]
    \REQUIRE Matrices $\vQ, \vK, \vV, \vO, \vdO \in \mathbb{R}^{N \times d}$ in HBM,
    vectors $\ell, m \in \mathbb{R}^N$ in HBM, on-chip SRAM of
    size $M$, softmax scaling constant $\tau \in \mathbb{R}$, masking function
    $\textsc{mask}$, dropout probability $p_\mathrm{drop}$, pseudo-random number
    generator state ${\cal R}$ from the forward pass.
    \STATE Set the pseudo-random number generator state to ${\cal R}$.
    \STATE Set block sizes $B_c = \left\lceil \frac{M}{4d} \right\rceil, B_r = \min \left( \left\lceil \frac{M}{4d} \right\rceil , d \right)$.
    \STATE Divide $\vQ$ into $T_r = \left\lceil\frac{N}{B_r} \right\rceil$ blocks $\vQ_1, \dots, \vQ_{T_r}$ of size $B_r \times d$ each,
    and divide $\vK, \vV$ in to $T_c = \left\lceil \frac{N}{B_c} \right\rceil$ blocks $\vK_1, \dots, \vK_{T_c}$ and
    $\vV_1, \dots, \vV_{T_c}$, of size $B_c \times d$ each.
    \STATE Divide $\vO$ into $T_r$ blocks $\vO_i, \dots, \vO_{T_r}$ of size
    $B_r \times d$ each, divide $\vdO$ into $T_r$ blocks $\vdO_i, \dots, \vdO_{T_r}$
    of size $B_r \times d$ each, divide $\ell$ into $T_r$ blocks $\ell_i, \dots, \ell_{T_r}$ of size
    $B_r$ each, divide $m$ into $T_r$ blocks $m_1, \dots, m_{T_r}$ of size $B_r$ each.
    \STATE Initialize $\vdQ = (0)_{N \times d}$ in HBM and divide it into $T_r$ blocks $\vdQ_1, \dots, \vdQ_{T_r}$ of size $B_r \times d$ each.
    Initialize $\vdK = (0)_{N \times d}, \vdV = (0)_{N \times d}$ in HBM and divide $\vdK, \vdV$ in to $T_c$ blocks $\vdK_1, \dots, \vdK_{T_c}$ and
    $\vdV_1, \dots, \vdV_{T_c}$, of size $B_c \times d$ each.
    \FOR{$1 \le j \le T_c$}
      \STATE Load $\vK_j, \vV_j$ from HBM to on-chip SRAM.
      \STATE Initialize $\tilde{\vdK}_j = (0)_{B_c \times d}, \tilde{\vdV}_j = (0)_{B_c \times d}$ on SRAM.
      \FOR{$1 \le i \le T_r$}
        \STATE Load $\vQ_i, \vO_i, \vdO_i, \vdQ_i, \ell_i, m_i$ from HBM to on-chip SRAM.
        \STATE On chip, compute $\vS_{ij} = \tau \vQ_i \vK_j^T \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute $\vS_{ij}^{\mathrm{masked}} = \textsc{mask}(\vS_{ij})$.
        \STATE On chip, compute $\vP_{ij} = \diag(l_i)^{-1}\exp(\vS_{ij}^{\mathrm{masked}} - m_{i}) \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute dropout mask $\vZ_{ij} \in \mathbb{R}^{B_r \times B_c}$ where
        each entry has value $\frac{1}{1 - p_{\mathrm{drop}}}$ with probability
          $1 - p_\mathrm{drop}$ and value 0 with probability $p_\mathrm{drop}$.
        \STATE On chip, compute
        $\vP_{ij}^{\mathrm{dropped}} = \vP_{ij} \circ \vZ_{ij}$ (pointwise multiply).
        \STATE On chip, compute
        $\tilde{\vdV_j} \leftarrow \tilde{\vdV_j} + (\vP_{ij}^{\mathrm{dropped}})^\top \vdO_i \in \mathbb{R}^{B_c \times d}$.
        \STATE On chip, compute
        $\vdP_{ij}^{\mathrm{dropped}} = \vdO_{i} \vV_j^\top \in \mathbb{R}^{B_r \times B_c}$.
        \STATE On chip, compute
        $\vdP_{ij} = \vdP_{ij}^{\mathrm{dropped}} \circ \vZ_{ij}$ (pointwise multiply).
        \STATE On chip, compute $D_{i} = \mathrm{rowsum}(\vdO_i \circ \vO_i) \in \mathbb{R}^{B_r}$.
        \STATE On chip, compute $\vdS_{ij} = \vP_{ij} \circ (\vdP_{ij} - D_i) \in \mathbb{R}^{B_r \times B_c}$.
        \STATE Write
        $\vdQ_{i} \leftarrow \vdQ_i + \tau \vdS_{ij} \vK_j \in \mathbb{R}^{B_r \times d}$ to HBM.
        \STATE On chip, compute $\tilde{\vdK}_{j} \leftarrow \tilde{\vdK}_j + \tau \vdS_{ij}^\top \vQ_i \in \mathbb{R}^{B_c \times d}$.
      \ENDFOR
      \STATE Write $\vdK_j \leftarrow \tilde{\vdK_j}, \vdV_j \leftarrow \tilde{\vdV_j}$ to HBM.
    \ENDFOR
    \STATE Return $\vdQ, \vdK, \vdV$.
  \end{algorithmic}
\end{algorithm}

We see that similar to the forward pass, the backward pass performs $O(N^2)$
FLOPs and only requires $O(N)$ extra memory beyond inputs, output, output
gradient, and input gradients.

We analyze the IO-complexity of the backward pass, similar to the forward pass (\cref{thm:io_complexity}).
\begin{theorem}\label{thm:io_complexity_bwd}
  Let $N$ be the sequence length, $d$ be the head dimension, and $M$ be size of
  SRAM with $d \leq M \leq Nd$.
  Standard attention (\cref{alg:standard_attn}) backward pass requires $\Theta(Nd + N^2)$ HBM
  accesses, while \sysname backward pass (\cref{alg:bwd_full}) requires
  $\Theta ( N^2 d^2 M^{-1} )$ HBM accesses.
\end{theorem}
The proof is in~\cref{sec:proofs}.

\subsection{Comparison with \citet{rabe2021self}}
\label{subsec:rabe_comparison}

We describe here some similarities and differences between our \sysname
algorithm and the algorithm of \citet{rabe2021self}.

Conceptually, both \sysname and \citet{rabe2021self} operate on blocks of the
attention matrix using the well-established technique of tiling (or softmax
scaling)~\citep{milakov2018online, kitaev2020reformer}.
To reduce the memory footprint, both methods avoid storing the large attention
matrix in the forward pass and recompute it in the backward pass.

The first major difference is that \citet{rabe2021self} focuses on the reducing
the total memory footprint (maximum amount of GPU memory required) while
\sysname focuses on reducing memory accesses (the number of memory
reads/writes).
As mentioned in~\cref{sec:background}, the amount of memory access is the
primary determining factor of runtime.
Reducing memory accesses also necessarily reduces the total amount of memory
required (e.g., if an operation incurs $A$ memory accesses, then its total
memory requirement is at most $A$).
As a result, \sysname is faster than standard attention (2-4$\times$) while
\citet{rabe2021self} is around the same speed or slightly slower than standard
attention.
In terms of total memory required, both methods offer substantial memory saving.

The second difference between the two methods is the way information is summarized
from each block to pass to the next block.
\citet{rabe2021self} summarizes each block with its temporary output along with the
softmax normalization statistics.
At the end of the forward pass, the temporary outputs of all the blocks are combined using
the statistics to produce the final output.
\sysname instead incrementally updates the output (\cref{alg:stream_attn} line
\ref{alg:stream_attn_aggregate}) after processing each block, so only one copy
of the output is needed (instead of $K$ copies for $K$ blocks).
This means that \sysname has smaller total memory requirement compared to \citet{rabe2021self}.

The final major difference is the way the backward pass is computed.
\citet{rabe2021self} uses gradient checkpointing to recompute the attention
matrix and the temporary output of each block.
\sysname instead simplifies the backward pass analytically (\cref{sec:backward,sec:algo_bwd_full}).
It only recomputes the attention matrix and does not recompute the
temporary output of each block.
This reduces the memory requirement for the backward pass and yields speedup.

