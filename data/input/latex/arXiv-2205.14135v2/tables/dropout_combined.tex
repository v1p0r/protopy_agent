\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with dropout}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & \underline{0.66} & \underline{0.67} & 1.43 & 4.82 & 17.47 & 67.29 & - & - & - & - \\
\textbf{Megatron} & 0.88 & 0.90 & 1.49 & 4.73 & 17.41 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 4.06 & 7.28 & 13.68 & 26.98 & 54.27 & 109.39 & 223.80 & - & - & - \\
\textbf{Local Attention} & 1.09 & 1.40 & 1.99 & 5.61 & 19.23 & 38.62 & 77.30 & 154.63 & 311.12 & - \\
\textbf{Linformer} & 1.31 & 1.21 & 1.30 & \underline{1.39} & \underline{3.73} & \underline{7.15} & \underline{14.05} & \underline{27.69} & \underline{55.00} & - \\
\textbf{Smyrf} & 3.00 & 4.37 & 8.05 & 15.66 & 31.04 & 61.64 & 123.04 & 245.65 & - & - \\
\textbf{LSformer} & 3.07 & 3.17 & 4.31 & 10.89 & 31.54 & 61.78 & 121.56 & 240.94 & - & - \\
\hline
\textbf{Block Sparse} & 2.54 & 2.52 & 3.71 & 5.44 & 13.29 & 39.19 & - & - & - & - \\
\textbf{Longformer} & 2.47 & 2.49 & 2.51 & 3.10 & 10.39 & 22.49 & 60.44 & - & - & - \\
\textbf{BigBird} & 2.51 & 2.49 & 2.52 & 3.40 & 10.97 & 23.89 & 63.28 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.35} & \textbf{0.36} & \textbf{0.80} & 2.52 & 9.16 & 36.70 & 146.13 & 583.45 & 2332.01 & \underline{9323.63} \\
\textbf{Block-Sparse \sysname} & 0.91 & 0.83 & \underline{0.94} & \textbf{0.92} & \textbf{1.83} & \textbf{3.50} & \textbf{7.02} & \textbf{13.56} & \textbf{26.71} & \textbf{53.92} \\
\bottomrule
\end{tabular}
\label{tab:dropout_combined}
\end{table}