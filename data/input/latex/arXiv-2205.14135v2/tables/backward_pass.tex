\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 0.26 & 0.29 & 0.78 & 2.44 & 8.82 & 33.87 & - & - & - & - \\
\textbf{Megatron} & 0.29 & 0.30 & 0.80 & 2.59 & 8.86 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 2.18 & 4.21 & 8.14 & 16.12 & 32.02 & 63.84 & 127.60 & - & - & - \\
\textbf{Local Attention} & 0.51 & 0.64 & 1.28 & 3.60 & 12.52 & 25.08 & 50.22 & 100.23 & 200.66 & - \\
\textbf{Linformer} & 0.69 & 0.76 & 0.69 & \underline{0.80} & \underline{2.04} & \underline{3.88} & \underline{7.67} & \underline{15.04} & \underline{30.11} & \underline{63.15} \\
\textbf{Smyrf} & 1.24 & 2.49 & 4.77 & 9.42 & 18.65 & 37.12 & 74.15 & 148.35 & - & - \\
\textbf{LSformer} & 1.68 & 1.61 & 3.02 & 7.40 & 19.72 & 38.27 & 74.89 & 147.99 & - & - \\
\hline
\textbf{Block Sparse} & 1.24 & 1.25 & 2.04 & 2.91 & 6.78 & 19.67 & - & - & - & - \\
\textbf{Longformer} & 1.27 & 1.23 & 1.24 & 1.85 & 4.99 & 10.21 & 24.89 & - & - & - \\
\textbf{BigBird} & 1.43 & 1.50 & 1.44 & 1.69 & 5.25 & 10.86 & 26.26 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.11} & \underline{0.16} & \underline{0.52} & 1.62 & 5.45 & 21.57 & 84.75 & 336.00 & 1338.56 & 5343.19 \\
\textbf{Block-Sparse \sysname} & \underline{0.11} & \textbf{0.12} & \textbf{0.16} & \textbf{0.38} & \textbf{1.20} & \textbf{2.34} & \textbf{4.69} & \textbf{9.10} & \textbf{18.74} & \textbf{37.04} \\
\bottomrule
\end{tabular}
\label{tab:backward_pass}
\end{table}