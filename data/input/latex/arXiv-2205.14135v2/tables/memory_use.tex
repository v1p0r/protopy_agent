\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Memory usage (MB) of various exact/approximate/sparse attention mechanisms by sequence length. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 36 & 104 & 336 & 1184 & 4416 & 17024 & - & - & - & - \\
\textbf{Megatron} & 36 & 104 & 336 & 1184 & 4416 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 377 & 754 & 1508 & 3016 & 6033 & 12067 & 24134 & - & - & - \\
\textbf{Local Attention} & 53 & 110 & 232 & 592 & 1696 & 3392 & 6784 & 13568 & 27136 & - \\
\textbf{Linformer} & 25 & 52 & 114 & 287 & 832 & 1652 & 3292 & 6572 & 13132 & 26252 \\
\textbf{Smyrf} & 217 & 434 & 868 & 1737 & 3474 & 6947 & 13894 & 27788 & - & - \\
\textbf{LSformer} & 72 & 152 & 333 & 796 & 2540 & 5068 & 10125 & 20240 & - & - \\
\hline
\textbf{Block Sparse} & 33 & 82 & 228 & 408 & 910 & 2401 & - & - & - & - \\
\textbf{Longformer} & 30 & 61 & 124 & 277 & 681 & 1370 & 2748 & - & - & - \\
\textbf{BigBird} & 33 & 66 & 131 & 294 & 708 & 1431 & 2872 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{22} & \textbf{44} & \textbf{104} & \textbf{209} & \textbf{418} & \textbf{836} & \textbf{1672} & \textbf{3344} & \textbf{6688} & \textbf{13376} \\
\textbf{Block-Sparse \sysname} & \underline{22} & \underline{44} & \underline{104} & \underline{209} & \underline{418} & \underline{836} & \underline{1672} & \underline{3344} & \underline{6690} & \underline{13384} \\
\bottomrule
\end{tabular}
\label{tab:memory}
\end{table}