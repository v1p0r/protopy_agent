\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with masking}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 0.44 & 0.46 & 1.53 & 5.33 & 20.34 & 79.87 & - & - & - & - \\
\textbf{Megatron} & 0.29 & 0.31 & 0.65 & 1.95 & 6.49 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 2.31 & 4.47 & 8.68 & 17.20 & 34.14 & 68.09 & 136.02 & - & - & - \\
\textbf{Local Attention} & 0.51 & 0.62 & 1.30 & 3.81 & 13.33 & 26.72 & 53.41 & 106.82 & 214.15 & - \\
\textbf{Linformer} & 0.76 & 0.81 & 0.94 & \underline{0.87} & \underline{2.24} & \underline{4.25} & \underline{8.35} & \underline{16.38} & \underline{32.67} & \underline{72.11} \\
\textbf{Smyrf} & 1.34 & 2.77 & 5.30 & 10.46 & 20.73 & 41.27 & 82.41 & 164.86 & - & - \\
\textbf{LSformer} & 1.66 & 1.61 & 3.09 & 7.42 & 19.68 & 38.35 & 74.92 & 147.86 & - & - \\
\hline
\textbf{Block Sparse} & 1.24 & 1.25 & 2.04 & 2.91 & 6.78 & 19.67 & - & - & - & - \\
\textbf{Longformer} & 1.27 & 1.23 & 1.24 & 1.85 & 4.99 & 10.21 & 24.89 & - & - & - \\
\textbf{BigBird} & 1.43 & 1.50 & 1.44 & 1.69 & 5.25 & 10.86 & 26.26 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.21} & \textbf{0.22} & \underline{0.62} & 1.84 & 5.77 & 22.25 & 86.21 & 338.91 & 1343.91 & 5361.09 \\
\textbf{Block-Sparse \sysname} & \underline{0.22} & \underline{0.22} & \textbf{0.26} & \textbf{0.57} & \textbf{1.55} & \textbf{3.13} & \textbf{5.98} & \textbf{12.21} & \textbf{23.49} & \textbf{47.85} \\
\bottomrule
\end{tabular}
\label{tab:masking_backward_pass}
\end{table}