\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with dropout and masking}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 0.37 & 0.49 & 1.66 & 5.81 & 22.32 & 87.67 & - & - & - & - \\
\textbf{Megatron} & 0.35 & 0.32 & 0.77 & 2.42 & 8.43 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 2.37 & 4.59 & 8.91 & 17.68 & 35.13 & 70.05 & 140.01 & - & - & - \\
\textbf{Local Attention} & 0.55 & 0.62 & 1.49 & 4.03 & 13.78 & 27.61 & 55.20 & 110.27 & 221.40 & - \\
\textbf{Linformer} & 0.89 & 0.80 & 0.81 & \underline{0.93} & \underline{2.48} & \underline{4.75} & \underline{9.29} & \underline{18.27} & \underline{36.53} & - \\
\textbf{Smyrf} & 1.41 & 2.83 & 5.43 & 10.72 & 21.25 & 42.31 & 84.48 & 168.95 & - & - \\
\textbf{LSformer} & 1.75 & 1.76 & 3.01 & 7.50 & 20.07 & 39.08 & 76.39 & 150.82 & - & - \\
\hline
\textbf{Block Sparse} & 1.29 & 1.28 & 2.18 & 3.04 & 7.27 & 21.16 & - & - & - & - \\
\textbf{Longformer} & 1.27 & 1.31 & 1.29 & 2.04 & 5.24 & 10.74 & 25.95 & - & - & - \\
\textbf{BigBird} & 1.33 & 1.28 & 1.32 & 1.81 & 5.55 & 11.44 & 27.45 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.30} & \textbf{0.26} & \underline{0.68} & 2.02 & 6.84 & 26.89 & 105.70 & 418.96 & 1666.89 & \underline{6660.44} \\
\textbf{Block-Sparse \sysname} & \textbf{0.30} & \underline{0.27} & \textbf{0.29} & \textbf{0.59} & \textbf{1.50} & \textbf{2.94} & \textbf{5.82} & \textbf{11.85} & \textbf{23.98} & \textbf{47.61} \\
\bottomrule
\end{tabular}
\label{tab:dropout_masking_backward_pass}
\end{table}