\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with dropout}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 0.44 & 0.35 & 0.90 & 2.94 & 10.77 & 41.67 & - & - & - & - \\
\textbf{Megatron} & 0.28 & 0.33 & 0.92 & 2.94 & 10.80 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 2.24 & 4.34 & 8.39 & 16.62 & 33.02 & 65.77 & 131.52 & - & - & - \\
\textbf{Local Attention} & 0.51 & 0.58 & 1.41 & 3.71 & 12.96 & 25.98 & 51.94 & 103.72 & 207.78 & - \\
\textbf{Linformer} & 0.84 & 0.74 & 0.79 & \underline{0.85} & \underline{2.28} & \underline{4.37} & \underline{8.66} & \underline{17.02} & \underline{33.78} & - \\
\textbf{Smyrf} & 1.27 & 2.56 & 4.90 & 9.66 & 19.16 & 38.13 & 76.17 & 152.39 & - & - \\
\textbf{LSformer} & 1.67 & 1.77 & 3.03 & 7.52 & 20.10 & 39.13 & 76.35 & 150.83 & - & - \\
\hline
\textbf{Block Sparse} & 1.27 & 1.36 & 2.15 & 3.04 & 7.27 & 21.18 & - & - & - & - \\
\textbf{Longformer} & 1.28 & 1.34 & 1.38 & 1.98 & 5.24 & 10.74 & 25.95 & - & - & - \\
\textbf{BigBird} & 1.48 & 1.47 & 1.50 & 1.81 & 5.57 & 11.38 & 27.43 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.15} & \underline{0.18} & \underline{0.58} & 1.86 & 6.50 & 26.21 & 104.27 & 416.10 & 1661.92 & \underline{6643.01} \\
\textbf{Block-Sparse \sysname} & \underline{0.17} & \textbf{0.17} & \textbf{0.17} & \textbf{0.40} & \textbf{1.10} & \textbf{2.04} & \textbf{4.43} & \textbf{9.33} & \textbf{18.28} & \textbf{37.31} \\
\bottomrule
\end{tabular}
\label{tab:dropout_backward_pass}
\end{table}