\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with masking}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 0.80 & 0.81 & 2.08 & 7.23 & 27.51 & 107.58 & - & - & - & - \\
\textbf{Megatron} & 0.81 & 0.83 & 1.09 & 3.36 & 12.39 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 4.16 & 7.46 & 14.06 & 27.68 & 55.66 & 112.15 & 229.37 & - & - & - \\
\textbf{Local Attention} & 1.39 & 1.68 & 2.08 & 5.83 & 20.04 & 40.16 & 80.44 & 161.35 & 325.11 & - \\
\textbf{Linformer} & 1.51 & 1.42 & 1.56 & \underline{1.67} & \underline{3.67} & \underline{6.99} & \underline{13.63} & \underline{26.77} & \underline{53.36} & \underline{117.56} \\
\textbf{Smyrf} & 3.38 & 4.93 & 9.07 & 17.66 & 34.94 & 69.55 & 138.72 & 277.41 & - & - \\
\textbf{LSformer} & 3.08 & 3.10 & 4.26 & 10.90 & 31.59 & 61.72 & 121.51 & 241.18 & - & - \\
\hline
\textbf{Block Sparse} & 2.39 & 2.40 & 3.31 & 5.02 & 12.25 & 35.94 & - & - & - & - \\
\textbf{Longformer} & 2.36 & 2.34 & 2.38 & 2.94 & 9.83 & 21.35 & 58.12 & - & - & - \\
\textbf{BigBird} & 2.35 & 2.35 & 2.37 & 3.25 & 10.36 & 22.57 & 60.63 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.32} & \textbf{0.30} & \underline{0.83} & 2.37 & 7.95 & 30.77 & 119.98 & 473.65 & 1883.43 & 7513.01 \\
\textbf{Block-Sparse \sysname} & \underline{0.34} & \underline{0.34} & \textbf{0.36} & \textbf{0.69} & \textbf{1.85} & \textbf{3.89} & \textbf{7.16} & \textbf{14.85} & \textbf{30.46} & \textbf{60.03} \\
\bottomrule
\end{tabular}
\label{tab:masking_combined}
\end{table}