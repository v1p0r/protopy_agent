\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with dropout}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & \underline{0.26} & \underline{0.24} & 0.57 & 1.80 & 6.56 & 25.34 & - & - & - & - \\
\textbf{Megatron} & 0.27 & 0.27 & 0.56 & 1.88 & 6.56 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 1.83 & 2.96 & 5.31 & 10.33 & 21.19 & 43.42 & 91.96 & 201.34 & - & - \\
\textbf{Local Attention} & 0.51 & 0.60 & 0.78 & 2.01 & 6.23 & 12.52 & 25.07 & 50.50 & 102.18 & - \\
\textbf{Linformer} & 0.47 & 0.37 & \underline{0.49} & \textbf{0.52} & \underline{1.37} & \underline{2.65} & \underline{5.12} & \underline{10.13} & \underline{20.25} & \underline{44.16} \\
\textbf{Smyrf} & 2.12 & 2.01 & 3.15 & 5.97 & 11.83 & 23.36 & 46.48 & 92.72 & - & - \\
\textbf{LSformer} & 1.28 & 1.33 & 1.51 & 3.39 & 11.40 & 22.54 & 44.96 & 89.85 & 179.73 & - \\
\hline
\textbf{Block Sparse} & 1.03 & 1.00 & 1.72 & 2.39 & 5.96 & 17.88 & - & - & - & - \\
\textbf{Longformer} & 1.02 & 1.03 & 1.03 & 1.73 & 5.10 & 11.63 & 34.22 & - & - & - \\
\textbf{BigBird} & 0.99 & 1.03 & 1.01 & 1.58 & 5.36 & 12.27 & 35.56 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.10} & \textbf{0.10} & \textbf{0.22} & 0.83 & 2.81 & 10.38 & 41.63 & 167.01 & 668.74 & 2678.11 \\
\textbf{Block-Sparse \sysname} & 0.54 & 0.51 & 0.68 & \underline{0.61} & \textbf{0.67} & \textbf{1.10} & \textbf{1.89} & \textbf{3.71} & \textbf{7.18} & \textbf{14.41} \\
\bottomrule
\end{tabular}
\label{tab:dropout_forward_pass}
\end{table}