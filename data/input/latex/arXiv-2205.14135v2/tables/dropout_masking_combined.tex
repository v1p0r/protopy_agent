\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Forward pass + backward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with dropout and masking}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 0.84 & 0.86 & 2.35 & 8.29 & 31.75 & 124.19 & - & - & - & - \\
\textbf{Megatron} & 0.87 & 0.89 & 1.33 & 4.21 & 16.50 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 4.30 & 7.76 & 14.60 & 28.74 & 57.79 & 116.34 & 237.57 & - & - & - \\
\textbf{Local Attention} & 1.40 & 1.60 & 2.06 & 6.06 & 20.94 & 42.01 & 84.08 & 168.48 & 339.45 & - \\
\textbf{Linformer} & 1.57 & 1.49 & 1.55 & \underline{1.60} & \underline{4.19} & \underline{8.04} & \underline{15.71} & \underline{30.92} & \underline{61.47} & - \\
\textbf{Smyrf} & 3.41 & 5.08 & 9.35 & 18.18 & 36.03 & 71.68 & 143.04 & 285.87 & - & - \\
\textbf{LSformer} & 3.08 & 3.10 & 4.26 & 10.90 & 31.59 & 61.72 & 121.51 & 241.18 & - & - \\
\hline
\textbf{Block Sparse} & 2.54 & 2.52 & 3.71 & 5.44 & 13.29 & 39.19 & - & - & - & - \\
\textbf{Longformer} & 2.47 & 2.49 & 2.51 & 3.10 & 10.39 & 22.49 & 60.44 & - & - & - \\
\textbf{BigBird} & 2.51 & 2.49 & 2.52 & 3.40 & 10.97 & 23.89 & 63.28 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.43} & \textbf{0.41} & \underline{0.95} & 2.55 & 9.56 & 37.49 & 147.75 & 586.61 & 2339.11 & \underline{9341.30} \\
\textbf{Block-Sparse \sysname} & \underline{0.44} & \underline{0.44} & \textbf{0.45} & \textbf{0.89} & \textbf{1.95} & \textbf{4.12} & \textbf{7.64} & \textbf{16.60} & \textbf{32.73} & \textbf{64.11} \\
\bottomrule
\end{tabular}
\label{tab:dropout_masking_combined}
\end{table}