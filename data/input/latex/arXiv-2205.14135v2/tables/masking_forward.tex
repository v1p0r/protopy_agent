\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with masking}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 0.30 & 0.30 & 0.63 & 1.93 & 7.08 & 27.45 & 112.90 & - & - & - \\
\textbf{Megatron} & 0.45 & 0.41 & 0.43 & 1.52 & 5.80 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 1.87 & 3.00 & 5.37 & 10.43 & 21.40 & 43.83 & 92.80 & 203.24 & - & - \\
\textbf{Local Attention} & 0.70 & 0.81 & 1.02 & 2.09 & 6.64 & 13.34 & 26.77 & 54.02 & 110.11 & - \\
\textbf{Linformer} & 0.63 & 0.50 & 0.67 & \underline{0.65} & \underline{1.36} & \underline{2.60} & \underline{5.04} & \underline{9.92} & \underline{19.69} & \underline{43.47} \\
\textbf{Smyrf} & 2.38 & 2.32 & 3.76 & 7.16 & 14.14 & 28.09 & 55.98 & 111.73 & - & - \\
\textbf{LSformer} & 1.22 & 1.29 & 1.44 & 3.28 & 10.99 & 21.72 & 43.29 & 86.32 & 172.76 & - \\
\hline
\textbf{Block Sparse} & 0.96 & 1.04 & 1.66 & 2.16 & 5.41 & 16.15 & - & - & - & - \\
\textbf{Longformer} & 0.99 & 0.98 & 0.99 & 1.56 & 4.79 & 11.07 & 32.98 & - & - & - \\
\textbf{BigBird} & 0.96 & 1.02 & 1.02 & 1.48 & 5.05 & 11.59 & 34.16 & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.03} & \textbf{0.04} & \underline{0.17} & 0.68 & 2.28 & 8.40 & 33.55 & 134.14 & 537.50 & 2150.88 \\
\textbf{Block-Sparse \sysname} & \underline{0.05} & \textbf{0.04} & \textbf{0.05} & \textbf{0.11} & \textbf{0.35} & \textbf{0.68} & \textbf{1.33} & \textbf{2.54} & \textbf{5.34} & \textbf{10.73} \\
\bottomrule
\end{tabular}
\label{tab:masking_forward_pass}
\end{table}