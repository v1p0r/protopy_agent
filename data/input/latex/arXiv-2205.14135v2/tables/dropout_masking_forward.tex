\begin{table}
\centering
\scriptsize
\captionsetup{font=small}
\caption{Forward pass runtime (ms) of various exact/approximate/sparse attention mechanisms by sequence length, \textbf{with dropout and masking}. Best in \textbf{bold}, second best \underline{underlined}.}
\begin{tabular}{@{}r|cccccccccc@{}}
\toprule
\textbf{Attention Method} & 128 & 256 & 512 & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 \\
\hline
\textbf{PyTorch Attention} & 0.36 & 0.34 & 0.78 & 2.54 & 9.33 & 36.33 & - & - & - & - \\
\textbf{Megatron} & 0.40 & 0.40 & 1.10 & 3.65 & 16.19 & - & - & - & - & - \\
\hline
\textbf{Reformer} & 2.03 & 3.15 & 5.67 & 11.02 & 22.59 & 46.14 & 97.38 & 212.13 & - & - \\
\textbf{Local Attention} & 0.83 & 0.86 & 1.01 & 2.20 & 7.13 & 14.32 & 28.60 & 57.79 & 117.67 & - \\
\textbf{Linformer} & 0.67 & 0.52 & 0.69 & \underline{0.71} & \underline{1.65} & \underline{3.18} & \underline{6.15} & \underline{12.16} & \underline{24.17} & \underline{52.39} \\
\textbf{Smyrf} & 2.27 & 2.34 & 3.91 & 7.44 & 14.71 & 29.22 & 58.27 & 116.41 & - & - \\
\textbf{LSformer} & 1.18 & 1.27 & 1.34 & 3.38 & 11.40 & 22.55 & 44.95 & 89.76 & 179.66 & - \\
\hline
\textbf{Block Sparse} & 1.12 & 1.11 & 2.13 & 2.77 & 6.95 & 20.91 & - & - & - & - \\
\textbf{Longformer} & 1.22 & 1.14 & 1.08 & 1.95 & 5.72 & 12.98 & - & - & - & - \\
\textbf{BigBird} & 1.13 & 1.12 & 1.12 & 1.77 & 6.03 & 13.68 & - & - & - & - \\
\hline
\textbf{\sysname} & \textbf{0.04} & \underline{0.06} & \underline{0.21} & 0.82 & 2.85 & 10.41 & 41.74 & 167.19 & 670.76 & 2682.35 \\
\textbf{Block-Sparse \sysname} & \underline{0.06} & \textbf{0.06} & \textbf{0.06} & \textbf{0.12} & \textbf{0.44} & \textbf{0.86} & \textbf{1.70} & \textbf{3.29} & \textbf{6.55} & \textbf{13.34} \\
\bottomrule
\end{tabular}
\label{tab:dropout_masking_forward_pass}
\end{table}