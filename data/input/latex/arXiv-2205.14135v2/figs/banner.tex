\iftoggle{icmlworkshop}{
\begin{figure*}[t]
}{
\begin{figure}[t]
}
\centering
\includegraphics[width=5.5in]{figs/banner_pdf.pdf}
\caption{
\textbf{Left:} \sysname uses tiling to prevent materialization of the large $N \times N$ attention matrix (dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), \sysname loops through blocks of the $\vK$ and $\vV$ matrices and loads them to fast on-chip SRAM.
In each block, \sysname loops over blocks of $\vQ$ matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM.
\textbf{Right:} Speedup over the PyTorch implementation of attention on GPT-2.
\sysname does not read and write the large $N\times N$ attention matrix to HBM, resulting in an 7.6$\times$ speedup on the attention computation.}
\label{fig:banner}
\iftoggle{arxiv}{}{
\vspace{-2em}
}
\iftoggle{icmlworkshop}{
\end{figure*}
}{
\end{figure}
}
