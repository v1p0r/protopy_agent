\section{Related Works} 

\subsection{Parameter-efficient Fine-tuning (PEFT)}
% \qq{you need to explicitly discuss at least LoRA, then explain its drawbacks compared to our method (since we build on top of LoRA), our method uses adaptive parameters xxx. Also, what about AdaLoRA>}
PEFT methods aim to tune {\lmabbr}s with limited resources by updating a small number of parameters~\citep{lialin2023scaling}, mainly falling into three categories: selective, additive, and dynamic. Selective methods focus on tuning a subset of parameters in {\lmabbr}s with pre-defined rules~\citep{ben-zaken-etal-2022-bitfit} or importance metrics~\citep{sung2021training,guo-etal-2021-parameter}. Additive methods tune injected layer modules~\citep{houlsby2019parameter,Pfeiffer2020AdapterFusionNT} or embeddings~\citep{lester-etal-2021-power,li2021prefix}. For example, LoRA~\citep{hu_lora_2021} tunes low-rank decomposed layers to avoid inference cost overhead. However, LoRA keeps the tuning layer shapes static without dynamic adjustments. Dynamic methods~\citep{he-etal-2022-sparseadapter} adjust tuning parameters during training. For instance, AdaLoRA~\citep{zhang2023adaptive} gradually reduces tuning parameters but does not benefit inference efficiency. Compared to these methods, {\ourmethod} adaptively adjusts the pruning and tuning parameters simultaneously, improving training and inference efficiency.
% In the meantime, even though IncreLoRA~\citep{zhang2023increlora} reverses the allocation strategy to increase LoRA parameters during training, the SVD decomposition used in it would incur training time and memory overhead compared to LoRA. 
% Furthermore, using PEFT alone does not bring inference efficiency gains, and adding non-linear modules for tuning even costs inference speed degradation. 
% \qq{also discuss their inference is not faster than fine-tuned models, some adapter methods (adding parameters) are even slower, maybe group these PEFT methods into three categories: only tuning parameters, add tunable parameters, dynamically allocate parameters (are they slower in inference too?). }
% \bowen{Sure. How about just two: static additive methods except LoRA bring inference efficiency degradation, and dynamic selective methods cost peak memory overhead at the start of the training. BitFit is a kind of ``corner case'' here which is a static selective method.}

\subsection{Model Compression}
% \qq{make it two paragraphs, like quantization and pruning, explain each}
% \qq{change to pruning and quantization (Or just model compression because next section you say combining compression and PEFT)? Unstructured pruning is not our focus because they do not provide speedup (need specialized software/hardware). Also need to briefly discuss quantization (some quantization does provide inference speedup?) } \bowen{Edited it as briefly talking about quantization, then pruning, and specifically for structured pruning, but I think it looks too long right now. }
% \hanna{1) are these models: lora + pruning or lora + quantization? does it mean lora + sparse pruning exists, but here apt is lora + structured pruning? what does it mean that structured pruning provides ubiquitous inference speedup?  }
% Quantization converts parameters to low-bit datatypes for memory reduction. QLoRA~\citep{dettmers2023qlora} quantizes {\lmabbr} during fine-tuning while AWQ~\citep{lin2023awq} conducts fast quantization after fine-tuning, yet the speedup gain of quantization methods needs specific framework support, which is not as ubiquitous and effective as structured pruning.
Model compression methods like quantization and pruning boost inference efficiency. Quantization aims to reduce {\lmabbr}s' memory consumption via converting parameters to low-bit data types~\citep{Frantar2022GPTQAP,Dettmers2022LLMint88M,lin2023awq}. However, despite reducing {\lmabbr}'s memory consumption, the speedup benefits of quantization require specific framework support, which limits their adaptability.
Pruning~\citep{LeCun1989OptimalBD,Han2015DeepCC,frankle2018lottery,xu2021rethinking} aims to discard unimportant parameters in {\lmabbr}s for inference efficiency. Unstructured pruning~\citep{sanh_movement_2020} prunes sparse parameters in {\lmabbr}s, which requires dedicated hardware support for efficiency improvements. Meanwhile, structured pruning~\citep{lagunas_block_2021,xia_structured_2022} prunes consistent blocks in transformer layers (MHA heads, FFN neurons, and model dimensions) for ubiquitous inference efficiency gains. Such pruning often uses knowledge distillation~\citep{Hinton2015DistillingTK}, which causes more training costs. Post-training pruning~\citep{kwon_fast_2022,Frantar2023SparseGPTML} aims to prune fine-tuned models with limited extra costs but requires initialization from fully fine-tuned models. Moreover, task-agnostic pruning~\citep{sun2023simple,ma2023llm} cannot achieve on-par performance with task-specific pruning.

\subsection{Combining Compression and PEFT}
Combining model compression and PEFT might achieve both training and inference efficiency improvements: QLoRA~\citep{dettmers2023qlora} and QA-LoRA~\citep{xu2023qa} bring quantization and LoRA together for large {\lmabbr} tuning. SPA~\citep{hedegaard_structured_2022} combines structured pruning and Compacter~\citep{karimi2021compacter}, yet suffers substantial performance loss. CPET~\citep{zhao2023cpet} leverages different task-agnostic model compression methods together with LoRA and knowledge distillation, but the performance loss becomes notable specifically when structured pruning is applied. PST~\citep{ijcai2022p586} and LRP~\citep{zhang2023pruning} also explored the combination of LoRA and pruning, yet their performance degradations are also substantial because their tuning parameters are static.
% \qq{why they are worse, any one sentence explanation will be helpful, another drawback for those unstructured pruning is they need dedicated hardware/framework to support inference speedup}
In contrast, {\ourmethod} identifies tuning and pruning parameters based on their salience in fine-tuning, which can improve training and inference efficiency under a new paradigm with minimal performance loss. 
% \qq{use is not a good word, something like considering tuning and pruning from parameters capacity view xxx, improving both training and inference in a new way xxx},
