\section{Conclusion}
We design {\ourmethod} to adaptively identify {\lmabbr}s' pruning and tuning parameters during fine-tuning, improving both training and inference efficiency. {\ourmethod} prunes small {\lmabbr}s faster while pruning large {\lmabbr}s with less memory consumption. With using similar memory costs as LoRA, {\ourmethod} prunes small {\lmabbr}s 8$\times$ faster than the LoRA plus pruning baseline. In large {\lmabbr} pruning, {\ourmethod} maintains 87\% performance with only 30\% pruning memory usage when 70\% {\lmabbr} parameter retained. {\ourmethod} opens new directions to pruning {\lmabbr}s in fine-tuning for resource-limited settings, allowing wider usage of {\lmabbr}s in practical applications. In the future, we could adapt {\ourmethod} to more PEFT architectures and target better performance-efficiency trade-offs for billion-level large {\lmabbr}s. Meanwhile, we hope future research will continue to find efficient and accurate techniques to identify salient structures in {\lmabbr}s based on our formulated setting.
