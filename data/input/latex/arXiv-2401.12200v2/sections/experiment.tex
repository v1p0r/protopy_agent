\section{Experiments}

% \input{tables/roberta_main_results}

% \input{tables/t5_main_results}
\input{tables/merged_main_results_time}

\input{tables/llama_7b_results_time}

To evaluate the training and inference efficiency gains of {\ourmethod}, we compare it with the combined use of PEFT with pruning and distillation baselines. We first describe the natural language understanding and generation tasks targeting different {\lmabbr} backbones, then the setup of baselines and {\ourmethod}. We then report task performance, speed, and memory usage for training and inference costs. 
% \qq{add a few sentences for the evaluation overview.}

\subsection{Tasks}
% \hanna{it seems that you also apply APT to llama based models, right? in that case, start with this: We add APT to the following language models, bert, roberta, T5 and llama; for bert and t5, we evaluate on ... For llama, we evaluate on ... , following... }

We apply {\ourmethod} to BERT~\citep{devlin-etal-2019-bert}, RoBERTa~\citep{liu2019roberta},  T5\citep{raffel2020exploring}\footnote{For fair comparisons, we use the t5-lm-adapt model, which is only pre-trained on the C4 corpus to make sure the initial {\lmabbr} does not observe downstream tasks in pre-training.}, and LLaMA~\citep{touvron2023llama}. 
For BERT, RoBERTa, and T5 models, we train and evaluate on SST2 and MNLI datasets from the GLUE benchmark~\citep{wang2018glue} and report the dev set accuracy. We also train and evaluate $\text{RoBERTa}_{\text{base}}$ on SQuAD v2.0~\citep{rajpurkar-etal-2018-know} and report the dev set F1 score. For T5 models, we also fine-tune them on CNN/DM~\citep{Nallapati2016AbstractiveTS} and report the ROUGE 1/2/L scores. Meanwhile, We use the GPT-4 generated Alpaca dataset~\citep{alpaca} to fine-tune large LLaMA models and evaluate them with the lm-eval-harness package~\citep{eval-harness} on four tasks from the Open LLM Leaderboard, namely 25-shot ARC~\citep{clark2018think}, 10-shot HellaSwag~\citep{zellers-etal-2019-hellaswag}, 5-shot MMLU~\citep{hendrycks2021measuring}, and zero-shot TruthfulQA~\citep{lin-etal-2022-truthfulqa}.
% and AlpacaEval~\citep{alpaca_eval} to test the model's generalization and instruction-following capabilities.

\subsection{Baselines}
% \hanna{provide a high level overview of what type of baselines you are slecting here? only PEFT methods? only pruning methods? then basic approaches for prune + PEFT, then sota approaches for prune + PEFT? }
% \qq{the first key baseline is Adapter+Pruning (in our case LoRA+Prune), we compare both the training and inference efficiency. Then you can describe other baselines to compare either training or inference efficiency. Also remember the two goals for our work, we make smaller models faster during training with more memory? and we make larger models train less memory while inference is still faster etc. Describe the baselines for these two scenarios. }

% Since {\ourmethod} optimizes training and inference efficiency together with task performance, we compare it to baselines that jointly use PEFT and pruning. 
% \bowen{I get it. Probably only mentioning the mixed-use baselines are enough, instead of pointing out FT, LoRA individually. I'll revise it later.}

% \noindent \textbf{Fine-tuning (FT)}. We compare {\ourmethod} to full fine-tuning without pruning on million-parameter scaled language models, where all parameters except for the embedding weights (in-line with existing structured pruning settings) are being tuned simultaneously. We do not compare our method to full fine-tuning with billion-level {\lmabbr}s, since the memory consumption would be too large to fit in single-card GPU training settings, where most parameter-efficient methods could.

% \noindent \textbf{LoRA}~\citep{hu_lora_2021} tunes language models with low-rank adaptation. Similar to existing usages of LoRA, we only tune the query and value layers in all MHA layers, including self-attention and cross-attention, with the inner rank set as 8 and the scaling factor set as 2. 
We validate the efficiency benefits of {\ourmethod} for both training and inference by comparing with PEFT, pruning, and distillation methods, along with their combinations. 

\noindent \textbf{LoRA+Prune}: a post-training pruning method over on LoRA-tuned {\lmabbr}s. We use Mask Tuning~\citep{kwon_fast_2022}, a state-of-the-art post-training structured pruning method based on fisher information. Due to that post-training pruning performs poorly on high-sparsity settings, we retrain the pruned {\lmabbr} after pruning to recover its performance.
% We apply MT to LoRA-tuned models and also add two baselines $\text{MT}_{\text{+train}}$ and $\text{MT}_{\text{+distill}}$ denoting conducting retraining or distilling with LoRA after MT.
% \qq{this also has pruning? then we should name it LoRA+Prune+distillation}

% \todo{CoFi without LoRA, fully fine-tuning on RoBERTa -> Pruning + Distillation}

\noindent \textbf{Prune+Distill}: knowledge distillation has been proved to be a key technique in recovering pruned {\lmabbr}s' task accuracy. In particular, we use the state-of-the-art pruning plus distillation method called CoFi~\citep{xia_structured_2022} which uses $L_0$ regularization for pruning plus dynamic layer-wise distillation objectives. We only compare {\ourmethod} to CoFi with RoBERTa models since the training memory usage of CoFi is too high for larger {\lmabbr}s.

\noindent \textbf{LoRA+Prune+Distill}: to reduce the training memory consumption in pruning and distillation, a simple baseline is to conduct CoFi pruning and distillation but with LoRA parameters tuned only. More specifically, only the $L_0$ module and LoRA parameters are tunable under this setting.  

\noindent \textbf{LLMPruner}~\citep{ma2023llm}: LLMPruner is the state-of-the-art task-agnostic pruning method on LLaMA that prunes its blocks or channels based on salience metrics while using LoRA for fast performance recovery. We compare {\ourmethod} to LLMPruner with fine-tuning on the same GPT-4 generated Alpaca data for fair comparisons.

We also compare {\ourmethod} to PST~\citep{ijcai2022p586} and LRP~\citep{zhang2023pruning}, which are the state-of-the-art parameter-efficient unstructured and structured pruning methods on $\text{BERT}$ model. We leave these results in \cref{sec:appendix-additional-exp}.

% \noindent \textbf{LLM-pruner} \bowen{also comparing to it on LLaMA 2 if have time}


\subsection{Evaluation Metrics}
% \hanna{start this paragraph with a title evaluation metrics} 
% \hanna{all these are scattered. You need to organize them better. Start with Training Efficiency metrics: we report ... then, Inference efficiency metrics: ... } 
We evaluate {\ourmethod} and baselines on training and inference efficiency, measured in runtime memory and time consumption as follows:

\noindent\textbf{Training Efficiency Metrics}: we report relative training peak memory (Train. Mem.) and relative training speed measured by time to accuracy (TTA\footnote{For instance, 97\% TTA denotes the time spent reaching 97\% of the fully fine-tuned model's performance})~\citep{10.1145/3352020.3352024} compared to full finetuning. For fair comparisons, we consider the training time of the teacher model plus the student for methods using knowledge distillation.

\noindent\textbf{Inference Efficiency Metrics}: we report the inference peak memory (Inf. Mem.) and the relative speedup (Inf. Speed) based on throughput (data processed per second) for inference efficiency. 

Both training and evaluation are conducted on a single A100 GPU. The inference test batch size is 128 for small models while 32 and 4 for LLaMA 7B and 13B models, respectively. We demonstrate detailed training and evaluation setups/implementations in \cref{appendix:hyper-param}.

\subsection{Main Results} \label{sec:main-result}
% \bowen{training AND inference efficiency -> detail, memory or time, how much. what experiments are conducted, detailed results and provide contexts}
% \qq{you want to highlight that our method achieves both the training and inference efficiency goal, show the key results over baselines. LoRA+Prune is an evidential example. First describe the results. Then describe the results detail.}

% \hanna{refer to table 2 here; table 2 shows the main results... Here, you can mention, APT generally doesn't hurt accuracy in most settings, while improving training ... [a precise term here] and refer to the details below.  }

% \hanna{I would categorize the results better. what are the main points you want to highlight. 1. compare APT with FT and LORA. APT improves training speed compared to FT and decreases memory by ... with ... cost at accuracy in both small and large lms. 2. compare APT with other peft and prune methods. 3. comapre apt with the distill models. }
% \todo{speedup compared to which baseline, be specific}
\noindent \textbf{Overview} We demonstrate the end-task performance of {\ourmethod} comparing to fine-tuning (FT), LoRA-tuning (LoRA), and pruning baselines in \cref{tab:small-model-results} and \cref{tab:llama-7b-results}. Overall, up to 99\% of fine-tuned {\lmabbr}'s task accuracy is maintained when pruning RoBERTa and T5 models leaving 40\% parameters, with only about 70\% training memory consumption than fine-tuning. When pruning LLaMA2-7B models with 70\% parameters remaining, {\ourmethod} recovers 86.4\% task performance on average, together with only 75.8\% training memory usage than LoRA-tuning. Furthermore, {\ourmethod} also significantly reduces end-task performance and training costs compared to the pruning and distillation baselines. The detailed comparisons are shown as follows.

\noindent \textbf{{\ourmethod} speeds up RoBERTa and T5 training 8$\times$ and reduces training memory costs to 30\% in LLaMA pruning compared to LoRA+Prune baseline.} 
% \todo{Highlight the advantages of {\ourmethod} -> efficiency. performance better in what conditions? Extracting main messages.}
% \qq{put this section to be the first main results}
% \qq{this is not accurate, describe clearly what efficiency for training or inference, e.g., our method speeds up small model training, xxx}
% with only 15.9\% and 19.2\% extra memory costs
% \todo{add detailed values, e.g., 2-8 times and xx\% memory usage}
Shown in \cref{tab:small-model-results}, when pruning RoBERTa models to 60\% sparsity, {\ourmethod} converges $8.4\times$ faster than the LoRA+Prune baseline with consuming similar GPU memory. {\ourmethod} also prunes T5 models 8.2$\times$ faster than the LoRA+Prune baseline. The reason is that {\ourmethod} adaptively prunes task-irrelevant parameters during training, reducing memory and per-step training time. Adding parameters in salient tuning layers also accelerates {\lmabbr} convergence. 
Also, {\ourmethod} costs less than 24GB of memory when pruning 30\% parameters in LLaMA2-7B models before tuning, which can be easily adapted to the consumer-level GPUs. In contrast, LLM-Pruner costs about 80GB memory when pruning the LLaMA 7B model\footnote{\url{https://github.com/horseee/LLM-Pruner/issues/4}}.

\noindent \textbf{{\ourmethod} achieves 2.5\%-9.9\% higher task performance than the LoRA+Prune baseline with the same pruning sparsities.} 
% \todo{performance superior to which baseline (LoRA+Prune)}
% \todo{how much performance improved, improves xx\% higher}
Presented in \cref{tab:small-model-results} and \cref{tab:llama-7b-results}
, when RoBERTa, T5, and LLaMA models, regardless of size, {\ourmethod} consistently reach higher task performance than the LoRA+Prune. 
% In detail, when pruning 60\% parameters, {\ourmethod} effectively recovers 99.4\% LoRA-tuned {\lmabbr} performance on average, while the LoRA+Prune baseline can only maintain 95.6\% performance. 
With similar inference speedup and memory when pruning RoBERTa models, {\ourmethod} reaches 2.5\% more end-task performance on average.
% Moreover, {\ourmethod} performs the same with the LoRA baseline on T5 classification tasks (SST2 and MNLI) without any performance loss. 
When pruning T5 models under the 60\% sparsity, the task performance achieved by {\ourmethod} is 5.1\% better than the LoRA+Prune baseline. However, the inference efficiency reached by {\ourmethod} (1.3$\times$ speedup and 81.5\% memory cost) is worse than the LoRA+Prune baseline (2.1$\times$ speedup and 73.4\% memory cost). This is because {\ourmethod} can adaptively prune more decoder parameters, which are also computationally cheaper than encoder parameters (due to shorter output sequence length) but relatively useless for classification tasks. 
% In the meantime, {\ourmethod} maintains 98.9\% LoRA tuning performance in generation tasks like CNN/DM, showing the capabilities of being used under generative settings.
% Furthermore, it shall be noticed that static LoRA tuning on the T5-base model for the CNN/DM task causes performance degradation. The reason is that for generative tasks, the ranks for LoRA tuning shall be set larger empirically. Therefore, with our {\ourarchabbr} architecture that expands its parameters during training, it is viable to tune generative models with lower peak memory consumption.
For LLaMA2-7B model pruning with 70\% sparsity, {\ourmethod} outperforms LLMPruner with 16.5\% and the LoRA+Prune baseline with 9.9\%, where the inference efficiency improvements of {\ourmethod} is slightly better than both LoRA+Prune and LLMPruner baselines. 
% For TruthfulQA, which shows the most gain through fine-tuning (39.0 to 49.9), both LoRA+Prune (46.2) and LLMPruner (40.6) can recover models' ability. However, these baselines fail to perform as well on other tasks such as MMLU (23.9 for MT-retrained and 24.9 for LLMPruner, worse than random guessing). This performance trajectory shows that the prune-then-retrain paradigm could harm large {\lmabbr}'s generalization capabilities when sub-optimal parameter pruning results are reached.

% \hanna{I don't think the following statement is accurate. You first need to explain Figure 3 and then explain the finding. I don't fully understand this result. Don't make innacurate stamenets boldface. }
% \todo{move this to forward and merge}
% \noindent \textbf{{\ourmethod} prunes {\lmabbr}s with 6\% to 60\% more inference speedup and reduces inference 7\%-25\% more memory with similar task performance.} 
% \todo{how much inference efficiency gained}

% \todo{Add comparison to Prune+Distill baseline, better training efficiency}
\noindent \textbf{{\ourmethod} reaches on-par performance with the Prune+Distill baseline given the same pruning sparsity but trains 2.5$\times$ faster and costs only 41.6\% memory.}
Compared to the Prune+Distill baseline, {\ourmethod} results in comparable task accuracy (0.9 point drop in MNLI and same in SST2). At the same time, with similar inference efficiency achieved, {\ourmethod} costs only 41.6\% training memory and converges 2.5$\times$ than the Prune+Distill baseline. This is because of the self-distillation technique in {\ourmethod} where no separated teacher model is required in pruning {\lmabbr}s. Moreover, {\ourmethod} achieves better task performance than the LoRA+Prune+Distill baseline as well, with less training time and memory consumption. These results demonstrate that {\ourmethod} successfully tackles the problem where simply combining PEFT and pruning hurts pruned {\lmabbr}'s task accuracy and training efficiency.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/llama2_7b_tradeoff.pdf}
%     \caption{The average performance of LLaMA 2 7B model pruning against relative training and inference memory usage along with inference speedup. The numbers beside each point denote the pruning sparsity.}
%     \label{fig:llama-tradeoff}
%     \vspace{-10pt}
% \end{figure}

% \qq{since here you only discuss inference efficiency, I assume the previous section is about training efficiency, you might want to adjust the title}
% Regarding the inference efficiency, {\ourmethod} reaches similar time and memory costs compared to the LoRA+Prune baseline when pruning RoBERTa models. However, when pruning T5 models, since {\ourmethod} prunes more parameters in the decoder that causes less computation when performing the SST2 classification task, {\ourmethod} results in relatively worse inference efficiency when the similar amount of parameters are pruned. On the other hand, {\ourmethod} achieves superior inference efficiency in throughput and memory when pruning LLaMA models than existing baselines. Overall, {\ourmethod} improves the model inference efficiency similar to baselines for encoder- and decoder-only {\lmabbr}s, but for encoder-decoder {\lmabbr}s, the inference efficiency gain is dependent on the property of fine-tuning tasks (classification or generation).

% \input{tables/t5_3b_results}
% \begin{figure}[t!]
%     \begin{minipage}{0.48\textwidth}
%         \begin{figure}[H]
%             \centering
%             \includegraphics[width=\textwidth]{figures/plot_training_tradeoff_scatter.pdf}
%             \caption{Caption}
%             \label{fig:enter-label}
%         \end{figure}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.48\textwidth}
%         \begin{figure}[H]
%             \centering
%             \includegraphics[width=\textwidth]{figures/plot_inference_tradeoff_scatter.pdf}
%             \caption{Caption}
%             \label{fig:enter-label}
%         \end{figure}
%     \end{minipage}
% \end{figure}

% \input{tables/roberta_sst2_training_efficiency}

% \noindent \textbf{Naively combining pruning and LoRA causes training time and memory overhead.}
% When pruning small {\lmabbr}s, shown in \cref{fig:perf-efficiency-tradeoff}, full fine-tuning converges the fastest but consumes high memory. At the same time, LoRA tuning reduces the memory consumption yet takes significantly longer time (21.4$\times$ for the 97\% accuracy TTA while 8.39$\times$ for the 99\% accuracy TTA on $\text{RoBERTa}_{\text{base}}$). Moreover, the training peak memory would be larger than fine-tuning when using CoFi pruning, while combining it with LoRA still causes extra memory. This is because fine-pruning methods require the gradient information of the pruning masks for model sparsity control, thus harming the advantage of memory reduction using LoRA. In the meantime, combining pruning (CoFi and MT) with LoRA results in substantially longer converge time for all models because of the requirement of training a teacher model before pruning.

% \qq{this sentence is the reason why we can larger models also efficient. So this set of results can be something like \ourmethod makes training smaller models faster and larger models more memory efficient, while provide inference speedups for both etc.}
% \noindent \textbf{{\ourmethod} achieves better training and inference efficiency with on-par performance.}
% \qq{decent is a vague term, this result can be merged with the first one, compare to LoRA+Prune baseline, you can say something like our method achieves better efficiency and comparable performance.} \bowen{Yes I agree. My original point was to say our method, even though pruning the model, can still converge faster to a certain accuracy compared to LoRA without pruning. But I guess this is not very solid since we still have slight performance loss compared to either FT or LoRA. I'll revise it and only consider the comparisions to pruning baselines.}
% \todo{Add comparison figure to replace the original radar map.}


% \noindent \textbf{{\ourmethod} achieves substantial performance gain without diminishing inference efficiency.}
% As indicated in \cref{fig:perf-efficiency-tradeoff} {\ourmethod} achieves on-par inference speedup and memory usage compared to all pruning baselines after merging LoRA layers under the same sparsity targets. The inference efficiency difference between {\ourmethod} to existing pruning baselines on $\text{RoBERTa}_{\text{base}}$ is minimal but more noticeable on $\text{T5}_{\text{base}}$ model. This is because MT prunes more T5 encoder parameters, which has more computation since the input sequence length (128) is significantly longer than the output (2) for classification tasks. For LLaMA model pruning, {\ourmethod} achieves better inference efficiency than all baselines.

% \begin{figure*}
%     \centering
%     \includegraphics[width=\textwidth]{figures/flop_corr.pdf}
%     \caption{Accuracy v.s. efficiency promotion, including model size, training speedup, training memory reduction, inference speedup, and inference memory reduction, respectively.}
%     \label{fig:main-result}
% \end{figure*}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/pruning_tradeoff.pdf}
    \caption{Task performance v.s. relative inference efficiency on RoBERTa, T5, and LLaMA-2 7B models with {\ourmethod} and baselines.}
    \label{fig:prune-tradeoff}
    \vspace{-10pt}
\end{figure}

\subsection{Pruning Sparsity Analysis} \label{sec:sparsity-analysis}

We further show the task performance changing trajectory with different pruning sparsities in \cref{fig:prune-tradeoff}. {\ourmethod} achieves superior inference speedup with less inference memory consumption than baselines targeting the same task performance. Compared to the LoRA+Prune baseline, when pruning RoBERTa models targeting similar task accuracy, {\ourmethod} is 21.8\% faster in inference and is 7\% more memory-efficient. For T5 model pruning with 97\% of dense model performance, {\ourmethod} results in 62.7\% more inference speedup with 24.8\% more inference memory reduction compared to the LoRA+Prune baseline.
When pruning large LLaMA2-7B models, {\ourmethod} speedup is 6.7\% more and reduces 9.2\% more inference memory than the LoRA+Prune baseline, maintaining over 85\% task performance of the dense model.

\subsection{Ablation Study} \label{sec:ablation}
We evaluate the impact of different components in {\ourmethod} by removing the adaptive pruning ($\mathcal{A}_{\text{P}}$), adaptive tuning ($\mathcal{A}_{\text{T}}$), and self-distillation ($\mathcal{D}_{\text{S}}$). Besides end-task performance, we also report the training efficiency metrics for each ablation. 
% We do not report the inference efficiency since they are similar under the same sparsity. 

% \hanna{don't say small LMs. mention exactly which lm? bert? roberta? explain the setting first. We evaluate the impact of different components in APT, by reoving the ... and ... }
\noindent \textbf{Adaptive pruning ($\mathcal{A}_{\text{P}}$)} We demonstrate the ablation of adaptive pruning (w/o $\mathcal{A}_{\text{P}}$) for RoBERTa models in \cref{tab:ablate-paramalloc} and LLaMA models in \cref{tab:llama-ablation}. In these cases, we only train {\lmabbr}s with adaptive tuning strategies with supervised fine-tuning objectives without distillation. In such settings, {\ourmethod} w/o $\mathcal{A}_{\text{P}}$ can be recognized as a PEFT method with tuning parameters' sizes adaptively changing during fine-tuning. Hence, the inference efficiency of the trained {\lmabbr}s are the same as full fine-tuning and LoRA. Without pruning, the task performance of RoBERTa reaches 94.4 for SST2 and 87.5 for MNLI (99.8\% fine-tuned {\lmabbr} performance on average). The average performance of the LLaMA model also achieves 96.6\% to its LoRA-tuned counterpart. In addition, we surprisingly find that the RoBERTA training speed with {\ourmethod} w/o $\mathcal{A}_{\text{P}}$ is even 21\% faster than full fine-tuning while costing only 62.2\% memory.
In the meantime, the training memory cost of {\ourmethod} w/o $\mathcal{A}_{\text{P}}$ in LLaMA tuning is higher than LoRA. The reason is that the tuning parameter number of {\ourmethod} will grow larger than static LoRA-tuning. This ablation demonstrates that adaptive pruning is essential in reducing the training memory consumption of LLaMA model fine-tuning, besides benefiting model inference efficiency.

\noindent \textbf{Adaptive tuning ($\mathcal{A}_{\text{T}}$)} In \cref{tab:ablate-paramalloc}, we show results of ablating adaptive tuning (w/o $\mathcal{A}_{\text{T}}$) where the tuning parameters are static when pruning RoBERTa models.
% \hanna{what are Ft teacher and lora teacher? shouldn't you ablate just pruning or just peft? are you also sweeping the r-apt? there are different compoenents in the apt architecture. explain the setting according to what you have in the apt architecture and explain results. }
Without $\mathcal{A}_{\text{T}}$, the model's performance decreases to 93.2/84.4, leading to a similar performance as the LoRA+Prune baseline (93.0/84.0). Moreover, equally increasing parameters across all layers instead of adding parameters based on salience notably hurts the task accuracy (84.4 on MNLI compared to 86.4). At the same time, $\mathcal{A}_{\text{T}}$ helps the model converge 16\% faster than static LoRA training.
% \hanna{In the tables 2 and 3, use up and down  arrows to show when higher numbers or lower numbers are better. }
For ablation results in LLaMA models shown in \cref{tab:llama-ablation}, we observe that $\mathcal{A}_{\text{T}}$ recovers the model performance under 50\% pruning setting (38.2 compared to 35.8). However, the difference under 70\% pruning is insignificant. Meanwhile, if calculating the pruning parameter salience without using kurtosis to consider outliers parameters, the pruned {\lmabbr}'s performance substantially drops from 50.0 to 38.1. We conclude that $\mathcal{A}_{\text{T}}$ substantially improves {\lmabbr} training speed and end-task performance. For large LLaMA-based {\lmabbr} pruning, and outlier parameters are essential to recovering the pruned large LLaMA-based models' capabilities.

\input{tables/ablate_elastic_time}

\input{tables/llama_7b_ablation}

% \input{tables/ablate_distillation}
% \noindent \textbf{Adaptively selecting teacher and student helps the model's performance recover.}
% As can be observed from \cref{tab:ablate-distillation}, our adaptive teacher-student selection and mapping method perform better compared to every ablation with at least 0.6\% accuracy, demonstrating that the adaptivity for both teacher and student contributes to {\ourmethod}'s performance recovery. Furthermore, compared to the retraining results shown in \cref{tab:main-result}, when not using distillation at all, simply increasing parameter numbers won't be beneficial for performance recovery, demonstrating the importance of adaptive distillation in {\ourmethod}.

% \noindent \textbf{Knowledge distillation used with static LoRA hurts model performance.} 
% \qq{you can put this into ablation analysis}


\noindent \textbf{Self-distillation ($\mathcal{D}_{\text{S}}$)}
Shown in \cref{tab:ablate-paramalloc}, tuning {\ourmethod} adapters dynamically without distillation objectives gets 1.35 worse task accuracy on average. However, pruning RoBERTa models without self-distillation is 22.5\% faster and costs 11.7\% less training memory. This result indicates the effectiveness of leveraging knowledge distillation to recover pruned {\lmabbr} performance, but conducting distillation will result in extra training costs regarding both time and memory. Detailed comparisons of self-distillation and traditional, static distillation strategies are shown in \cref{appendix:distillation}.

Besides the ablation study results demonstrated above, we show the detailed analysis of adaptive pruning and tuning's effect on {\lmabbr}s' end-task performance, training, and inference efficiency in \cref{sec:analysis}.
% The results in \cref{tab:main-result} also depict that the LoRA + $\text{MT}_{\text{train}}$ goes worse than LoRA + $\text{MT}_{\text{distill}}$ without distillation. This further proves our addressed hypothesis that low-rank adaptation cannot give the model enough learning capacity and make it able to imitate the teacher model's behavior. However, after expanding salient parameter sizes with {\ourarchabbr}, the student model would

% Furthermore, self-distillation helps {\ourmethod} converge faster than static LoRA training. 
% {\ourmethod} converges 2.13$\times$ faster than using fine-tuned teacher with only 62.7\% memory usage. Compared to using a fully-tuned teacher with LoRA, {\ourmethod} converges 9.91$\times$ faster using 72.9\% memory. 
% We further show the results of pruning LLaMA models with distillation when the model's initial sparsity is set as 15\% in \cref{tab:llama-ablation}, where most tasks' performance decreases despite TruthfulQA, highlighting that knowledge distillation hurts large {\lmabbr}'s generalizabilities.

\section{Limitation and Discussion} \label{sec:discussion}
\noindent \textbf{Towards better performance gain and inference speedup of large {\lmabbr} in limited resource settings.}
By comparing \cref{tab:small-model-results} to \cref{tab:llama-7b-results}, we notice the performance gap in pruned LLaMA models is larger than smaller {\lmabbr}s because we use distillation-free settings in large {\lmabbr} pruning to reduce training memory consumption. One can improve performance-efficiency trade-offs with better memory-efficient distillation, parameter sharing, and re-allocation strategies. Furthermore, because of the hardware features of Ampere-architecture GPUs, layer dimensions divisible by 8 for FP16 and divisible by 16 for Int8 would reach more realistic speedups. One possible direction is to explore a higher level of structured pruning, for example, grouped neurons and dimensions, in LLMs.

\noindent \textbf{Training could be unstable because of parameter shape changes.} Since we adjust tuning parameters dynamically during training, newly initialized parameters are added to the model while existing parameters are pruned. We reset the optimizer every time after each parameter size changes to avoid stability issues, but this strategy might cause unstable training. Meanwhile, the time of selecting the teacher checkpoints during training highly affects the pruned model's performance, whereas non-converged or sparse teachers do not help in performance recovery. The pruned {\lmabbr}s' end-task accuracy could benefit from better and more stable strategies in adaptive pruning and tuning.

\noindent \textbf{Could non-linear adapters perform better for performance recovery?} To avoid inference time and memory overhead, we specifically adapt {\ourarchabbr} to LoRA since the added tuning parameters can be merged after LMs' training. However, low-rank decomposition does not add more complexity to a {\lmabbr}, whereas the model's overall representation capacity doesn't increase. The adaptation with a wider range of adapters, such as Prefix-tuning~\citep{li2021prefix}, HAdapters~\citep{houlsby2019parameter}, and Parallel-adapters~\citep{he_towards_2022}, could be better explored.