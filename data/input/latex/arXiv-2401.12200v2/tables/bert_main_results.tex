% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table*}[t!]
\centering
\begin{tabular}{@{}ll|rrr|rrrr@{}}
\toprule
Tune & Prune & SST-2 & MNLI & SQuAD & Train $\nearrow$ & Train Mem. & Inf. $\nearrow$ & Inf. Mem \\ \midrule
FT & None & 93.5 & 84.6 & 88.5 & 1.00$\times$ & 100\% & 1.00$\times$ & 100\% \\
FT & CoFi & 93.0 & 85.3 & 89.1 & 0.67$\times$ & 162\% &  &  \\
FT & MT &  & 68.1 &  & 1.00$\times$ & 100\% &  &  \\
FT & MT* &  &  &  &  &  &  &  \\ \midrule
LoRA & None & 92.9 & 84.2 & 83.5 & 1.16$\times$ &  & 1.00$\times$  & 100\% \\
LoRA & CoFi &  &  &  & &  &   &  \\
LoRA & MT &  & 31.8 & & 1.16$\times$ & 80.5\% &  &  \\ 
LoRA & MT* &  & & & &  &  &  \\ \midrule
\begin{tabular}[c]{@{}l@{}}\ourmethod\\ \tableindent w/o Distillation\end{tabular} &  & 91.5 & 80 & 75.3 & 1.31$\times$ & 77.94\% & 2.16$\times$ & 51.3\% \\
\ourmethod &  & 92.4 &  & 87.0 &  &  &  &  \\ \bottomrule
\end{tabular}
\caption{The main results of \ourmethod compared with baselines on $\text{BERT}_{\text{BASE}}$ backbone given 40\% tuned model FLOP constraint with the tuning parameter number less than 2.5M. Train $\nearrow$ denotes the average training speed per epoch compared to fine-tuning. Train Mem. denotes the peak memory consumption throughout training. Inf. $\nearrow$ and Inf. Mem denotes the inference time speedup and memory usage compared with the vanilla fine-tuned model. FT denotes fully fine-tuning. MT denotes Mask Tuning, while MT* denotes retraining after Mask Tuning.}
\label{tab:main-result}
\end{table*}