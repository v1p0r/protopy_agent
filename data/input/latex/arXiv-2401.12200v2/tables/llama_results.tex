% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table}[htbp]
\centering
\begin{tabular}{@{}l|lrrrr@{}}
\toprule
Method                            & ARC           & HellaSwag     & MMLU          & TruthfulQA    & Avg.          \\ \midrule
LLaMA2 7B                              & 53.1          & 77.7          & 43.8          & 39.0          & 53.4          \\ \midrule
LoRA                              & 55.6          & 79.3          & 46.9          & 49.9          & 57.9          \\ \midrule
% LoRA + $\text{MT}$                & 42.1          & 49.1          & 25.6          & 43.5          & 40.1          \\
LoRA+Prune & \textbf{46.8} & 65.2          & 23.9          & 46.2          & 45.5          \\
LLMPruner                         & 39.2          & 67.0          & 24.9          & 40.6          & 42.9          \\
APT                               & 45.4          & \textbf{71.1} & \textbf{36.9} & \textbf{46.6} & \textbf{50.0} \\ \midrule
LLaMA2 13B                       & 59.4          & 82.1          & 55.8          & 37.4          & 58.7          \\ \midrule
LoRA                              & 60.8          & 82.8          & 56.0          & 46.5          & 61.5          \\ \midrule
% LoRA + $\text{MT}$                & 51.2          & 74.0          & 44.7          & 39.9          & 52.4          \\
LoRA+Prune & \textbf{56.4} & \textbf{79.1} & 50.7          & 42.1          & \textbf{57.1} \\
LLMPruner                         & 46.8          & 74.0          & 24.7          & 34.8          & 45.1          \\
APT                               & 49.5          & 75.8          & \textbf{52.5} & \textbf{44.7} & 55.6          \\ \bottomrule
\end{tabular}
\caption{LLaMA2 7B and 13B 30\% sparsity pruning results with GPT4-generated Alpaca dataset, evaluated on the Open LLM leaderboard few-shot tasks.}
\label{tab:llama-results}
\vspace{-10pt}
\end{table}