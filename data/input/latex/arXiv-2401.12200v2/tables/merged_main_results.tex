% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}
\begin{table*}[t!]
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{@{}ll|rrrr|rrrr@{}}
\toprule
Model                                           & Method         & MNLI          & SST2          & SQuAD v2      & CNN/DM                  & Train Speed($\Uparrow$) & Train Mem($\Downarrow$) & Inf Speed($\Uparrow$) & Inf Mem($\Downarrow$) \\ \midrule
\multirow{5}{*}{$\text{RoBERTa}_{\text{base}}$} & FT             & 87.6          & 94.8          & 82.9          & -                       & 100.0\%      & 100.0\%     & 100.0\%    & 100.0\%   \\
                                                & LoRA           & 87.5          & 95.1          & 83.0          & -                       & 4.7\%        & 60.5\%      & 100.0\%    & 100.0\%   \\ \cmidrule(l){2-10} 
                                                & LoRA+Prune   & 84.0          & 93.0          & 79.2          & -                       & 2.0\%        & 60.5\%      & 262.9\%    & 75.1\%    \\
                                                & Prune+Distill   & \textbf{87.3}          & \textbf{94.5}          &   -        & -                       & 6.7\%        & 168.5\%      & 259.2\%    & 79.2\%    \\
                                                & LoRA+Prune+Distill & 84.2          & 91.9          & -             & -                       & 1.5\%        & 141.4\%     & 253.7\%    & 82.3\%    \\ \cmidrule(l){2-10} 
                                                & \ourmethod     & 86.4 & \textbf{94.5} & \textbf{81.8} & -                       & 16.9\%       & 70.1\%      & 241.9\%    & 78.1\%    \\ \midrule
\multirow{4}{*}{$\text{T5}_{\text{base}}$}      & FT             & 87.1          & 95.2          & -             & 42.1/20.3/39.4          & 100.0\%      & 100.0\%     & 100.0\%    & 100.0\%   \\
                                                & LoRA           & 87.0          & 95.0          & -             & 38.7/17.2/36.0          & 39.1\%       & 62.0\%      & 100.0\%    & 100.0\%   \\
                                                & LoRA+Prune   & 80.9          & 92.3          & -             & 36.7/15.7/33.9          & 2.5\%        & 62.0\%      & 212.5\%    & 73.4\%    \\ \cmidrule(l){2-10} 
                                                & \ourmethod     & \textbf{87.0} & \textbf{95.0} & -             & \textbf{38.6/17.0/35.8} & 20.6\%       & 73.9\%      & 134.1\%    & 81.5\%    \\ \bottomrule
\end{tabular}
}
\caption{RoBERTa and T5 pruning with {\ourmethod} compared to baselines under 60\% sparsity. We measure the training and inference efficiency with {\lmabbr}s pruned on the SST2 task. Training speed is measured via 97\% accuracy TTA. $\Uparrow$ denotes the metric is better when bigger, and $\Downarrow$ denotes smaller is better.}
\label{tab:small-model-results}
\vspace{-5pt}
\end{table*}