\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alrashedy et~al.(2025)Alrashedy, Tambwekar, Zaidi, Langwasser, Xu, and Gombolay]{alrashedy2025generating}
Kamel Alrashedy, Pradyumna Tambwekar, Zulfiqar~Haider Zaidi, Megan Langwasser, Wei Xu, and Matthew Gombolay.
\newblock Generating {CAD} code with vision-language models for 3d designs.
\newblock In \emph{ICLR}, 2025.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Beitz et~al.(1996)Beitz, Pahl, and Grote]{beitz1996engineering}
W~Beitz, G~Pahl, and K~Grote.
\newblock Engineering design: a systematic approach.
\newblock \emph{Mrs Bulletin}, 71:\penalty0 30, 1996.

\bibitem[Besta et~al.(2024)Besta, Blach, Kubicek, Gerstenberger, Podstawski, Gianinazzi, Gajda, Lehmann, Niewiadomski, Nyczyk, et~al.]{besta2024graph}
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et~al.
\newblock Graph of thoughts: Solving elaborate problems with large language models.
\newblock In \emph{AAAI}, 2024.

\bibitem[Chen et~al.(2024)Chen, Su, Zuo, Yang, Yuan, Chan, Yu, Lu, Hung, Qian, Qin, Cong, Xie, Liu, Sun, and Zhou]{chen2024agentverse}
Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou.
\newblock Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors.
\newblock In \emph{ICLR}, 2024.

\bibitem[Chen et~al.(2025{\natexlab{a}})Chen, Zhang, Huang, Qiu, Zhang, Wen, and Liu]{chen2025symbolic}
Yamei Chen, Haoquan Zhang, Yangyi Huang, Zeju Qiu, Kaipeng Zhang, Yandong Wen, and Weiyang Liu.
\newblock Symbolic graphics programming with large language models.
\newblock \emph{arXiv preprint arXiv:2509.05208}, 2025{\natexlab{a}}.

\bibitem[Chen et~al.(2025{\natexlab{b}})Chen, Lan, Zhou, Wang, and Pan]{chen2025sar3d}
Yongwei Chen, Yushi Lan, Shangchen Zhou, Tengfei Wang, and Xingang Pan.
\newblock Sar3d: Autoregressive 3d object generation and understanding via multi-scale 3d vqvae.
\newblock In \emph{CVPR}, 2025{\natexlab{b}}.

\bibitem[Chen et~al.(2025{\natexlab{c}})Chen, Qin, Wu, Ling, Ye, Zhao, and Shi]{chen2025pass}
Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne~Xin Zhao, and Guang Shi.
\newblock Pass@ k training for adaptively balancing exploration and exploitation of large reasoning models.
\newblock \emph{arXiv preprint arXiv:2508.10751}, 2025{\natexlab{c}}.

\bibitem[Cheng et~al.(2025)Cheng, Huang, Zhu, Dai, Zhao, Zhang, and Wei]{cheng2025reasoning}
Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo~Dai, Wayne~Xin Zhao, Zhenliang Zhang, and Furu Wei.
\newblock Reasoning with exploration: An entropy perspective.
\newblock \emph{arXiv preprint arXiv:2506.14758}, 2025.

\bibitem[Coulom(2006)]{coulom2006efficient}
R{\'e}mi Coulom.
\newblock Efficient selectivity and backup operators in monte-carlo tree search.
\newblock In \emph{International conference on computers and games}, pp.\  72--83. Springer, 2006.

\bibitem[Cui et~al.(2025)Cui, Zhang, Chen, Yuan, Wang, Zuo, Li, Fan, Chen, Chen, et~al.]{cui2025entropy}
Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et~al.
\newblock The entropy mechanism of reinforcement learning for reasoning language models.
\newblock \emph{arXiv preprint arXiv:2505.22617}, 2025.

\bibitem[Deng et~al.(2022)Deng, Kulal, Dong, Deng, Tian, and Wu]{deng2022unsupervised}
Boyang Deng, Sumith Kulal, Zhengyang Dong, Congyue Deng, Yonglong Tian, and Jiajun Wu.
\newblock Unsupervised learning of shape programs with repeatable implicit parts.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Deng et~al.(2024)Deng, Zhang, Zhang, Yuan, Ng, and Chua]{deng2024multi}
Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, and Tat-Seng Chua.
\newblock On the multi-turn instruction following for conversational web agents.
\newblock \emph{arXiv preprint arXiv:2402.15057}, 2024.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Shleifer, and Zettlemoyer]{dettmers20228bit}
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
\newblock 8-bit optimizers via block-wise quantization.
\newblock In \emph{ICLR}, 2022.

\bibitem[Fan et~al.(2022)Fan, Wang, Jiang, Mandlekar, Yang, Zhu, Tang, Huang, Zhu, and Anandkumar]{fan2022minedojo}
Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar.
\newblock Minedojo: Building open-ended embodied agents with internet-scale knowledge.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and Neubig]{gao2023pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Pal: Program-aided language models.
\newblock In \emph{ICML}, 2023.

\bibitem[Guo et~al.(2025)Guo, Yang, Zhang, Song, Wang, Zhu, Xu, Zhang, Ma, Bi, et~al.]{guo2025deepseek}
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et~al.
\newblock Deepseek-r1 incentivizes reasoning in llms through reinforcement learning.
\newblock \emph{Nature}, 645\penalty0 (8081):\penalty0 633--638, 2025.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{ICLR}, 2022.

\bibitem[Hu et~al.(2024{\natexlab{a}})Hu, Jain, Elmoznino, Kaddar, Lajoie, Bengio, and Malkin]{hu2023amortizing}
Edward~J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin.
\newblock Amortizing intractable inference in large language models.
\newblock In \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Hu et~al.(2024{\natexlab{b}})Hu, Huang, Hu, and Liu]{hu20243d}
Shiying Hu, Zengrong Huang, Chengpeng Hu, and Jialin Liu.
\newblock 3d building generation in minecraft via large language models.
\newblock In \emph{2024 IEEE Conference on Games (CoG)}, pp.\  1--4. IEEE, 2024{\natexlab{b}}.

\bibitem[Hu et~al.(2024{\natexlab{c}})Hu, Iscen, Jain, Kipf, Yue, Ross, Schmid, and Fathi]{hu2024scenecraft}
Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David~A Ross, Cordelia Schmid, and Alireza Fathi.
\newblock Scenecraft: An llm agent for synthesizing 3d scenes as blender code.
\newblock In \emph{ICML}, 2024{\natexlab{c}}.

\bibitem[Huang et~al.(2024)Huang, Yang, and Guibas]{huang2024blenderalchemy}
Ian Huang, Guandao Yang, and Leonidas Guibas.
\newblock Blenderalchemy: Editing 3d graphics with vision-language models.
\newblock In \emph{ECCV}, 2024.

\bibitem[Jones et~al.(2025)Jones, Guerrero, Mitra, and Ritchie]{jones2025shapelib}
R~Kenny Jones, Paul Guerrero, Niloy~J Mitra, and Daniel Ritchie.
\newblock Shapelib: Designing a library of programmatic 3d shape abstractions with large language models.
\newblock \emph{arXiv preprint arXiv:2502.08884}, 2025.

\bibitem[Kim et~al.(2024)Kim, Stennett, Shah, Sinha, and Orso]{kim2024leveraging}
Myeongsoo Kim, Tyler Stennett, Dhruv Shah, Saurabh Sinha, and Alessandro Orso.
\newblock Leveraging large language models to improve rest api testing.
\newblock In \emph{Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results}, pp.\  37--41, 2024.

\bibitem[Koh et~al.(2024)Koh, McAleer, Fried, and Salakhutdinov]{koh2024tree}
Jing~Yu Koh, Stephen~Marcus McAleer, Daniel Fried, and Russ Salakhutdinov.
\newblock Tree search for language model agents.
\newblock In \emph{ICLR}, 2024.

\bibitem[Kulits et~al.(2025)Kulits, Feng, Liu, Abrevaya, and Black]{kulits2025re}
Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria~Fernandez Abrevaya, and Michael~J Black.
\newblock Re-thinking inverse graphics with large language models.
\newblock \emph{Transactions on Machine Learning Research (TMLR)}, 2025.

\bibitem[Lambert et~al.(2025)Lambert, Morrison, Pyatkin, Huang, Ivison, Brahman, Miranda, Liu, Dziri, Lyu, Gu, Malik, Graf, Hwang, Yang, Bras, Tafjord, Wilhelm, Soldaini, Smith, Wang, Dasigi, and Hajishirzi]{lambert2024tulu}
Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James~Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena~D. Hwang, Jiangjiang Yang, Ronan~Le Bras, Oyvind Tafjord, Christopher Wilhelm, Luca Soldaini, Noah~A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi.
\newblock Tulu 3: Pushing frontiers in open language model post-training.
\newblock In \emph{COLM}, 2025.

\bibitem[Li et~al.(2023)Li, Hammoud, Itani, Khizbullin, and Ghanem]{li2023camel}
Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.
\newblock Camel: Communicative agents for" mind" exploration of large language model society.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Li et~al.(2025)Li, Ma, Li, Lou, Zhou, and Zhou]{li2025cad}
Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, and Xiangdong Zhou.
\newblock Cad-llama: leveraging large language models for computer-aided design parametric 3d model generation.
\newblock In \emph{CVPR}, 2025.

\bibitem[Liu et~al.(2025{\natexlab{a}})Liu, Li, Zhang, Cui, Fang, Zheng, Zheng, and Song]{liu2024odyssey}
Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya Zheng, and Mingli Song.
\newblock Odyssey: Empowering minecraft agents with open-world skills.
\newblock In \emph{IJCAI}, 2025{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Qiu, Feng, Xiu, Xue, Yu, Feng, Liu, Heo, Peng, et~al.]{liu2024boft}
Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, et~al.
\newblock Parameter-efficient orthogonal finetuning via butterfly factorization.
\newblock In \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Peng, Yi, Xie, Xiang, Liu, and Xu]{liu2024toolnet}
Xukun Liu, Zhiyuan Peng, Xiaoyuan Yi, Xing Xie, Lirong Xiang, Yuchen Liu, and Dongkuan Xu.
\newblock Toolnet: Connecting large language models with massive tools via tool graph.
\newblock \emph{arXiv preprint arXiv:2403.00839}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2025{\natexlab{b}})Liu, Xiao, Liu, Bengio, and Zhang]{liu2025nablagfn}
Zhen Liu, Tim~Z Xiao, Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang.
\newblock Efficient diversity-preserving diffusion alignment via gradient-informed gflownets.
\newblock In \emph{ICLR}, 2025{\natexlab{b}}.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2023self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Makatura et~al.(2023)Makatura, Foshey, Wang, H{\"a}hnLein, Ma, Deng, Tjandrasuwita, Spielberg, Owens, Chen, et~al.]{makatura2023can}
Liane Makatura, Michael Foshey, Bohan Wang, Felix H{\"a}hnLein, Pingchuan Ma, Bolei Deng, Megan Tjandrasuwita, Andrew Spielberg, Crystal~Elaine Owens, Peter~Yichen Chen, et~al.
\newblock How can large language models help humans in design and manufacturing?
\newblock \emph{arXiv preprint arXiv:2307.14377}, 2023.

\bibitem[Minaee et~al.(2024)Minaee, Mikolov, Nikzad, Chenaghlu, Socher, Amatriain, and Gao]{minaee2024large}
Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao.
\newblock Large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2402.06196}, 2024.

\bibitem[Novikov et~al.(2025)Novikov, V{\~u}, Eisenberger, Dupont, Huang, Wagner, Shirobokov, Kozlovskii, Ruiz, Mehrabian, et~al.]{novikov2025alphaevolve}
Alexander Novikov, Ng{\^a}n V{\~u}, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam~Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco~JR Ruiz, Abbas Mehrabian, et~al.
\newblock Alphaevolve: A coding agent for scientific and algorithmic discovery.
\newblock \emph{arXiv preprint arXiv:2506.13131}, 2025.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Pun et~al.(2025)Pun, Deng, Liu, Ramanan, Liu, and Zhu]{pun2025generating}
Ava Pun, Kangle Deng, Ruixuan Liu, Deva Ramanan, Changliu Liu, and Jun-Yan Zhu.
\newblock Generating physically stable and buildable brick structures from text.
\newblock In \emph{ICCV}, 2025.

\bibitem[Putta et~al.(2024)Putta, Mills, Garg, Motwani, Finn, Garg, and Rafailov]{putta2024agent}
Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov.
\newblock Agent q: Advanced reasoning and learning for autonomous ai agents.
\newblock \emph{arXiv preprint arXiv:2408.07199}, 2024.

\bibitem[Qin et~al.(2024)Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, Zhao, Hong, Tian, Xie, Zhou, Gerstein, dahai li, Liu, and Sun]{qin2024toolllm}
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun.
\newblock Tool{LLM}: Facilitating large language models to master 16000+ real-world {API}s.
\newblock In \emph{ICLR}, 2024.

\bibitem[Qiu et~al.(2023)Qiu, Liu, Feng, Xue, Feng, Liu, Zhang, Weller, and Sch{\"o}lkopf]{qiu2023controlling}
Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Sch{\"o}lkopf.
\newblock Controlling text-to-image diffusion by orthogonal finetuning.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Qiu et~al.(2025{\natexlab{a}})Qiu, Buchholz, Xiao, Dax, Sch{\"o}lkopf, and Liu]{qiu2025reparameterized}
Zeju Qiu, Simon Buchholz, Tim~Z Xiao, Maximilian Dax, Bernhard Sch{\"o}lkopf, and Weiyang Liu.
\newblock Reparameterized llm training via orthogonal equivalence transformation.
\newblock In \emph{NeurIPS}, 2025{\natexlab{a}}.

\bibitem[Qiu et~al.(2025{\natexlab{b}})Qiu, Liu, Feng, Liu, Xiao, Collins, Tenenbaum, Weller, Black, and Sch{\"o}lkopf]{qiu2025sgpbench}
Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim~Z. Xiao, Katherine~M Collins, Joshua~B Tenenbaum, Adrian Weller, Michael~J Black, and Bernhard Sch{\"o}lkopf.
\newblock Can large language models understand symbolic graphics programs?
\newblock In \emph{ICLR}, 2025{\natexlab{b}}.

\bibitem[Qiu et~al.(2025{\natexlab{c}})Qiu, Liu, Weller, and Sch{\"o}lkopf]{qiu2025oftv2}
Zeju Qiu, Weiyang Liu, Adrian Weller, and Bernhard Sch{\"o}lkopf.
\newblock Orthogonal finetuning made scalable.
\newblock In \emph{EMNLP}, 2025{\natexlab{c}}.

\bibitem[Renze \& Guven(2024)Renze and Guven]{renze2024self}
Matthew Renze and Erhan Guven.
\newblock Self-reflection in llm agents: Effects on problem-solving performance.
\newblock \emph{arXiv preprint arXiv:2405.06682}, 2024.

\bibitem[Ritchie et~al.(2023)Ritchie, Guerrero, Jones, Mitra, Schulz, Willis, and Wu]{ritchie2023neurosymbolic}
Daniel Ritchie, Paul Guerrero, R~Kenny Jones, Niloy Mitra, Adriana Schulz, Karl~D Willis, and Jiajun Wu.
\newblock Neurosymbolic models for computer graphics.
\newblock In \emph{Eurographics}, 2023.

\bibitem[Savva et~al.(2019)Savva, Kadian, Maksymets, Zhao, Wijmans, Jain, Straub, Liu, Koltun, Malik, et~al.]{savva2019habitat}
Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et~al.
\newblock Habitat: A platform for embodied ai research.
\newblock In \emph{CVPR}, 2019.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli, Hambro, Zettlemoyer, Cancedda, and Scialom]{schick2023toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Schwarz et~al.(2018)Schwarz, Czarnecki, Luketina, Grabska-Barwinska, Teh, Pascanu, and Hadsell]{schwarz2018progress}
Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Shao et~al.(2024)Shao, Wang, Zhu, Xu, Song, Bi, Zhang, Zhang, Li, Wu, et~al.]{shao2024deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK~Li, Yang Wu, et~al.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Sheng et~al.(2025)Sheng, Zhang, Ye, Wu, Zhang, Zhang, Peng, Lin, and Wu]{sheng2025hybridflow}
Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru~Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu.
\newblock Hybridflow: A flexible and efficient rlhf framework.
\newblock In \emph{Proceedings of the Twentieth European Conference on Computer Systems}, pp.\  1279--1297, 2025.

\bibitem[Shinn et~al.(2023)Shinn, Cassano, Gopinath, Narasimhan, and Yao]{shinn2023reflexion}
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Shridhar et~al.(2021)Shridhar, Yuan, Cote, Bisk, Trischler, and Hausknecht]{shridhar2020alfworld}
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht.
\newblock Alfworld: Aligning text and embodied environments for interactive learning.
\newblock In \emph{ICLR}, 2021.

\bibitem[Sun et~al.(2025)Sun, Han, Deng, Wang, Qin, and Gould]{sun20253d}
Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould.
\newblock 3d-gpt: Procedural 3d modeling with large language models.
\newblock In \emph{3DV}, 2025.

\bibitem[Tang et~al.(2025)Tang, Zheng, Synnaeve, and Munos]{tang2025optimizing}
Yunhao Tang, Kunhao Zheng, Gabriel Synnaeve, and Remi Munos.
\newblock Optimizing language models for inference time objectives using reinforcement learning.
\newblock In \emph{ICML}, 2025.

\bibitem[Teng et~al.(2025)Teng, Yu, Shi, Zhang, Wu, and Luo]{teng2025atom}
Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo.
\newblock Atom of thoughts for markov llm test-time scaling.
\newblock \emph{arXiv preprint arXiv:2502.12018}, 2025.

\bibitem[Wang et~al.(2024)Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and Anandkumar]{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{Transactions on Machine Learning Research (TMLR)}, 2024.

\bibitem[Wang et~al.(2025)Wang, Yang, Zeng, Ren, Liu, Peng, Cheng, He, Wang, Gao, et~al.]{wang2025reinforcement}
Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et~al.
\newblock Reinforcement learning for reasoning in large language models with one training example.
\newblock \emph{arXiv preprint arXiv:2504.20571}, 2025.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, brian ichter, Xia, Chi, Le, and Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed~H. Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Wong et~al.(2025)Wong, Lyu, Rios, Menzel, and Ong]{wong2025llm}
Melvin Wong, Yueming Lyu, Thiago Rios, Stefan Menzel, and Yew-Soon Ong.
\newblock Llm-to-phy3d: Physically conform online 3d object generation with llms.
\newblock \emph{arXiv preprint arXiv:2506.11148}, 2025.

\bibitem[Wu et~al.(2023)Wu, Khasahmadi, Katz, Jayaraman, Pu, Willis, and Liu]{wu2023cad}
Sifan Wu, Amir Khasahmadi, Mor Katz, Pradeep~Kumar Jayaraman, Yewen Pu, Karl Willis, and Bang Liu.
\newblock Cad-llm: Large language model for cad generation.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Xiao et~al.(2025)Xiao, Bamler, Sch{\"o}lkopf, and Liu]{xiao2025verbalized}
Tim~Z Xiao, Robert Bamler, Bernhard Sch{\"o}lkopf, and Weiyang Liu.
\newblock Verbalized machine learning: Revisiting machine learning with language models.
\newblock \emph{Transactions on Machine Learning Research}, 2025.

\bibitem[Yao et~al.(2023{\natexlab{a}})Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Yao et~al.(2023{\natexlab{b}})Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2023react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock In \emph{ICLR}, 2023{\natexlab{b}}.

\bibitem[Yu et~al.(2025)Yu, Yuan, Xiao, Xia, Fu, Zhang, Liu, et~al.]{yu2025generating}
Zhouliang Yu, Yuhuan Yuan, Tim~Z Xiao, Fuxiang~Frank Xia, Jie Fu, Ge~Zhang, Weiyang Liu, et~al.
\newblock Generating symbolic world models via test-time scaling of large language models.
\newblock \emph{Transactions on Machine Learning Research}, 2025.

\bibitem[Yuan et~al.(2024)Yuan, Xu, Pan, Bousseau, Mitra, and Li]{yuan2024cadtalk}
Haocheng Yuan, Jing Xu, Hao Pan, Adrien Bousseau, Niloy~J Mitra, and Changjian Li.
\newblock Cadtalk: An algorithm and benchmark for semantic commenting of cad programs.
\newblock In \emph{CVPR}, 2024.

\bibitem[Yue et~al.(2025)Yue, Chen, Lu, Zhao, Wang, Yue, Song, and Huang]{yue2025does}
Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang.
\newblock Does reinforcement learning really incentivize reasoning capacity in {LLM}s beyond the base model?
\newblock In \emph{ICML}, 2025.

\bibitem[Zhang et~al.(2025)Zhang, Xiang, Yu, Teng, Chen, Chen, Zhuge, Cheng, Hong, Wang, Zheng, Liu, Luo, and Wu]{zhang2024aflow}
Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xiong-Hui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, and Chenglin Wu.
\newblock {AF}low: Automating agentic workflow generation.
\newblock In \emph{ICLR}, 2025.

\bibitem[Zhu et~al.(2025{\natexlab{a}})Zhu, Xia, Wei, Chen, Chen, and Meng]{zhu2025surprising}
Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu~Meng.
\newblock The surprising effectiveness of negative reinforcement in llm reasoning.
\newblock \emph{arXiv preprint arXiv:2506.01347}, 2025{\natexlab{a}}.

\bibitem[Zhu et~al.(2025{\natexlab{b}})Zhu, Cheng, Zhang, Li, Zhang, Jiang, Sun, Hua, Zuo, Lv, et~al.]{zhu2025flowrl}
Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo Hua, Yuxin Zuo, Xingtai Lv, et~al.
\newblock Flowrl: Matching reward distributions for llm reasoning.
\newblock \emph{arXiv preprint arXiv:2509.15207}, 2025{\natexlab{b}}.

\end{thebibliography}
