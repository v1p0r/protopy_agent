\newpage
\appendix
\addcontentsline{toc}{section}{Appendix} %
\renewcommand \thepart{} %
\renewcommand \partname{}
\part{\vspace{-7mm}\Large{\centerline{Appendix}}\vspace{-2mm}}
\parttoc


\newpage

\section{Supplementary Tables}

\begin{table}[h!]
  \scriptsize
  \centering
  \setlength{\tabcolsep}{4pt} %
  \renewcommand{\arraystretch}{1.2} %
  \begin{tabularx}{\textwidth}{l*{9}{>{\centering\arraybackslash}X}} %
    \toprule
    \multirow{2}{*}{Models} & \multicolumn{3}{c}{Meta-Designer \& Designer} & \multicolumn{3}{c}{Blind Refinement} & \multicolumn{3}{c}{Modification w/ Env Feedback} %
    \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
     & Valid & Mean & Max & Valid & Mean & Max & Valid & Mean & Max \\
    \midrule
    \multicolumn{10}{l}{\textit{"Catapult" Task}} \\
    \midrule
    Gemini 2.5 Pro        &5&8.49&9.14 &5&8.18&11.07&5&15.73&18.19\\
    Claude Opus 4            &4&4.17&4.36&2&5.38&5.8&2&9.10&9.32\\
    o3                       &3&0&0&3&0&0&3&5.34&11.11\\
    Qwen3-Coder-480B-A35B    &6&0.75&4.5&6&1.61&5.0&6&5.21&6.52\\
    Doubao Seed 1.6           &3&4.34&4.37&3&0.31&0.49&3&4.62&4.76 \\
    DeepSeek-V3                &7&0&0&7&0.98&3.18&4&4.82&4.93\\
    Kimi K2                &6&7.18&12.02&2&5.29&7.36&0&-&-\\
    GPT-5                   &8&3.87&4.92&1&5.35&5.35&0&-&-\\
    Llama 4 Scout 17B 16E   &8&3.59&11.83&0&-&-&0&-&-\\
    \midrule
    \multicolumn{10}{l}{\textit{"Car" Task}} \\
    \midrule
    Gemini 2.5 Pro        &7&27.85&38.06&7&17.81&39.64&7&29.96&41.52\\
    Claude Opus 4       &3&2.96&3.41&3&36.18&37.05&3&26.59&38.67  \\
    o3                    &8&29.27&41.43&2&20.03&40.04&2&28.39&36.18\\
    Qwen3-Coder-480B-A35B  &6&5.43&8.72&6&3.90&11.25&6&11.75&34.05\\
    Doubao Seed 1.6       &5&21.80&29.91&5&13.25&26.05&5&18.75&26.02 \\
    DeepSeek-V3           &3&0.27&0.47&3&16.94&29.87&3&17.92&31.94  \\
    Kimi K2               &1&6.74&6.74&1&0.39&0.39&1&14.99&14.99  \\
    GPT-5                 &8&5.67&20.32&8&3.75&9.65&8&8.43&13.72 \\
    Llama 4 Scout 17B 16E &4&1.55&2.00&1&0.47&0.47&1&0.47&0.47 \\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Comparison between the performance of machines generated by different stages. The mean score is computed by taking the average of the scores of all valid machines. We sample 8 machines at the designer stage and keep only the valid machines. The maximum number of retries in the following stages is thus equal to the number of valid machines produced at the designer stage.}
  \label{tab:test time scaling benchmark}
\end{table}

\begin{table}[h!]
  \scriptsize
  \centering
  \setlength{\tabcolsep}{4pt} %
  \renewcommand{\arraystretch}{1.2} %
  \begin{tabularx}{\textwidth}{l*{9}{>{\centering\arraybackslash}X}} %
    \toprule
    \multirow{2}{*}{Models} & \multicolumn{3}{c}{Designer} & \multicolumn{3}{c}{Blind Refinement} & \multicolumn{3}{c}{Modification w/ Env Feedback} 
    \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
     & Valid & Mean & Max & Valid & Mean & Max & Valid & Mean & Max \\
    \midrule
    \multicolumn{10}{l}{\textit{"Catapult" Task}} \\
    \midrule
    Gemini 2.5 Pro      &3&6.13&9.0 &3&8.10&12.09 &3&11.08&21.95 \\
    Claude Opus 4       &2&4.76&4.91 &0&-&-&0&-&-    \\
    o3                 &8&2.87&5.22 &8&2.98&9.17&8&9.14&14.01      \\
    Qwen3-Coder-480B-A35B   &4&3.5&9.24 &4&6.39&10.78&4&10.2&12.02 \\
    Doubao Seed 1.6        &6&4.24&8.2 &6&4.61&8.75&6&6.43&9.10   \\
    DeepSeek-V3            &6&4.67&4.86&5&4.33&4.78&5&4.91&5.24    \\
    Kimi K2              &3&6.85&9.05&2&8.31&8.97&2&11.28&11.39  \\
    GPT-5  &5&1.50&1.88     &5&5.86&12.77&5&7.53&9.48            \\
    Llama 4 Scout 17B 16E  &7&3.63&5.64 &2&5.88&6.95&2&5.12&5.94 \\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Performance of machines generated after different stages of the iterative editing workflow (without meta-designer). Mean scores are computed on valid machines.}
  \label{tab:test time scaling designer benchmark}
\end{table}




\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \newcommand{\cgr}[1]{\textcolor[rgb]{.329, .51, .208}{\textbf{#1}}}
  \newcommand{\cre}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}

  \begin{tabularx}{\textwidth}{l*{6}{>{\centering\arraybackslash}X}}
    \toprule
    \multicolumn{1}{c}{\multirow{2.4}{*}{Models}}
    & \multicolumn{3}{c}{Baseline}
    & \multicolumn{3}{c}{w/o Parsed 3D Information} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    & Valid & Mean & Max 
    & Valid & Mean & Max \\
    \midrule
    Gemini 2.5 Pro
       & 5 & 8.18 & 11.07
       & 5 & 7.13 & 11.36 \\
    o3
       & 3 & 0 & 0
       & 3 & 0 & 0 \\
    Qwen3-Coder-480B-A35B
       & 5 & 1.61 & 5.0
       & 0 & - & - \\
    Claude Opus 4
       & 2 & 5.38 & 5.8
       & 2 & 0.18 & 0.26 \\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Ablation on the effect of parsed 3D information. We compute the blind refinement score under two machine representations. The average score is computed with respect to valid machines only; 8 tries for each experiment.}
  \label{tab:abl:machine-3d-info}
\end{table}


\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{3pt}       %
  \renewcommand{\arraystretch}{1.1} %
  \newcommand{\cgr}[1]{\textcolor[rgb]{.329, .51, .208}{\textbf{#1}}} %
  \newcommand{\cre}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}         %

  \begin{tabularx}{\textwidth}{l*{4}{>{\centering\arraybackslash}X}} %
    \toprule
    \multirow{2}{*}{Models} & \multicolumn{2}{c}{Refiner Avg Retry $\downarrow$} & \multicolumn{2}{c}{Refiner validity rate $\uparrow$} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    & Baseline & w/o Modify History & Baseline & w/o Modify History \\ %
    \midrule
    Gemini 2.5 Pro        & 1.42  & \cgr{1.33}         & 100\% & 100\% \\
    o3                    & 1.94  & \cre{2.37}                 & 97.87\% & \cre{88.89\%} \\
    Qwen3-Coder-480B-A35B & 2.50    & \cre{2.75}          & 82.65\% & \cgr{91.94\%} \\
    Doubao Seed 1.6       & 2.74  & \cre{2.93}            & 85.18\% & 85.18\% \\
    Claude Opus 4       & 3.24  &  \cre{3.69}                 & 94.12\% & \cre{53.85\%}              \\
    DeepSeek-V3           & 1.54  &  \cre{1.68}                 & 100\% &  \cre{98.31\%}            \\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Ablation of edit history as refiner inputs.}
  \label{tab:edit_history}
\end{table}


\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \newcommand{\cgr}[1]{\textcolor[rgb]{.329, .51, .208}{\textbf{#1}}}
  \newcommand{\cre}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}
  \renewcommand{\pm}{\mathbin{\text{Â±}}}

  \begin{tabularx}{\textwidth}{lCCC>{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{1.5cm}}
    \toprule
    \multirow{2}{*}{Models}
    & \multicolumn{3}{c}{Machine Validity (pass/total)}
    & \multicolumn{2}{c}{Designer Score} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-6}
    & File Valid & 3D Valid & Final Valid & Mean & Max \\
    \midrule
    \multicolumn{6}{l}{\textit{Baseline (construction tree)}} \\
    \midrule
    Gemini 2.5 Pro pro    & 8/8 & 5/8 & 5/8 & 8.49 & 9.14 \\
    o3            & 3/8 & 3/3 & 3/8 & 0      & 0 \\
    Qwen3-Coder-480B-A35B    & 8/8 & 6/8 & 6/8 & 0.75 & 4.5 \\
    Doubao Seed 1.6    & 7/8 & 3/7 & 3/8 & 4.34 & 4.37 \\
    Claude Opus 4      & 8/8 & 4/8 & 4/8 & 4.17 & 4.36 \\
    DeepSeek-V3   & 7/8 & 7/7 & 7/8 & 0      & 0 \\
    Kimi K2       & 6/8 & 6/6 & 6/8 & 7.18 & 12.02 \\
    Llama 4 Scout 17B 16E        & 8/8 & 8/8 & 8/8 & 3.59      & 11.83 \\
    \midrule
    \multicolumn{6}{l}{\textit{Global position-based 3D representation}} \\
    \midrule
    Gemini 2.5 Pro pro    & 5/8 & 5/8 & 5/8 & 4.96 & 12.85 \\
    o3            & 0/8 & - & - & - & - \\
    Claude Opus 4      & 0/8 & - & - & - & - \\
    Kimi K2       & 5/8 & 4/5 & 4/8 & 0 & 0 \\
    Llama 4 Scout 17B 16E        & 0/8 & - & - & -      & - \\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Ablation study on machine representations.}
  \label{tab:rep_comparison}
\end{table}



\clearpage
\pagebreak
\newpage
\section{Details on the \envname Environment} \label{appendix:env}

We built \envname by creating plug-in modules for the game Besiege that create interfaces to allow flexible composition of parts (once certain rules are obeyed), control policies on multiple powered parts (\eg. powered cogs), recording of state information of any block (\eg, position, orientation, part integrity, etc.) and settings of termination conditions (\eg, some part passing through a line). \envname supports multi-process launching and thus allows for efficient parallel RL training. As the game natively supports multi-player gameplay, \envname can naturally be applied to multi-agent RL settings. As the game Besiege (shown in Fig.~\ref{fig:game_editor_view}) is built with the (mostly) open-sourced Unity3D game engine\footnote{https://en.wikipedia.org/wiki/Unity\_(game\_engine)}, \envname is highly-customizable: the environment 1) natively supports modification of physical parameters, external forces, terrains and obstacles (\eg, stone buildings) and 2) allows for extension patches (known as mods\footnote{https://en.wikipedia.org/wiki/Video\_game\_modding}) to introduce other mechanisms, such as new block types, fluid simulation and many other components.



\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/game_editor_view.png}
  \caption{\footnotesize Besiege editor view.}
  \label{fig:game_editor_view}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/block_stats.png}
  \caption{\footnotesize Common useful blocks in constructing standard machines in \envname.}
  \label{fig:block_intro}
\end{figure}

\subsection{Construction Rule}

Each machine is built by attaching new blocks to the existing structure, starting from a special root block. For convenience, we describe each construction step as an âattacherâ block (child) connected to an âattacheeâ block (parent). As an attacher, each block has exactly one face available for connection; as an attachee, each block has none to several attachable faces. Once a face is used, it is considered occupied and cannot be reused. If, after construction, the free end of a block happens to coincide with an attachable face of an existing block, the two blocks are automatically connected.

A few special blocks violate the rule described above, such as spring. These blocks have two ends and thus must have two parent blocks, do not have physical volume can be attached to either vacant or occupied faces of other blocks.

Finally, each block can be rescaled and rotated after construction. Since post-construction scaling and rotation introduce unnecessary complexity into our pipeline, we exclude them from our experiments and leave their handling to future work.

\subsection{Simulation}

Once constructed, the machine will be placed at the designated pose indicated by the position and orientation of the starting block (not necessary near the ground, but there is a maximum height constraint). The machine will be subject to gravity and Newtonian physical laws (rigid and elastic ones) after placement.

\subsection{Blocks}\label{sec:b.1}
Out of the 75 construction blocks provided by Besiege, we filter out a list of 27 blocks (shown in Fig.~\ref{fig:block_intro}) that are most relevant to build machines with classical mechanical mechanisms such as levers and trusses.


\begin{itemize}
    \item \textbf{Starting Block}: the root of any mechanism; initial orientation is along z+ axis.
    \item \textbf{Small Wooden Block}: a cubic basic construction block.
    \item \textbf{Wooden Block}: shaped like two small wooden blocks attached together.
    \item \textbf{Wooden Rod}: a slender, fragile construction block.
    \item \textbf{Log}: shaped like three small wooden blocks arranged in parallel.
    \item \textbf{Steering Hinge}: powered; controls rotation of sub-blocks, swinging left or right along the axis perpendicular to its placement axis.
    \item \textbf{Steering Block}: powered; rotates blocks along its placement axis.
    \item \textbf{Powered Wheel}: radius 1m; provides ground movement.
    \item \textbf{Unpowered Wheel}: identical to the powered wheel but requires external force to rotate.
    \item \textbf{Large Powered Wheel}: larger version of the powered wheel (radius 3m).
    \item \textbf{Large Unpowered Wheel}: unpowered version of the large powered wheel.
    \item \textbf{Small Wheel}: functions like a caster wheel (e.g., shopping cart), unpowered, 1.2m long.
    \item \textbf{Roller Wheel}: similar to the small wheel, but shorter (0.8m).
    \item \textbf{Universal Joint}: freely rotates around its placement axis, unpowered.
    \item \textbf{Hinge}: swings up and down along the axis perpendicular to its placement axis, unpowered.
    \item \textbf{Ball Joint}: swings freely in all directions, unpowered.
    \item \textbf{Axle Connector}: similar to a ball joint but allows unrestricted $360^\circ$ rotation.
    \item \textbf{Suspension}: shaped like a wooden block, it can buffer
forces from all directions.
    \item \textbf{Rotating Block}: powered; motor-like block that generates torque and rotates about its local y-axis.
    \item \textbf{Grabber}: grabs objects on contact and can release them.
    \item \textbf{Boulder}: a large rock, loosely attached; useful for throwing.
    \item \textbf{Grip Pad}: block with the highest friction.
    \item \textbf{Elastic Pad}: block with the highest elasticity.
    \item \textbf{Container}: typically used to hold a boulder.
    \item \textbf{Spring}: can contract; one of the special blocks that can have two parent attachments (without occupying attachable faces).
    \item \textbf{Brace}: reinforces structural strength.
    \item \textbf{Ballast}: a heavy cubic block used as a counterweight.
\end{itemize}

\subsection{Tasks}

We define a set of tasks in which the goal is to construct machines within a designated building area to accomplish specific objectives.

\begin{itemize}
    \item \textbf{Movement.}
Referred to as the \textit{car} task in the main text, the objective is to build a machine capable of driving along tracks and traversing various terrains.
    \item \textbf{Throw.}
Referred to as the \textit{catapult} task in the main text, the goal is to construct a machine that can launch boulders over long distances. To prevent unintended strategies (e.g., carrying the boulder instead of throwing it, or letting it roll along the ground), the building area is enclosed by a medium-height wall.
    \item \textbf{Delivery.}
This task requires building a machine that can transport a large stone forward across different terrains (Fig.~\ref{fig:task_delivery}).

    \item \textbf{Pick.}
The objective here is to design a machine that can retrieve a stone located at the bottom of a deep well (Fig.~\ref{fig:task_pick}).

\end{itemize}

For many of these tasks, we introduce multiple difficulty levels (not used in the experiments reported in this paper) to encourage progressively more sophisticated designs:

\begin{itemize}
    \item \textbf{Movement and Delivery.}
We consider: (1) randomized terrains with stones and wooden rods (\eg, Fig.~\ref{fig:task_movement_rocky}), (2) curved tracks (Fig.~\ref{fig:task_curved_track}), and (3) obstacles such as height-limiting bars.

    \item \textbf{Throw.}
We design: (1) varied objectives, such as requiring the boulder to pass through an aerial ring (Fig.~\ref{fig:task_thru_ring}) or land precisely within a small target zone, (2) environmental factors such as wind, and (3) obstacles, including height restrictions either within the building area or along the boulderâs trajectory.

\end{itemize}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/tasks/movement_rocky.png}
  \caption{\footnotesize Illustration of the task \textit{car} / \textit{movement} on a rocky terrain, a more difficult setting compared to the environment used for the \textit{car} task in our experiments.}
  \label{fig:task_movement_rocky}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/tasks/throw.png}
  \caption{\footnotesize Illustration of the task \textit{catapult} / \textit{throw}.}
  \label{fig:task_throw}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/tasks/pick.png}
  \caption{\footnotesize Illustration of the task \textit{pick}.}
  \label{fig:task_pick}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/tasks/delivery.png}
  \caption{\footnotesize Illustration of the task \textit{delivery} with a bump on the track.}
  \label{fig:task_delivery}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/tasks/thru_ring.png}
  \caption{\footnotesize Illustration of the task \textit{catapult} / \textit{Throw} with the objective of throwing the boulder through the target ring.}
  \label{fig:task_thru_ring}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/tasks/curved_track.png}
  \caption{\footnotesize Illustration of the task \textit{car} / \textit{movement} with a curved track.}
  \label{fig:task_curved_track}
\end{figure}











\clearpage
\newpage

\section{Search Strategies in Machine Modification Loops}



Apart from the MCTS strategy used in the main experiments, we also evaluate two alternatives: (i) best-of-N, where we select the best-performing machine out of N candidates, and (ii) random search, which mimics best-of-N but instead selects a random candidate. For clarity, we refer to one consecutive ``querierârefiner'' call as a search node (consistent with our MCTS setup). Unlike classical MCTS or best-of-N, here each search node is allowed up to five retries to prevent child statistics from being too sparse. We perform $R$ search rounds, each aiming to obtain $5$ valid candidate machines (though this may fail; if fewer than 5 are found, the parent nodeâs machine is used as a candidate). Full algorithmic details are provided in Algorithm~\ref{alg:random-search}, Algorithm~\ref{alg:bestofn}, and Algorithm~\ref{alg:mcts}. In Fig.~\ref{tab:mcts_ablation} we show the improvement of machine performance with respect to the number of search rounds used. In Fig.~\ref{fig:search_curve} we compare the efficiency of different search methods in our agentic compositional machine design setting.



\begin{algorithm}
\caption{Random Search Algorithm}
\label{alg:random-search}
\begin{algorithmic}[1]
\Require Agentic Search Node $N$
\Require Scoring function $S$, machine valid check function $F$
\Require Search Round $R$
\Ensure The Best result with the highest score

\State Input machine $ori\_machine$
\State $max\_retry \gets 5$
\State $machine\_last\_round \gets ori\_machine$

\For{$r=1$ to $R$}
    \State $best\_score \gets -\infty$
    \State $retry \gets 0$
    \While{$retry < max\_retry$}
        \State $retry \gets retry + 1$
        \State $machine\_next\_round \gets N.generate(machine\_last\_round)$
        \If{$F(machine\_next\_round)$}
            \State \textbf{break} 
        \EndIf
    \EndWhile
    \State $score \gets S(machine\_next\_round)$
    \State $machine\_last\_round \gets machine\_next\_round$
\EndFor

\State \Return $(machine\_last\_round, score)$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Best-of-N Algorithm}
\label{alg:bestofn}
\begin{algorithmic}[1]
\Require Agentic Search Node $N$
\Require Scoring function $S$, machine valid check function $F$
\Require Search Round $R$, number of samples $n$
\Ensure The Best result with the highest score

\State Input machine $ori\_machine$
\State $best\_score \gets -\infty$
\State $best\_machine \gets ori\_machine$
\State $max\_retry \gets 5$

\For{$r=1$ to $R$}
    \State $best\_score \gets -\infty$
    \State $best\_machine\_this\_round \gets best\_machine$
    \For{$i=1$ to $n$}
        \State $retry \gets 0$
        \While{$retry < max\_retry$}
            \State $retry \gets retry + 1$
            \State $machine_i \gets N.generate(best\_machine\_this\_round)$
            \If{$F(machine_i)$}
                \State \textbf{break} 
            \EndIf
        \EndWhile
        \State $score_i \gets S(machine_i)$
        \If{$score_i > best\_score$}
            \State $best\_score \gets score_i$
            \State $best\_machine \gets machine_i$
        \EndIf
    \EndFor
\EndFor

\State \Return $(best\_machine, best\_score)$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Monte Carlo Tree Search (MCTS)}
\label{alg:mcts}
\begin{algorithmic}[1]
\Require Agentic Search Node $N$
\Require Root node $root$, maximum iterations $MAX\_ITER$
\Require \text{Select}(node): Traverse tree using UCB until leaf node
\Require \text{Expand}(node): Generate 4 child candidates via LLM. Validate them in parallel: keep valid ones, and regenerate invalid ones until they pass or hit the max retry limit. If all fail, use the parent node as the child.
\Require \text{Simulate}(node): Besiege simulation
\Require \text{Backpropagate}(node, reward): Update visit counts and rewards along path
\Require \text{BestChild}(node): Return child with highest simulation score
\Ensure Best action from the search tree

\State $root \gets s_0$
\State $max\_retry \gets 5$
\For{$i = 1$ to $MAX\_ITER$}
    \State $retry \gets 0$
    \State $node \gets \text{Select}(root)$ \Comment{Step 1: Selection}
    \If{$\textit{node} == \textit{root}$ or $\textit{node}.\textit{visited}$}
        \State $\textit{should\_expand} \gets True$
    \EndIf
    \If{not $\textit{should\_expand}$}
        \State $child \gets node$ \Comment{Unvisited leaf \textit{node}; no children yet}
    \EndIf
    \While{$\textit{should\_expand}$ and not all child nodes are valid}
        \State $retry \gets retry + 1$
        \State $child \gets \text{Expand}(node)$ \Comment{Step 2: Expansion}
        \If{$retry \geq max\_retry$}
            \State \textbf{break} 
        \EndIf
    \EndWhile
    \State $reward \gets \text{Simulate}(child)$ \Comment{Step 3: Simulation}
    \State $\text{Backpropagate}(child, reward)$ \Comment{Step 4: Backpropagation}
\EndFor
\State \textbf{return} $\text{BestChild}(root)$ \Comment{Return best child}
\end{algorithmic}
\end{algorithm}



\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{6pt}       %
  \renewcommand{\arraystretch}{1.2} %
  \newcommand{\cgr}[1]{\textcolor[rgb]{.329, .51, .208}{\textbf{#1}}}
  \newcommand{\cre}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}
  \begin{tabularx}{\textwidth}{l*{9}{>{\centering\arraybackslash}X}} %
    \toprule
    \multirow{2}{*}{Models} & \multicolumn{3}{c}{Random Search} & \multicolumn{3}{c}{Best-of-N } & \multicolumn{3}{c}{MCTS} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
     & Avg.I & Mean  & Max & Avg.I & Mean  & Max & Avg.I & Mean  & Max \\
    \midrule
    Gemini 2.5 Pro       & 5 & 15.02     & 20.5    & 20 & 14.67  &16.66  & 8 &15.73     & 18.19     \\
    Claude Opus 4       & 5 &  7.67   &  7.88   & 18 & 8.18  & 8.50  & 6 &  9.10    &   9.32   \\
    o3                    & 5 & 7.71     & 11.94     & 8 & 10.60  &15.07  & 7 & 5.34     & 11.11     \\
    Qwen3-Coder-480B-A35B & 5 &4.50     &7.64     & 11 &5.61   &9.87  & 8.5 & 5.21  & 6.52  \\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Ablation study on different search strategies. We compare the agentic workflow final scores. MCTS is executed for 5 rounds, with Random Search and Best-of-N also run for the same number of rounds. Avg.I denotes the average number of node expansions per search round.}
  \label{tab:abl:env-search}
\end{table}


\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{6pt}       %
  \renewcommand{\arraystretch}{1.2} %
  \newcommand{\cgr}[1]{\textcolor[rgb]{.329, .51, .208}{\textbf{#1}}}
  \newcommand{\cre}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}
  \begin{tabularx}{\textwidth}{l*{6}{>{\centering\arraybackslash}X}} %
    \toprule
    \multirow{2}{*}{Models} & \multicolumn{2}{c}{R2} & \multicolumn{2}{c}{R5} & \multicolumn{2}{c}{R10} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
     & Mean  & \multicolumn{1}{c}{Max} & Mean  & \multicolumn{1}{c}{Max} & Mean  & Max \\
    \midrule
    Gemini 2.5 Pro            &15.04&17.31&15.73&18.19&16.44&18.19\\
    Claude Opus 4       &8.61&9.32&9.10&9.32&9.43&9.98\\
    o3                   &5.33&11.11&5.34&11.11&8.46&14.52 \\
    Qwen3-Coder-480B-A35B &5.18&6.52&5.21&6.52&5.74&6.52\\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Ablation study on the effect of search depth in MCTS. R2, R5, and R10 represent the running rounds of MCTS on the same search tree.}
  \label{tab:mcts_ablation}
\end{table}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/RL_Metrics/search_strategy.png}
  \caption{\footnotesize The variation in machine average scores with the increasing number of LLM node expansion operations under different search strategies.}
  \label{fig:search_curve}
\end{figure}

\clearpage
\newpage
\section{Environment Settings for Agentic Design and RL Finetuning}

\subsection{Machine Representation}
\label{sec:machine_rep}

To reduce complexity in compositional machine design, our machine representation assumes all blocks remain at their default scale and are not further rotated after attachment (note: the attachment operation itself may rotate blocks).

\subsection{Global Position-based Representation}
By simplifying the default XML representation that \envname receives, we obtain the global position-based representation. Below is a concrete example:
\begin{lstlisting}
[
    {"type": 0, "Position": [0,0,  0], "Rotation": [0,0,0,1]},
    {"type": 1, "Position": [0,0,0.5], "Rotation": [0,0,0,1]},
    {"type": 2, "Position": [0,0,2.5], "Rotation": [0,0,0,1]},
    {"type": 9, "Position": [0,0.5,2], "Rotation": [-0.707,0,0,0.707],
                "end-position": [0,2,0]}
]
\end{lstlisting}

Basically, each block in the machine is independently recorded without mentioning its adjacent blocks. For most of the block types, only the block type and its pose (position + orientation) are recorded. For special blocks that have two parents, the other end has to be specified, for which the corresponding dictionary has an additional entry of ``end-position''.


\subsection{Construction Tree Representation}
\label{sec:tree_rep}

With our parsimonious construction tree representation, the example machine above is represented by the following the following JSON list:
\begin{lstlisting}
[
    {'type': 0, 'id': 0, 'parent'  : -1, 'face_id'  : -1}, 
    {'type': 1, 'id': 1, 'parent'  :  0, 'face_id'  :  0}, 
    {'type': 2, 'id': 2, 'parent'  :  1, 'face_id'  :  0}, 
    {'type': 9, 'id': 3, 'parent_a':  0, 'face_id_a':  4,
                         'parent_b':  1, 'face_id_b':  6}
]
\end{lstlisting}

Specifically, the ordered list of dictionaries of the machine construction JSON file represents the construction order of blocks. Each dictionary contains the following information of corresponding block: 1) ``type'': block type; 2) ``id'': the order ID of this block (the same as the order in the list), included so that LLMs do not have to parse it by itself; 3) ``parent'', the ID of its parent block; 4) ``face\_id'', the face of the block's parent to which the block is attached. In cases that the block has two parents (\eg, a string that connects both parts), we use ``parent\_a'' and ``parent\_b'' to record both parents; similar for ``face\_id''.

Note: the first block with ``id'' 0 is always the unique starting block, of which the local position and rotation are always zero. 








\subsection{Reward Setting} \label{sec:reward setting}

Here we elaborate on the reward design for RL experiments in Sec.~\ref{sec: setting}. Our reward is in the form of $R = \texttt{is\_valid} \times \texttt{performance}$ where $\texttt{is\_valid}$ is the boolean representing machine validity and $\texttt{performance}$ is the task-specific performance metric.


\textbf{\textit{Car}.} We set $\texttt{is\_valid}$ to $1$ as long as the policy produces a machine that can be parsed from the generated construction tree and can be successfully placed into the environment without any self-collision; otherwise it is set to 0. $\texttt{performance}$ is set to the distance between the starting position and the end position of the root block. 


\textbf{\textit{Catapult}.} For $\texttt{is\_valid}$ to be $1$ in this task, the machine has to satisfy an additional constraint compared to the \textit{car} task: the maximum height of the boulder position during simulation must be greater than a threshold of 3m. As explained in the main text, $\texttt{performance}$ for $\textit{catapult}$ is the product of the maximum height and maximum distance (towards some pre-defined direction) during simulation.


\subsection{Environment Feedback}\label{sec:env_feedback}

In principle, we are able to obtain all state variables of each single part of a simulated machine. Due to the space complexity of the simulation results, not all information can be fed to LLM agents. Here we consider a minimal set of environment feedback information that the environment querier always gathers and returns to the refiner. Below are the minimal set of feedback information for each task:


\textit{Car.} 1) machine orientation; 2) machine maximum moving distance (towards a designated direction); 3) machine max speed; 4) machine average speed per second; 5) machine position per 0.2 second (atomic time).

\textit{Catapult.} 1) boulder maximum distance (horizontal, towards a designated distance); 2) boulder maximum height; 3) boulder position per 0.2 second (atomic time).

Beyond these basic pieces of feedback, the querier, after seeing the candidate machine and its simulation results, in our default setting selectively extract a subset of environment feedback given its speculation on the issues of the simulated machine. For instance, parts during simulation may collide with each other and break. Such behavior carries important hints on why machines fail, and an LLM agent with sufficient capability in spatial and physics understanding can possibly identify the vulnerable blocks in the design.

Below we elaborate on the additional information that the querier may gather: \begin{itemize}
    \item Block index to query;
    \item Time interval of interest (states outside this interval will not be considered);
    \item Feedback type of interest (one or more from the list)
    \begin{itemize}
        \item Position;
        \item Orientation;
        \item Velocity;
        \item Length (for spring only)
    \end{itemize}
\end{itemize}








\clearpage
\newpage
\section{Challenges in Compositional Machine Design}

\subsection{Failure Patterns}

Generated machines often fail in systematic ways. As shown in Fig.~\ref{fig:failure-mode}, we observe several recurring categories of errors, including flawed reasoning, structural attachment errors, incorrect part orientations and failures in instruction following. These diverse failure types highlight both the reasoning and execution challenges inherent in compositional machine design.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/failure_mode_v3_cropped.pdf}
  \caption{\footnotesize Examples to illustrate failure patterns. In each example, the original machine is shown on the left and the modified machine on the right. Failure patterns are sampled from Qwen3-Coder-480B-A35B-Instruct.}
  \label{fig:failure-mode}
\end{figure}


\subsection{Need for Precision}


In Fig.~\ref{fig:tiny-modify} we present a simple example to illustrate how the task of compositional machine design requires high precision in the spatial design of configurations of different parts. Even though the high-level design is feasible, the machine in the top row fails to throw the boulder out due to the incorrect position of the container.


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/tiny_modify_v3_cropped.pdf}
  \caption{\footnotesize Illustration of how machines built with feasible high-level designs may fail due to inaccurate part placement. Machine sampled from Gemini 2.5 Pro. Left: designed machines; Right: simulation results.}
  \label{fig:tiny-modify}
\end{figure}

\subsection{Appearance vs.~Performance}

As illustrated in Fig.~\ref{fig:throw-traj}, a machine's appearance does not necessarily reflect its actual performance. A design that seems well-aligned with human intuition can fail dramatically, while one that looks awkward or unintuitive may achieve superior results. For LLMs to design machines that are both effective and visually intuitive to humans, reward functions must account not only for task performance but also for stability and other factors that shape human perception of functionality.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/throw_comp_v4_cropped.pdf}
  \caption{\footnotesize Boulder-throwing trajectories for various machine designs generated by Gemini 2.5 Pro. From left to right, each row first shows the machine design, followed by a time-lapsed birdâs-eye view of its throw.}
  \label{fig:throw-traj}
\end{figure}


\clearpage
\newpage
\section{Settings for RL Finetuning}



\subsection{Cold-Start Dataset Curation} \label{sec:cold_start_data} 


Noticing that Gemini 2.5 Pro produces the most satisfactory machines with reasonable CoT, we adopt the single-agent generation setting and collect Gemini-generated machines along with their CoT. We curated 100 design objectives: 75 captions of machines created by Besiege players from the Internet and 25 authored by us. These 25 prompts are constructed by 1) first writing down simple design objectives that are realizable by \envname and can emerge from some simple rewards, and 2) then introducing environment constraints and machine-specific requirements. Using this prompt dataset, we generate 250 machines per prompt, and after filtering out inappropriate ones (those that fail to parse, cannot be built in the environment, or do not have a specific physics-driven functionality, \eg, a statue), we obtain 9,984 machines with their corresponding CoT. A sample gallery is shown in Fig.~\ref{fig:synthesized_dataset}.


We present examples in the curated prompt set: 
\begin{lstlisting}
1. Build a machine that can provide an exciting spinning amusement ride experience.
2. Build a machine that can mimic the movements of a humanoid figure for entertainment or functional demonstrations.
3. Build a machine that can glide smoothly over snow or ice.
\end{lstlisting}

Below we present the text prompts with our simple authoring strategy, which can possibly be scaled with LLMs:
\begin{lstlisting}
-Additional Environment Constraints-
1. On an uneven, bumpy straight road, build a small car that must travel in a straight line to the finish.
2. On a straight road stands a stone wall; build a battering ram that must accelerate straight ahead, smash the wall, and finish with minimal damage to the machine.
-Modified Demands for Target Machines-
1. Build a tall tower that must keep a heavy block (id 36) at 15 m height for 5 s without collapse.
2. On a straight road stands a 10 m high wall; build a siege ladder that must advance, extend its top above the wall, and remain upright throughout.
\end{lstlisting}





\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/synthesised_dataset_v3_cropped.pdf}
  \caption{\footnotesize Examples of Gemini-synthesized machines.}
  \label{fig:synthesized_dataset}
\end{figure}


























\subsection{Cold-Start Details} \label{sec:cold_start_details}

In our experiment, we use Qwen2.5-14B-Instruct as the base model and train it on the Gemini synthesized dataset. To save GPU memory, we employ the parameter-efficient quantized OFT (QOFT) technique~\citep{qiu2025oftv2, qiu2023controlling, liu2024boft, qiu2025reparameterized} for updating the model parameters, with OFT block size 64. We use 8-bit training with the 8-bit AdamW optimizer implmented with bitsandbytes~\citep{dettmers20228bit}, a learning rate of 1e-6 and a linear warmup schedule (3\% of the total training steps).





\subsection{RL Experiment Details}
\label{sec:rl_exp_details}
We use verl framework to implement our RL experiments. The LLM is finetuned from Qwen2.5-14B-Instruct (with LoRA of rank 64 on all linear layers) using the Gemini-synthesized dataset described above. We set learning rate to 5e-6 with gradient clipping threshold set to 0.5. The GRPO advantage estimator uses an advantage clipping ratio of 0.2. We add a KL penalty (weight 0.001) with respect to the pretrained LLM and do not introduce any entropy regularization. For rollouts, we use a temperature of $1.0$ and top-$p$ value of 0.95. Maximum input and output lengths are 3440 and 1168 tokens, respectively. We train each model for 400 update steps which take approximately 48 hours.










\clearpage
\newpage
\section{Additional Ablation Studies}

\textbf{Meta-Designer in hierarchical design.} In Table~\ref{tab:ablation_meta_designer}, we show that how a meta-designer for hierarchical design may benefit compositional machine design. Leveraging the knowledge on existing machines, meta-designers can identify the key macro-level mechanical components that are easier to design compared to the whole task, as shown in the results for Gemini 2.5 Pro, Kimi K2 and Llama 4 Scout. However, introducing an additional stage can introduce compounding error and, if the LLM agent is not capable of integrating different macro-level mechanical components, they may lead to lower scores, which we hypothesize is the reason for the failure of hierarchical design in models like Qwen3. Moreover, we examine if the meta-designer should provide step-by-step building instruction for the designer (Fig.~\ref{fig:detailed-meta-designer}), or simply provide high-level mechanical component descriptions. We find that a meta-designer that provides more detailed information is beneficial mostly when the base model is powerful enough (\eg, Gemini 2.5 Pro).

\textbf{Effect of feedback-free self-critic.} In Table~\ref{tab:ablation_inspector}, we show that the inspector agent which does self-critic before running any environment simulation tend to improve performance for models like Gemini 2.5 Pro (the most powerful model for the task of compositional machine design in \envname) but can fail drastically for models like o3.

\textbf{Effect of active feedback queries.} In Table~\ref{tab:rl_env_feedback}, we show that the active queries on the environment feedbacks help most of the models achieve better performance, compared to the setting with no environment feedback and that with only environment simulation final scores.

\textbf{Additional RL results.} In Fig.~\ref{fig:RL-catapult-max-score} and \ref{fig:RL-catapult-valid-rate} and , we show the maximum scores achieved in the environments with different RL methods plus the validity rate of machines. We visualize the maximum score since, in the case when one is allowed to use inference-time scaling techniques, the best performing machines are the ones people care most about. We show that our settings with Pass@64 training achieves the best maximum score with two different random seeds. In additiona, in Fig.~\ref{fig:RL-catapult-best@n}, we visualize the corresponding Best@N metrics.

For completeness, we also visualize the results with our default setting on the task \textit{car} in Fig.~\ref{fig:RL-car-max-score}, \ref{fig:RL-car-valid-rate} and \ref{fig:RL-car-best@n}.





\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \newcommand{\cgr}[1]{\textcolor[rgb]{.329, .51, .208}{\textbf{#1}}}
  \newcommand{\cre}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}
  \renewcommand{\pm}{\mathbin{\text{Â±}}}

  \begin{tabularx}{\textwidth}{lCCC>{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{1.5cm}}
    \toprule
    \multirow{2}{*}{Models}
    & \multicolumn{3}{c}{Machine Validity (pass/total)}
    & \multicolumn{2}{c}{Designer Score} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-6}
    & File Valid & 3D Valid & Final Valid & Mean & Max \\
    \midrule
    \multicolumn{6}{l}{\textit{Baseline (Meta-Designer \& Designer)}} \\
    \midrule
    Gemini 2.5 Pro    & 8/8 & 5/8 & 5/8 & 8.49 & 9.14 \\
    o3            & 3/8 & 3/3 & 3/8 & 0      & 0 \\
    Qwen3-Coder-480B-A35B    & 8/8 & 6/8 & 6/8 & 0.75 & 4.5 \\
    Doubao Seed 1.6    & 7/8 & 3/7 & 3/8 & 4.34 & 4.37 \\
    Claude Opus 4      & 8/8 & 4/8 & 4/8 & 4.17 & 4.36 \\
    DeepSeek-V3   & 7/8 & 7/7 & 7/8 & 0    & 0 \\
    Kimi K2       & 6/8 & 6/6 & 6/8 & 7.18 & 12.02 \\
    Llama 4 Scout 17B 16E        & 8/8 & 8/8 & 8/8 & 3.59      & 11.83 \\
    \midrule
    \multicolumn{6}{l}{\textit{Single Agent}} \\
    \midrule
    Gemini 2.5 Pro    & 6/8 & 3/6 & 3/8 & 6.13 & 9.00 \\
    o3            & 8/8 & 8/8 & 8/8 & 2.87 & 5.22 \\
    Qwen3-Coder-480B-A35B    & 8/8 & 4/8 & 4/8 & 3.5 & 9.24 \\
    Doubao Seed 1.6    & 7/8 & 6/7 & 6/8 & 4.24 & 8.2 \\
    Claude Opus 4      & 8/8 & 2/6 & 2/8 & 4.76 & 4.91 \\
    DeepSeek-V3   & 7/8 & 6/7 & 6/8 & 4.67 & 4.86 \\
    Kimi K2       & 8/8 & 3/8 & 3/8 & 6.85 & 9.05 \\
    Llama 4 Scout 17B 16E        & 8/8 & 7/8 & 7/8 & 3.63 & 5.64 \\
    \midrule
    \multicolumn{6}{l}{\textit{w/ detailed Meta-Designer \& Designer }} \\
    \midrule
    Gemini 2.5 Pro    & 8/8 & 7/8 & 7/8 & 9.19 &11.94 \\
    o3            & 7/8 & 6/7 & 6/8 & 0.92& 1.18 \\
    Qwen3-Coder-480B-A35B    & 8/8 & 2/8 & 2/8 & 4.87 & 4.87 \\
    Doubao Seed 1.6    & 0/8 & -   & -   & - & - \\
    Claude Opus 4      & 7/8 & 5/7 & 5/8 & 4.13 & 4.79 \\
    DeepSeek-V3   & 8/8 & 8/8 & 8/8 & 6.12 & 9.0 \\
    Kimi K2       & 8/8 & 0/8 & -   & - & - \\
    Llama 4 Scout 17B 16E        & 8/8 & 7/8 & 7/8 & 4.01 & 6.93 \\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Ablation study on the meta-designer. Machine validity is evaluated in two aspects: file validity, 3D validity. Note that 3D validity requires the machine to first pass file validity. Final validity refers to a fully valid machine (satisfying both file and 3D validity). The mean simulation score is calculated based solely on the final valid outputs. \textit{Detailed Meta-Designer} provides more concisely, step-by-step construction guidance to the \textit{Designer}.  Compared to \textit{Baseline}, \textit{Single Agent} is slightly harder to construct valid machines, but the simulation scores are better. \textit{Detailed Meta-Designer} improves both metrics, but requires LLMs to have a strong 3D understanding and a large context window. The comparison between \textit{Meta-Designer} and \textit{Detailed Meta-Designer} is illustrated in Fig.~\ref{fig:detailed-meta-designer}.}
  \label{tab:ablation_meta_designer}
\end{table}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/designer_detailed_designer_cropped.pdf}
  \caption{\footnotesize Construction guidance comparison of \textit{Meta Designer} and \textit{Detailed Meta Designer}, sampled with Gemini 2.5 Pro.}
  \label{fig:detailed-meta-designer}
\end{figure}

\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \newcommand{\cgr}[1]{\textcolor[rgb]{.329, .51, .208}{\textbf{#1}}}
  \newcommand{\cre}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}

  \begin{tabularx}{\textwidth}{l*{6}{>{\centering\arraybackslash}X}}
    \toprule
    \multicolumn{1}{c}{\multirow{2.4}{*}{Models}}
    & \multicolumn{6}{c}{Blind Refinement Simulation Scores} \\
    \cmidrule(lr){2-7}
    & \multicolumn{3}{c}{Baseline}
    & \multicolumn{3}{c}{w/o Inspector} \\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}
    & Valid & Mean & Max & Valid & Mean & Max \\
    \midrule
    Gemini 2.5 Pro
       & 5 & \bf 8.18 & \bf 11.07
       & 5 & 5.67 & 9.37 \\
    o3
       & 3 &0 & 0
       & 3 &\bf 3.08  & \bf 9.24 \\
    Qwen3-Coder-480B-A35B
       & 6 & \bf 1.61 & \bf 5.0
       & 6 & 0.75 & 4.51\\
    Doubao Seed 1.6
       & 3 & 0.31 & \bf 0.49
       & 3 & \bf 0.47 &1.41  \\
    Claude Opus 4
       & 2 &\bf 5.38  &5.8 
       & 4 &5.20  & \bf 8.25  \\
    DeepSeek-V3
       & 7 & \bf 0.98 & \bf 3.18
       & 7 &0.38  &2.16  \\
    Kimi K2
       & 2 & \bf 5.29 &7.36 
       & 6 & 2.31 & \bf 8.91 \\
    Llama 4 Scout 17B 16E
       & 0 & - & -
       & 0 & - & - \\
    \bottomrule
  \end{tabularx}
    \caption{\footnotesize Ablation study on inspector agentic design. The mean simulation score is calculated based solely on the valid machines after blind refinement. Removing the inspector from the agentic flow lowers the blind refinerâs mean performance on LLMs with weaker 3D understanding, while barely affecting other models.}
  \label{tab:ablation_inspector}
\end{table}


\begin{table}[h!]
  \centering
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \newcommand{\cgr}[1]{\textcolor[rgb]{.329, .51, .208}{\textbf{#1}}}
  \newcommand{\cre}[1]{\textcolor[rgb]{1, 0, 0}{\textbf{#1}}}

  \begin{tabularx}{\textwidth}{l*{9}{>{\centering\arraybackslash}X}}
    \toprule
    \multicolumn{1}{c}{\multirow{2.4}{*}{Models}}
    & \multicolumn{9}{c}{Refiner Simulation Scores} \\
    \cmidrule(lr){2-10}
    & \multicolumn{3}{c}{Baseline}
    & \multicolumn{3}{c}{w/o Env Querier}
    & \multicolumn{3}{c}{Score Only}\\
    \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}
    & std & Mean & Max & std & Mean & Max& std & Mean & Max \\
    \midrule
    Gemini 2.5 Pro
       & 2.47 &\bf 15.73  &18.19 
       & 4.18 &14.89  &\bf 19.77
       & 2.05 &9.68  &13.18\\
    o3
       & 5.36 &5.34  & \bf 11.11
       & 4.24 &4.06  &8.55
       & 2.76 &\bf 7.05  &10.24\\
    Qwen3-Coder-480B-A35B
       & 0.95 & \bf 5.21 & \bf 6.52
       & 2.32 &4.05 &6.89 
       & 2.53 &2.81 &5.56\\
    Claude Opus 4
       & 0.31 &\bf 9.10 & \bf 9.32
       & 0.82 &8.50  &9.08
       & 0.42 &5.75  &6.05\\
    Doubao Seed 1.6
       & 0.23 &4.62  & 4.76
       & 0.08 &\bf 4.89 &4.94
       & 0.29 &4.79 &\bf 5.05\\
    DeepSeek-V3
       & 0.09 &\bf 4.82  & 4.93
       & 1.96 &4.37  &\bf 6.00
       & 1.91 &2.76  &5.15\\
    \bottomrule
  \end{tabularx}
  \caption{\footnotesize Ablation study on the environment querier agent. For the refiner, the baseline includes simulation scores, basic environment feedback, and querier-required feedback. The "w/o env querier" setting provides only simulation scores and basic environment feedback. In the "pure score only" setting, only simulation scores are provided. Removing the environment querier causes a slight drop in average machine performance. With reward signals only, the performance markedly degrades across most LLMs.}
  \label{tab:rl_env_feedback}
\end{table}




    

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/RL_Metrics/Catapult/score_max.png}
  \caption{\footnotesize \textit{Catapult} task machine scores across RL steps. KL regularization helps the model discover better structure designs. Pass@64 is greatly more efficient at uncovering powerful machine designs. Pass@8 (roll-out 8) outperforms Pass@1 (roll-out 64) in efficiency and matches its performance with fewer roll-outs. No cold start models lack the advanced knowledge needed to find better machines.} \label{fig:RL-catapult-max-score}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=.5\linewidth]{figures/RL_Metrics/Car/score_max.png}
    \caption{\footnotesize \textit{Car} machine scores across RL steps. The RL finetuning hyperparameter setting is the same as the base hyperparameter setting of \textit{Catapult}. Machine performance slightly rises as training steps increase.}
    \label{fig:RL-car-max-score}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/RL_Metrics/Catapult/MachineValid_Reward.png}
  \caption{\footnotesize \textit{Catapult task} machine validity rate and reward non-zero rate across RL steps. The machine validity rate refers to the proportion of machines that can successfully run simulations. The reward non-zero rate represents the ratio of machines that can simulate with a non-zero reward. LLM constructs more legal machines as training steps increase, and rewards non-zero machines. Pass@8 and Pass@1 converge early. ``No KL'' fills roll-outs with failure cases, slowing performance gains. ``No cold start'' lacks design knowledge, encounters more failures than no KL, and improves validity rate most slowly. The base setting balances convergence and performance improvement.}
  \label{fig:RL-catapult-valid-rate}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/RL_Metrics/Car/MachineValid_Reward.png}
  \caption{\footnotesize \textit{Car task} machine validity rate and reward non-zero rate across RL steps. The machine validity converges early and remains stable during further training.} \label{fig:RL-car-valid-rate}
\end{figure}


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/RL_Metrics/Catapult/Val_Best.png}
  \caption{\footnotesize \textit{Catapult task}. Average Best@N metric. At each test step, the LLM generates 64 samples, selects the top $N$ samples, and records the maximum score. This process is repeated 1,000 times, and the mean value is calculated. Base settings (both seeds) dominates Best@N performance; excluding base settings, ``no KL'' dominates the rest. Pass@1 and Pass@8 spawn only a handful of high-performance machines. No cold start produces machines of more average quality.}
  \label{fig:RL-catapult-best@n}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/RL_Metrics/Car/Val_Best.png}
  \caption{\footnotesize \textit{Car task}. Mean Best@N metrics. Similar to the machine validity rate, the Best@N performance increases quickly and remains stable in rest training periods.}\label{fig:RL-car-best@n}
\end{figure}


\clearpage
\newpage
\section{Generated Samples}

\subsection{From RL-finetuned Models}
Here, we present some of the best RL samples from rollouts, as well as examples from the agentic workflow. Fig.~\ref{fig:RL_best_sample} displays the RL rollout samples, while Fig.~\ref{fig:test_time_scaling_gallery} illustrates the agentic workflow samples. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/RL_sample_new_v2.pdf}
  \caption{\footnotesize Qwen2.5-14B-Instruct cold started RL model \textit{catapult} task sample from roll-out. Throwing distances are labeled on the bottom-right corner of the image.}
  \label{fig:RL_best_sample}
\end{figure}

\newpage
\subsection{From Agentic Workflow}
\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/test_time_scaling_gallery_withcaption.pdf}
  \caption{\footnotesize The LLM inference gallery of machine-generated samples. The rows, from top to bottom, were inferred by the following models, respectively: Claude 4 Opus, Gemini 2.5 Pro, o3, Doubao Seed 1.6, and Qwen3-Coder-480B-A35B-Instruct. Throwing distances are labeled on the bottom-right corner of the image.}
  \label{fig:test_time_scaling_gallery}
\end{figure}


\clearpage
\newpage
\section{Relations between CoT and Machines}



To further investigate if high-level machine blueprint or low-level part placement is more important, 
we experiment with machine generation of LLMs by generating machine details conditioned on Gemini-generated CoT (instead of on CoT produced by themselves). 
We find that with Gemini CoT, almost all LLMs design machines that are more visually similar to "catapults", as shown in Fig.~\ref{fig:LLM-feed-gemini-cot}. We therefore hypothesize that the major gap between other LLMs, especially open-source ones, and Gemini 2.5 Pro is the abstract-level spatial and physics reasoning on machine designs.


\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{figures/LLM_feed_gemini_cot_v4_cropped.pdf}
  \caption{\footnotesize Comparison between generated machines conditioned on their own CoT or Gemini-generated CoT.}
  \label{fig:LLM-feed-gemini-cot}
\end{figure}

\clearpage
\newpage
\section{CoT Samples from Gemini 2.5 Pro}
\input{Gemini_CoT_appendix}

\input{LLM_prompt_appendix}

