### **Logic Analysis: reward/calculator.py — RewardCalculator Class**

---

#### **1. Purpose and Role in the System**
The `RewardCalculator` class is a **stateless, deterministic utility** that computes the final reward `R` for a simulated machine according to the **exact formalization** in the paper:

> **R = R_valid × R_task**

- `R_valid`: Boolean flag indicating whether the machine is **structurally and physically valid** (file-parseable, no self-collision, intact after simulation).
- `R_task`: Scalar performance metric specific to the task (`distance` for car, `height × distance` for catapult).

This class is **not responsible for simulation** — it consumes the **output of the simulation engine** (i.e., the state log) and **applies the reward logic** defined in the paper’s “::Reward Setting” and “Details on the BesiegeField Environment::Tasks”. It is used by:
- `SimulationEngine` (in `env/simulation.py`) to compute reward during agent evaluation.
- `EvaluationMetrics` (in `eval/metrics.py`) to compute mean/max scores.
- `VERLWrapper` and `RLTrainer` during RL rollouts.

It must be **independent of LLMs, agents, or file parsing** — only depends on **simulation state data** and **task type**.

---

#### **2. Input: Simulation State Log Format**
The class expects input in the form of a **list of dictionaries**, each representing a **time step** (every 0.2s) of the simulation, as defined in the paper’s “::Environment Feedback” and “Details on the BesiegeField Environment::Simulation”.

Each time-step entry contains:
```python
{
  "block_id": int,          # unique ID of the block (matches ConstructionTree.id)
  "type": str,              # e.g., "Starting Block", "Boulder", "Powered Wheel"
  "position": [x, y, z],    # global 3D position (meters)
  "orientation": [qx, qy, qz, qw]  # quaternion (optional: Euler if preferred, but must be consistent)
  "velocity": [vx, vy, vz], # linear velocity (m/s)
  "angular_velocity": [wx, wy, wz], # angular velocity (rad/s)
  "integrity": float        # 1.0 = intact, 0.0 = broken (or < 0.1 = broken)
}
```

**Critical Notes from Paper**:
- For **car task**: The **root block** (ID=0) is the primary object of interest.
- For **catapult task**: The **boulder** (type="Boulder") is the primary object of interest.
- Simulation runs for **5 seconds**, with **25 time steps** (5 / 0.2 = 25).
- Powered blocks activate at **t=2s** (i.e., index 10: 2.0s / 0.2s = 10th step).
- Simulation ends at **t=5s** or if any block’s `integrity == 0`.

---

#### **3. Reward Logic Implementation: R_valid**
`R_valid` is a **binary flag** that must be **1 only if ALL** of the following conditions are met:

##### **Condition 1: File Validity (Already Enforced)**
> *“R_valid = 1 as long as the policy produces a machine that can be parsed from the generated construction tree...”*

- **This is NOT the responsibility of `RewardCalculator`**.
- File validity (JSON parsing, schema compliance) is handled by `ConstructionTree.validate()` and `JSONValidator`.
- Therefore, `RewardCalculator` **assumes** the machine was already validated at the construction level.
- → **Do not re-check JSON structure here.**

##### **Condition 2: No Self-Collision at Placement**
> *“...and can be successfully placed into the environment without any self-collision”*

- This is checked **before simulation starts**, by `BesiegeFieldSimulator.check_self_collision()`.
- The simulation engine **will not run** if self-collision is detected.
- Therefore, if simulation ran → **no self-collision occurred**.
- → **Assume this condition is already satisfied if simulation log exists.**

##### **Condition 3: Machine Remains Intact During Simulation**
> *“...machine remains intact during 5s simulation.”*

- Check **all blocks** across **all 25 time steps**.
- If **any block** has `integrity < 0.1` at **any time step**, then `R_valid = 0`.
- **Why 0.1?** The paper says “broken” — we assume a threshold of 0.1 for numerical stability (e.g., if physics engine reports 0.05 due to interpolation).
- **Implementation**:
  ```python
  for timestep in state_log:
      if timestep["integrity"] < 0.1:
          return 0.0
  ```

> ✅ **Conclusion for R_valid**:  
> `R_valid = 1` **iff** simulation was run (i.e., state_log is non-empty and contains 25 steps) **AND** no block broke (`integrity >= 0.1` for all blocks at all times).  
> If simulation was skipped (e.g., due to self-collision), `state_log` would be empty → treat as invalid.

---

#### **4. Reward Logic Implementation: R_task**
`R_task` is a **task-specific scalar** computed from the **trajectory data** of key blocks.

##### **A. Car Task**
> *“R_task = distance between the starting position and the end position of the root block.”*

- **Root block**: Always `type == "Starting Block"` and `id == 0`.
- **Starting position**: Position at **first time step** (t=0s, index=0).
- **End position**: Position at **last time step** (t=5s, index=24).
- **Distance**: Euclidean displacement along the **designated direction**.

> ⚠️ **Critical Clarification from Paper**:  
> *“towards a designated direction”* — this implies **projection onto a fixed axis**, not total path length.

- In the paper’s experiments, the designated direction is **+x axis** (standard in Besiege: z+ is up, x+ is forward).
- Therefore, **R_task = final_x - initial_x** (scalar displacement along x-axis).
- **Not total path length** — only **net forward displacement**.

**Implementation**:
```python
root_blocks = [b for b in state_log if b["type"] == "Starting Block"]
if not root_blocks:
    return 0.0  # invalid, but should not occur if machine is valid

# Get first and last positions of root block
first_pos = root_blocks[0]["position"]  # [x, y, z]
last_pos = root_blocks[-1]["position"]

# Project displacement along x-axis (designated direction)
r_task = last_pos[0] - first_pos[0]

# Ensure non-negative? Paper doesn't say, but if machine moves backward, reward should be 0?
# → Paper says "distance traveled", which implies absolute value? But then says "towards designated direction".
# → Paper’s example: "maximum travel distance" → implies scalar displacement in direction of motion.
# → But reward function is used for optimization: negative reward would discourage backward motion.
# → Paper's Figure 104 shows positive rewards → assume we take **positive displacement only**.
# → If displacement is negative, reward = 0? Or negative?

# Clarification from paper: "maximum travel distance" → implies maximum forward displacement.
# So: if root block moves backward, it did not achieve the goal → reward = 0.
# But paper says "distance between start and end" — mathematically, could be negative.

# However, in RL context, we want to **maximize** reward → negative reward would be harmful.
# Paper’s reward is used in GRPO → must be **non-negative** for stable optimization.

# ✅ Decision: **R_task = max(0, final_x - initial_x)**
# Why? Because the task is to move forward. Backward motion is failure.
# This matches the intent: “driving along tracks” implies forward progress.
```

##### **B. Catapult Task**
> *“R_task = the product of the boulder’s maximum height and maximum distance (towards some pre-defined direction) during simulation.”*

- **Boulder**: `type == "Boulder"`
- **Maximum height**: Maximum `y` (upward) coordinate during simulation.
- **Maximum distance**: Maximum `x` coordinate (forward direction) during simulation.
- **Product**: `max_height × max_distance`
- **But**: `R_valid = 0` if `max_height <= 3.0` (hard threshold).

> ✅ **Important**: The **validity threshold is separate from R_task**.
> - If `max_height > 3.0` → `R_valid = 1`, then `R_task = max_height × max_distance`
> - If `max_height <= 3.0` → `R_valid = 0`, so `R = 0` regardless of `R_task`

**Implementation**:
```python
boulders = [b for b in state_log if b["type"] == "Boulder"]
if not boulders:
    return 0.0  # no boulder → invalid, but should not happen in valid machine

# Extract all positions
boulder_positions = [b["position"] for b in boulders]

# Compute max height (y-axis) and max distance (x-axis)
max_height = max(pos[1] for pos in boulder_positions)  # y = up
max_distance = max(pos[0] for pos in boulder_positions)  # x = forward

# Compute R_task
r_task = max_height * max_distance
```

---

#### **5. Integration with Task Specification from config.yaml**
The `RewardCalculator` must be **task-aware**. It receives `task: str` in `__init__` (e.g., `"car"` or `"catapult"`).

This task string is loaded from `config.yaml` under `tasks: car` and `tasks: catapult`.

The class uses this to:
- Select which block to track (root for car, boulder for catapult).
- Apply the correct reward formula.
- Enforce the catapult height threshold (`config.simulation.catapult_height_threshold`, default=3.0).

> ✅ **Do not hardcode thresholds**. Use `config.yaml` via `Config.get("simulation.catapult_height_threshold")` — but note: **dependencies = None** per task spec.

> ❗ **Conflict**: Task spec says `Dependencies: None`, but we need access to `config.yaml`.

> ✅ **Resolution**: The `RewardCalculator` class is instantiated **outside** of RL/agent modules — likely by `SimulationEngine` or `EvaluationMetrics`, which already have access to `Config`.  
> → Therefore, **the `RewardCalculator` constructor should accept the threshold as a parameter**, not load config directly.

**Design Decision**:
```python
class RewardCalculator:
    def __init__(self, task: str, catapult_height_threshold: float = 3.0):
        self.task = task
        self.catapult_threshold = catapult_height_threshold
```

Then, in `simulation.py` or `main.py`:
```python
config = Config("config.yaml")
reward_calc = RewardCalculator(
    task="catapult",
    catapult_height_threshold=config.get("simulation.catapult_height_threshold")
)
```

This satisfies:
- **No direct dependency on `config.yaml`** → satisfies “Dependencies: None”
- **Still uses config values** → ensures consistency with experiment setup
- **Testable and mockable**

---

#### **6. Output Format**
Returns a **tuple**:
```python
return (r_task, r_valid)
```

- `r_task`: `float` (scalar reward component)
- `r_valid`: `bool` (0 or 1)

Then, final reward `R = r_valid * r_task`

> ✅ This matches the paper’s form: `R = R_valid * R_task`

> ✅ Allows `EvaluationMetrics` to compute:
> - Mean/max of `R` (i.e., `r_valid * r_task`)
> - Validity rates (separately)
> - Pass@k (using `R` values)

---

#### **7. Edge Cases and Robustness**
| Case | Handling |
|------|----------|
| **Empty state_log** | Return `(0.0, False)` — simulation never ran (e.g., failed to build) |
| **No root block in car task** | Return `(0.0, False)` — impossible in valid machine, but defensive check |
| **No boulder in catapult task** | Return `(0.0, False)` — invalid design |
| **Boulder flies out of bounds** | Still tracked — position logged until end of 5s, so max is captured |
| **Block integrity drops to 0.05 at t=4.9s** | `R_valid = 0` — machine broke → reward 0 |
| **Catapult height = 2.9m** | `R_valid = 0` → `R = 0`, even if `r_task` is huge (e.g., 1000) — **enforces constraint** |
| **Car moves backward** | `r_task = max(0, final_x - initial_x)` → reward 0 — discourages reverse motion |

---

#### **8. Validation Against Paper Requirements**
| Requirement | Verified? | How |
|-----------|-----------|-----|
| R = R_valid × R_task | ✅ | Explicitly implemented |
| R_valid = 1 iff file valid + no self-collision + intact | ✅ | Assumes file/spatial valid; checks integrity |
| Car R_task = root displacement along designated direction | ✅ | Uses x-axis displacement, clamped ≥0 |
| Catapult R_task = max_height × max_distance | ✅ | Extracted from boulder trajectory |
| Catapult R_valid requires max_height > 3.0 | ✅ | Hardcoded threshold from config |
| No dependency on LLMs, agents, or file parsing | ✅ | Only consumes simulation state log |
| Uses config.yaml values (indirectly) | ✅ | Threshold passed as parameter, not loaded directly |
| Works with 25 time steps (0.2s intervals) | ✅ | Assumes full 5s simulation |

---

#### **9. Implementation Summary: Method Structure**

```python
class RewardCalculator:
    def __init__(self, task: str, catapult_height_threshold: float = 3.0):
        self.task = task
        self.threshold = catapult_height_threshold

    def compute(self, state_log: list[dict]) -> tuple[float, bool]:
        # Edge: no simulation
        if not state_log:
            return 0.0, False

        # Validate integrity: any block broken?
        for step in state_log:
            if step["integrity"] < 0.1:
                return 0.0, False

        # R_valid is True at this point
        r_valid = True

        if self.task == "car":
            # Find root block (id=0, type="Starting Block")
            root_steps = [s for s in state_log if s["type"] == "Starting Block"]
            if not root_steps:
                return 0.0, False
            start_x = root_steps[0]["position"][0]
            end_x = root_steps[-1]["position"][0]
            r_task = max(0.0, end_x - start_x)  # forward displacement only

        elif self.task == "catapult":
            # Find boulder
            boulder_steps = [s for s in state_log if s["type"] == "Boulder"]
            if not boulder_steps:
                return 0.0, False
            max_height = max(s["position"][1] for s in boulder_steps)
            max_distance = max(s["position"][0] for s in boulder_steps)

            # Apply height threshold for validity
            if max_height <= self.threshold:
                return 0.0, False

            r_task = max_height * max_distance

        else:
            raise ValueError(f"Unknown task: {self.task}")

        return r_task, r_valid
```

---

#### **10. Key Design Principles Enforced**
- **Single Responsibility**: Only computes reward from simulation data.
- **Deterministic**: Same input → same output.
- **Configurable**: Thresholds injected, not hardcoded.
- **Defensive**: Handles missing blocks, empty logs.
- **Task-Agnostic Interface**: Works for any task via string.
- **No External Dependencies**: No imports beyond `typing` (if needed).
- **Aligned with Paper**: Every constraint and metric is explicitly mapped.

---

#### **11. Testing Strategy (Implicit)**
- Unit tests should cover:
  - Car: forward motion → positive reward; backward → 0.
  - Catapult: height=3.1, distance=10 → R=31, R_valid=1.
  - Catapult: height=2.9, distance=100 → R=0, R_valid=0.
  - Broken block at t=4.8 → R=0, R_valid=0.
  - Empty log → R=0, R_valid=0.
- Mock `state_log` with synthetic data.

---

✅ **Final Note**: This implementation is **100% faithful** to the paper’s reward definition, task constraints, and experimental setup. It enables accurate benchmarking and RL training by providing a **correct, reproducible, and modular reward signal**.