### **Logic Analysis: eval/metrics.py — EvaluationMetrics Class**

The `EvaluationMetrics` class is the central evaluation module responsible for quantitatively assessing the performance of all agent-generated machine designs across the three core dimensions: **structural validity**, **physical feasibility**, and **functional reward**. It serves as the final arbiter of success in the compositional machine design task and is invoked during both **agent benchmarking** and **RL evaluation (Pass@k)**. Its design must be **stateless**, **deterministic**, and **fully aligned** with the paper’s defined metrics and the `config.yaml` specifications.

---

#### **1. Core Responsibilities and Inputs**

The class receives a list of `ConstructionTree` objects (each representing a generated machine) and their corresponding task types (`"car"` or `"catapult"`). It does **not** generate or modify designs — it only **evaluates** them.

- **Inputs**:
  - `designs: list[ConstructionTree]` — Generated machine structures from any agent (single-agent, iterative, hierarchical, RL rollout).
  - `tasks: list[str]` — Task type for each design (must match length of `designs`). Each element is either `"car"` or `"catapult"`.
- **Outputs**:
  - A dictionary containing all 6 metrics:
    ```python
    {
      "file_validity_rate": float,
      "spatial_validity_rate": float,
      "machine_validity_rate": float,
      "mean_simulation_score": float,
      "max_simulation_score": float,
      "pass_at_k": dict  # { "k=1": float, "k=8": float, "k=64": float }
    }
    ```

---

#### **2. Metric Computation Logic (Step-by-Step)**

##### **(a) File Validity Rate**
- **Definition**: Proportion of `ConstructionTree` objects that are syntactically and structurally valid JSON.
- **Implementation**:
  - For each `tree` in `designs`, call `tree.validate()` (from `representation/construction_tree.py`).
  - `tree.validate()` returns `(is_valid: bool, error_msg: str)`.
  - Count valid trees → `valid_file_count`.
  - `file_validity_rate = valid_file_count / len(designs)`
- **Dependency**: `ConstructionTree.validate()` must enforce:
  - All `"id"` are unique and sequential from 0.
  - `"parent"` is null for ID=0 (root), and for others, must be a valid existing ID < current ID.
  - `"parent_a"` and `"parent_b"` only appear for `"Spring"` (per `BlockRegistry.is_special_block()`).
  - No cycles (via DFS or parent-tracing from root).
  - `"face_id"` is integer ∈ [0,5] for standard blocks; for Spring, `"face_id_a"` and `"face_id_b"` must be valid.
- **Note**: Invalid JSON parsing (e.g., malformed string) should be caught **before** `ConstructionTree` instantiation — if `ConstructionTree` constructor raises an exception, the design is invalid.

##### **(b) Spatial Validity Rate**
- **Definition**: Proportion of **file-valid** machines that have **no self-collisions** at placement (before simulation).
- **Implementation**:
  - For each **file-valid** `tree`, instantiate a `BesiegeFieldSimulator` (from `env/besiegefield.py`).
  - Call `simulator.build_from_tree(tree)` → returns `bool` indicating if placement succeeded.
  - If `build_from_tree()` returns `False`, or if `simulator.check_self_collision()` returns `True`, mark as spatially invalid.
  - Count spatially valid → `valid_spatial_count`.
  - `spatial_validity_rate = valid_spatial_count / len(designs)`
- **Dependency**: `BesiegeFieldSimulator.build_from_tree()` must:
  - Place blocks in order of `"id"` using `BlockRegistry` for block dimensions and collision volumes.
  - Use `collision_threshold` from `config.yaml` (default: 0.01m) to detect overlaps.
  - Return `False` if any block overlaps with another (even partially).
- **Important**: This step **must not** run simulation — only placement and static collision check.

##### **(c) Machine Validity Rate**
- **Definition**: Proportion of machines that are **both** file-valid and spatially valid.
- **Implementation**:
  - `machine_validity_rate = count_of_machines_valid_in_both / len(designs)`
  - Can be computed as:  
    `valid_machine_count = sum(1 for t in designs if tree.validate()[0] and simulator.build_from_tree(t) and not simulator.check_self_collision())`
- **Note**: This metric is **not** a product of the two above — it’s the intersection. A machine must pass both checks to be counted.

##### **(d) Mean and Maximum Simulation Scores**
- **Definition**: Average and maximum of the **reward** `R = R_valid * R_task` computed over **all machine-valid** designs.
- **Implementation**:
  - For each **machine-valid** `tree` (i.e., passed file + spatial checks):
    - Initialize `SimulationEngine` (from `env/simulation.py`) with `task` (from `tasks` list).
    - Call `simulation_engine.simulate(tree)` → returns `state_log: list[dict]` (per 0.2s).
    - Pass `state_log` to `RewardCalculator.compute(state_log, task)` → returns `(reward: float, is_valid: bool)`.
      - `is_valid` here is **redundant** (we already know machine is valid), but used for double-checking.
      - `reward` is `R_task` if `R_valid=True`, else `0.0` (though machine validity already filters this).
  - Collect all non-zero rewards → `valid_rewards`.
  - `mean_simulation_score = mean(valid_rewards)`
  - `max_simulation_score = max(valid_rewards)` (if `valid_rewards` empty → 0.0)
- **Reward Logic (from `reward/calculator.py`)**:
  - **Car**:
    - `R_valid = 1` if:  
      (i) no self-collision (already ensured),  
      (ii) machine intact after 5s (i.e., all blocks have `integrity == 1` at final timestep).  
    - `R_task = displacement of root block along +x direction` (max displacement over 5s).
  - **Catapult**:
    - `R_valid = 1` if:  
      (i) above car conditions,  
      (ii) **boulder’s max height > 3.0m** (from `config.yaml: catapult_height_threshold`).  
    - `R_task = max_height × max_distance` (boulder’s horizontal displacement along designated direction).
  - **Critical**: The `state_log` must contain per-block position and integrity at each 0.2s interval. Must extract:
    - Root block position for car.
    - Boulder position (identified by `"type": "Boulder"`) for catapult.
- **Note**: Only machine-valid designs are scored. Invalid designs contribute 0.0 to mean/max, but are **not counted** in the numerator — they are filtered out before scoring.

##### **(e) Pass@k Scores (k=1, 8, 64)**
- **Definition**: Maximum reward achievable among the top-k generated designs **per prompt**. Used for RL evaluation.
- **Implementation**:
  - **Assumption**: The `designs` and `tasks` lists are grouped by prompt. That is, for each test prompt, exactly `k` rollouts are generated (e.g., 64 rollouts per prompt).
  - **Input Constraint**: `len(designs)` must be divisible by `k` (e.g., 640 designs for 100 prompts × 64 rollouts).
  - **Steps**:
    1. Group `designs` and `tasks` into chunks of size `k` → `prompt_groups = [designs[i:i+k] for i in range(0, len(designs), k)]`
    2. For each group (i.e., one prompt):
       - Compute the **reward** for each design in the group using `RewardCalculator` (as above).
       - Take the **maximum reward** in the group → `best_score_for_prompt`.
    3. Compute:
       - `pass_at_k[k] = mean(best_score_for_prompt for all prompts)`
  - **Example**:
    - `k=64`: 100 prompts → 6400 designs → 100 groups of 64 → 100 best scores → mean of those 100 = `pass_at_64`.
    - `k=8`: same 6400 designs, but group into chunks of 8 → 800 groups → mean of 800 best scores.
    - `k=1`: group into chunks of 1 → 6400 groups → mean of 6400 scores (equivalent to mean_simulation_score).
- **Note**: 
  - `pass_at_1` is mathematically identical to `mean_simulation_score` **only if** each prompt has exactly one design. But in RL, we have multiple rollouts per prompt, so `pass_at_1` = mean of the **single best** per prompt.
  - The paper uses `Pass@k` to measure the **best possible outcome** under inference-time scaling — this is critical for evaluating RL’s ability to discover high-reward designs.
- **Dependency**: Must know `k` values from `config.yaml`: `pass_k` (default=64). Also compute for `k=8` and `k=1` as per ablation.

---

#### **3. Integration with Other Modules**

| Module | Interaction | Purpose |
|--------|-------------|---------|
| `ConstructionTree` | `tree.validate()` | Validates JSON structure and topology. |
| `BlockRegistry` | Indirect via `ConstructionTree` | Ensures block types are known and special rules (Spring) are respected. |
| `BesiegeFieldSimulator` | `build_from_tree()` + `check_self_collision()` | Determines spatial validity. |
| `SimulationEngine` | `simulate()` | Runs physics and returns state log. |
| `RewardCalculator` | `compute(state_log, task)` | Computes final reward `R` based on task-specific logic. |
| `config.yaml` | `simulation.duration_seconds`, `catapult_height_threshold`, `agent.pass_k` | Provides thresholds and k-values. Must be loaded once at class init. |

---

#### **4. Error Handling and Robustness**

- **Invalid Design**: If a `ConstructionTree` fails `validate()`, skip to next — do not crash.
- **Missing Block**: If a block type in `tree` is not in `BlockRegistry`, treat as invalid.
- **Simulation Crash**: If `simulate()` throws an exception (e.g., Unity crash), log error and treat as `reward=0.0`, but **do not count** in mean/max (only valid simulations are scored).
- **Empty Valid Set**: If no designs are valid, return `0.0` for all scores (mean, max, pass@k).
- **Logging**: Use `Logger` to record:
  - Number of invalid files
  - Number of spatial collisions
  - Number of simulation failures
  - Per-prompt best scores (for debugging)

---

#### **5. Performance and Scalability**

- **Batch Processing**: Must use `ParallelSimulator` (from `utils/parallel_sim.py`) when evaluating large batches (e.g., 6400 rollouts).
  - `ParallelSimulator.simulate_batch(trees)` → returns list of `state_log` dicts.
  - Avoid sequential simulation — critical for RL evaluation speed.
- **Memory**: `state_log` for 5s at 0.2s intervals = 25 steps per block. With 20 blocks → 500 state entries per machine. For 6400 machines → 3.2M entries — manageable with NumPy arrays.
- **Parallelization**: `compute_all()` must internally use `ParallelSimulator` for simulation steps, but **not** for validation (validation is lightweight).

---

#### **6. Alignment with Paper and Config**

| Paper Requirement | Implementation Match |
|-------------------|----------------------|
| File validity rate | ✅ `tree.validate()` + count |
| Spatial validity rate | ✅ `build_from_tree()` + `check_self_collision()` |
| Machine validity rate | ✅ Intersection of above |
| Mean/max simulation score | ✅ `RewardCalculator.compute()` on machine-valid designs |
| Pass@k | ✅ Grouped by prompt, max per group, mean over prompts — k=1,8,64 from config |
| Catapult height threshold | ✅ `config.yaml: catapult_height_threshold: 3.0` |
| Simulation duration | ✅ `config.yaml: duration_seconds: 5` |
| State log interval | ✅ `config.yaml: state_log_interval: 0.2` |
| Reward function | ✅ Car: distance; Catapult: height×distance with >3m constraint |

---

#### **7. Output Format and Usage in main.py**

The `compute_all()` method returns a **flat dictionary** suitable for:
- Logging to file (`logs/experiment.log`)
- Writing to CSV/JSON results (`results/eval_results.json`)
- Plotting in `eval/visualizer.py`
- Comparing agent types (e.g., “Hierarchical vs Iterative”)

Example output:
```json
{
  "file_validity_rate": 0.82,
  "spatial_validity_rate": 0.76,
  "machine_validity_rate": 0.70,
  "mean_simulation_score": 124.5,
  "max_simulation_score": 489.2,
  "pass_at_k": {
    "k=1": 215.8,
    "k=8": 342.1,
    "k=64": 489.2
  }
}
```

This structure allows direct comparison with the paper’s **Table TABREF15** and **Fig. FIGREF104**.

---

#### **8. Critical Assumptions and Clarifications**

| Assumption | Justification |
|-----------|---------------|
| **All designs are generated for known tasks** | The `tasks` list is provided externally — we assume correct mapping. |
| **Boulder is uniquely identifiable** | By `"type": "Boulder"` — confirmed in `BlockRegistry`. |
| **Root block is always ID=0** | Enforced by `ConstructionTree` constructor. |
| **Displacement direction is +x** | Paper says “towards a fixed and given direction”; assume +x axis (standard in Unity/Besiege). |
| **No post-construction scaling/rotation** | Enforced by paper — so all positions are from attachment logic only. |
| **Pass@k rollouts are grouped by prompt** | Required by RLVR framework — must be enforced in dataset loader. |

---

#### **9. Final Design Notes**

- **No State**: `EvaluationMetrics` is stateless. Initialize once, call `compute_all()` multiple times.
- **Modular**: Does not instantiate simulators or LLMs — only uses interfaces.
- **Reproducible**: Uses only `config.yaml` for thresholds and k-values — no hardcoding.
- **Extensible**: Easy to add new tasks (e.g., “delivery”) by extending `RewardCalculator` and adding to `config.yaml`.

---

### **Conclusion**

The `EvaluationMetrics` class is the **quantitative backbone** of the entire system. It must be **bulletproof**, **efficient**, and **faithfully aligned** with the paper’s metrics. Its logic is deterministic and relies entirely on the validated interfaces of `ConstructionTree`, `SimulationEngine`, and `RewardCalculator`. By centralizing all evaluation logic here, we ensure that benchmarking results are **consistent**, **reproducible**, and **directly comparable** to the findings in “Agentic Design of Compositional Machines”. This module will be invoked in `main.py` for every agent type and RL variant — its correctness is non-negotiable.