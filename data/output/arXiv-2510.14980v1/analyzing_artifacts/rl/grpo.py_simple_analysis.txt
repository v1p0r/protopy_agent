# Logic Analysis: rl/grpo.py

The `GRPO` class in `rl/grpo.py` implements **Group Relative Policy Optimization**, a variant of policy optimization designed for large language models (LLMs) under the **Reinforcement Learning with Verifiable Rewards (RLVR)** paradigm, as described in the paper. This module is a core component of the RL finetuning pipeline and must be implemented to align precisely with the experimental setup in the paper, particularly the use of `verl` framework, LoRA parameter efficiency, and the Pass@k advantage estimator.

---

## **1. Core Objective and Context**

The GRPO algorithm is used to **finetune Qwen2.5-14B-Instruct** on the curated cold-start dataset using **verifiable rewards** from BesiegeField simulations. The goal is to improve the LLM’s ability to generate valid, high-performing construction trees for car and catapult tasks by optimizing its policy (i.e., output distribution over token sequences) based on **reward feedback** from physical simulation.

Key constraints from the paper:
- **No entropy regularization** is applied.
- **KL penalty** is used to prevent deviation from the pretrained model (weight = 0.001).
- **Gradient clipping** is applied at 0.5.
- **LoRA** is used with rank=64 on all linear layers.
- **8-bit AdamW optimizer** via `bitsandbytes` is used for memory efficiency.
- Training uses **group-relative advantages**, meaning advantages are computed **across a group of rollouts per prompt** (not per individual rollout), which is critical for the **Pass@k** setting.
- The **advantage estimator** defaults to **Pass@k** (k=64), meaning only the top-k rollouts per prompt contribute to the gradient update.

This implies GRPO is not a standard PPO or REINFORCE variant — it is **group-aware**, **top-k selective**, and **KL-regularized**, designed to **prevent mode collapse** while encouraging **high-reward exploration**.

---

## **2. Algorithmic Foundations: GRPO vs. Standard PPO**

Standard PPO computes advantage per sample:
```python
advantage_i = R_i - V(s_i)
```
Then applies clipped surrogate objective:
```python
L = min( ratio * adv, clip(ratio, 1-ε, 1+ε) * adv )
```

In contrast, **GRPO** (as used in the paper and `verl` framework) computes **group-relative advantages**:
- For each prompt, generate `k` rollouts (e.g., k=64).
- Compute reward `R_j` for each rollout `j`.
- Compute **relative advantage** for rollout `j` within its group:
  ```
  A_j = R_j - mean(R_{j' in group})
  ```
- Then, **only retain top-k rollouts** (or all, if Pass@1) — but **in Pass@k, we use all k rollouts**, and the advantage is **relative to the group mean**.
- Apply **KL penalty** between current policy and reference (pretrained) policy.
- Use **gradient clipping**.

This design is **critical** because:
- In LLMs, a single prompt can yield many diverse outputs.
- The best design (e.g., a catapult that throws 150m) may be rare — we want to **amplify its signal**.
- Using group-relative advantage prevents **reward scale bias** (e.g., if one prompt’s best rollout is 100x better than others, it doesn’t dominate globally).
- **Pass@k** is used because the task has **low validity rate** — we care about the **best** output, not average.

> ✅ **Paper Confirmation**:  
> “We adopt group relative policy optimization (GRPO)… We evaluate both the standard GRPO advantage estimator and the pass@k variant… Pass@k finetuning is more likely to discover promising machine designs.”  
> — Section “Main Results and Observations”

> ✅ **Configuration Match**:  
> `config.yaml` → `advantage_estimator: "Pass@k"` and `pass_k: 64`

---

## **3. Input to GRPO.update()**

The `update()` method receives a **batch** of data from `verl_wrapper.rollout()`, structured as:

```python
batch = {
    "prompts": List[str],  # e.g., ["Build a car to drive far"]
    "responses": List[str],  # generated JSON strings (construction trees)
    "rewards": List[float],  # R = R_valid * R_task (float)
    "logprobs": List[float],  # log probability of generated sequence under current policy
    "ref_logprobs": List[float],  # log probability under reference (pretrained) model
    "prompt_ids": List[int],  # indices to group responses by prompt (e.g., 0,0,0,1,1,1...)
}
```

- **Grouping**: Responses are grouped by `prompt_ids`. For each group (e.g., 64 responses for one prompt), we compute the **group mean reward**.
- **Advantage per rollout**:  
  `A_j = R_j - mean(R_group)`
- **KL Divergence**:  
  `KL = mean(logprobs - ref_logprobs)` — computed per token, averaged over sequence.
- **Objective**: Maximize the **weighted sum** of:
  - Group-relative advantage (positive signal)
  - Negative KL penalty (stability)

The loss function for GRPO is defined as:
```
L_GRPO = E[ min( ratio * A, clip(ratio, 1-ε, 1+ε) * A ) ] - λ * KL
```
where:
- `ratio = exp(logprobs - ref_logprobs)` (importance sampling ratio)
- `A = group-relative advantage`
- `λ = kl_penalty_weight = 0.001` (from config)

> ✅ **Paper Confirmation**:  
> “We add a KL penalty (weight 0.001) with respect to the pretrained LLM and do not introduce any entropy regularization.”  
> — Section “RL Finetuning Settings”

> ✅ **Implementation Note**:  
> The `verl` framework expects this exact structure. GRPO must be compatible with its batch format.

---

## **4. LoRA Integration and Parameter Efficiency**

The paper specifies:
> “We adopt group relative policy optimization (GRPO) with LoRA parametrization (rank 64)”

This means:
- The base model (`Qwen2.5-14B-Instruct`) is **frozen**.
- Only **LoRA adapters** (low-rank matrices) on **all linear layers** (query, key, value, output) are trainable.
- LoRA rank = 64 → each adapter matrix is of shape `(d, 64)` and `(64, d)` for a layer of dimension `d`.

**Implementation Requirements**:
- Use `peft` (Parameter-Efficient Fine-Tuning) library to inject LoRA adapters.
- Apply LoRA to **all linear layers** in the transformer (not just attention). The paper says “all linear layers”, so include:
  - `q_proj`, `k_proj`, `v_proj`, `o_proj`
  - `gate_proj`, `up_proj`, `down_proj` (for MLP in Llama-style models like Qwen)
- Use `transformers` + `peft` to load model with LoRA.
- **Do not** use full fine-tuning — this would exceed GPU memory (14B model).

> ✅ **Configuration Match**:  
> `config.yaml` → `lora_rank: 64`

> ✅ **Dependency**: `peft` (via `transformers`), `bitsandbytes`

---

## **5. 8-bit AdamW Optimizer via bitsandbytes**

The paper states:
> “We use 8-bit AdamW optimizer implemented with bitsandbytes”

This is critical for memory efficiency during training on 8 A100s with batch size 1 per GPU and grad accumulation 8.

**Implementation Requirements**:
- Initialize optimizer using `bitsandbytes.optim.AdamW8bit` (not standard `torch.optim.AdamW`).
- Must be applied **only to trainable parameters** (i.e., LoRA adapters).
- Set `lr = 5e-6` (from config).
- Set `weight_decay = 0.01` (standard for AdamW, not specified in paper, but default in `bitsandbytes`).

> ✅ **Dependency**: `bitsandbytes==0.41.0`

> ✅ **Memory Impact**: 8-bit AdamW reduces optimizer memory from ~2× model size to ~0.5× model size.

---

## **6. Advantage Clipping and Gradient Clipping**

The paper specifies:
> “The GRPO advantage estimator uses an advantage clipping ratio of 0.2”

This means:
- **Advantage clipping** is applied to prevent extreme gradients.
- For each rollout, compute:
  ```
  A_clipped = clamp(A, -ε, +ε), where ε = 0.2
  ```
- Then use `A_clipped` in the surrogate objective.

This is **different from PPO’s policy ratio clipping**. Here, we clip the **advantage value itself**, not the importance sampling ratio.

> ✅ **Paper Confirmation**:  
> “advantage clipping ratio of 0.2” — Section “RL Finetuning Settings”

> ✅ **Configuration Match**:  
> `config.yaml` → `advantage_clip_ratio: 0.2`

Additionally:
> “gradient clipping threshold set to 0.5”

Apply gradient clipping to **all trainable parameters** (LoRA weights) using `torch.nn.utils.clip_grad_norm_` with `max_norm=0.5`.

---

## **7. Rollout Sampling and Pass@k Integration**

The `GRPO` class does **not** perform rollouts itself — it **relies on `verl_wrapper.rollout()`** to generate:
- `k=64` rollouts per prompt (from `config.yaml`)
- Each rollout is a full sequence of tokens (JSON construction tree)
- Returns `rewards`, `logprobs`, `ref_logprobs`

**GRPO’s role**:
- Receive batch of rollouts.
- Group by prompt ID.
- For each group, compute:
  - Mean reward: `mean_R = mean([R_1, ..., R_k])`
  - Advantage for each rollout: `A_j = R_j - mean_R`
  - Clip advantages: `A_j = clamp(A_j, -0.2, +0.2)`
- Compute importance sampling ratio: `ratio = exp(logprob_j - ref_logprob_j)`
- Compute surrogate loss:
  ```
  surrogate = min(ratio * A_j, clip(ratio, 1-0.2, 1+0.2) * A_j)
  ```
- Compute KL loss: `KL = mean(logprob_j - ref_logprob_j)`
- Total loss: `loss = -mean(surrogate) + 0.001 * KL`
- Backpropagate through LoRA adapters only.

> ✅ **Critical Note**:  
> The paper uses **Pass@k** advantage estimator — this means **all k rollouts are used** in the update, and advantage is **group-relative**.  
> This is distinct from “Pass@k as selection” (i.e., pick best and ignore others). Here, **all k are used**, but advantage is relative to group mean.  
> This ensures **diversity is preserved** while still amplifying high-reward signals.

> ✅ **Contrast with Pass@1**:  
> In Pass@1, only the single best rollout per prompt is used → high variance, low diversity.  
> In Pass@k (k=64), all 64 are used → stable, diverse, and effective.

> ✅ **Paper Confirmation**:  
> “Pass@k finetuning is more likely to discover promising machine designs.”  
> — Section “Main Results and Observations”

---

## **8. Training Loop Integration**

The `GRPO.update()` method is called **400 times** (from `config.yaml`), each time with a batch of:
- **Effective batch size**: 8 GPUs × 1 per GPU × 8 grad accum = 64 rollouts per update
- **Per-prompt rollouts**: 64 (k=64) → so each batch = **1 prompt’s 64 rollouts**

Thus, **each `update()` call processes one prompt’s full set of 64 rollouts**.

This is consistent with:
> “Each experiment is run for 400 iterations on 8 A100 GPUs with per-GPU batch size of 1 and gradient accumulation of 8.”

So the training loop is:
```python
for step in range(400):
    prompt = next_prompt()  # sample one prompt from dataset
    rollouts = verl_wrapper.rollout(prompt, num_rollouts=64)  # 64 samples
    batch = pack_into_batch(rollouts)  # includes logprobs, ref_logprobs, rewards
    loss = grpo.update(batch)
    optimizer.step()
    optimizer.zero_grad()
```

> ✅ **Note**: The `verl` framework likely handles batching across prompts, but since `k=64` and `grad_accum=8`, and we have 8 GPUs, it’s likely each GPU processes 8 rollouts per step, grouped into 1 prompt per step.

> ✅ **Assumption**: Each batch fed to `GRPO.update()` contains **exactly one prompt’s 64 rollouts**.

---

## **9. Mixed Precision and Hardware**

The paper states:
> “mixed-precision training”

This is enabled via:
- `torch.cuda.amp.autocast()`
- `GradScaler()` from `torch.cuda.amp`

In `GRPO.update()`:
- Wrap forward/backward in autocast context.
- Use `GradScaler` to scale gradients for stability with 8-bit AdamW.

> ✅ **Configuration Match**:  
> `config.yaml` → `use_mixed_precision: true`

---

## **10. Error Handling and Logging**

- **Validation**: Ensure all rollouts are valid JSON (via `ConstructionTree.validate()` before computing reward). If invalid, reward = 0.
- **Numerical Stability**: Add `eps=1e-8` in `exp(logprob - ref_logprob)` to avoid `inf`.
- **Logging**: Log mean advantage, mean reward, KL divergence, loss per step for debugging.
- **Gradient Norm**: Log `grad_norm` after clipping to detect instability.

---

## **11. Dependencies Summary**

| Dependency | Purpose |
|----------|---------|
| `torch` | Core tensors, autograd, optimizers |
| `transformers` | Load Qwen2.5-14B-Instruct, tokenizer, model |
| `accelerate` | Multi-GPU training, mixed precision, distributed setup |
| `peft` | Inject LoRA adapters on all linear layers |
| `bitsandbytes` | 8-bit AdamW optimizer, 8-bit quantization for memory efficiency |
| `numpy` | Numerical operations (e.g., advantage clipping) |

> ✅ **No external dependencies beyond the listed 5**.

---

## **12. Critical Implementation Constraints**

| Constraint | Implementation Requirement |
|----------|----------------------------|
| **LoRA on all linear layers** | Must use `peft.LoraConfig(target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"])` |
| **KL penalty = 0.001** | Must be hardcoded as scalar multiplier in loss |
| **Advantage clipping = 0.2** | Must clamp `A_j ∈ [-0.2, 0.2]` before computing surrogate |
| **Pass@k = 64** | Must use all 64 rollouts per prompt; group-relative advantage |
| **8-bit AdamW** | Must use `bitsandbytes.optim.AdamW8bit` |
| **Gradient clipping = 0.5** | Apply `clip_grad_norm_(model.parameters(), max_norm=0.5)` |
| **No entropy regularization** | Do NOT add `entropy_loss` term |
| **Mixed precision** | Use `torch.cuda.amp.autocast()` in forward pass |
| **Single prompt per batch** | Batch = 64 rollouts from one prompt (aligned with `verl` and config) |

---

## **13. Failure Modes and Mitigations**

| Failure Mode | Cause | Mitigation |
|--------------|-------|------------|
| **Loss explodes** | Large advantage outliers | Advantage clipping (0.2), gradient clipping (0.5) |
| **KL divergence spikes** | Policy drifts too far | KL penalty (0.001), monitor KL per step |
| **Reward = 0 for all rollouts** | All machines invalid | Log invalid rate; ensure simulation returns 0 for invalid, not None |
| **Model collapses to one structure** | Low diversity | Pass@k + group-relative advantage encourages exploration |
| **Memory OOM** | Full model loaded | Use LoRA + 8-bit AdamW + gradient accumulation |
| **Invalid JSON parsing** | LLM outputs malformed JSON | Validate in `verl_wrapper.rollout()` before passing to GRPO |

---

## **14. Final Design Summary**

The `GRPO` class is a **lightweight, high-performance policy optimizer** tailored for LLM-based compositional machine design. It:

1. **Accepts a batch of 64 rollouts per prompt** (from `verl_wrapper`).
2. **Groups rollouts by prompt** and computes **group-relative advantages**.
3. **Clips advantages** to ±0.2 to stabilize gradients.
4. **Computes importance sampling ratios** and applies **clipped surrogate objective**.
5. **Adds KL penalty** with weight 0.001 to constrain policy drift.
6. **Uses LoRA (rank=64)** on all linear layers for parameter efficiency.
7. **Uses 8-bit AdamW** for memory efficiency.
8. **Applies gradient clipping** at 0.5.
9. **Uses mixed precision** for speed and memory.
10. **Returns loss** to be backpropagated — **only LoRA parameters are updated**.

This implementation is **fully aligned** with:
- The paper’s methodology (Section “Experimental Settings” and “Main Results”),
- The configuration file (`config.yaml`),
- The `verl` framework’s expected input/output,
- The requirement to avoid entropy regularization and preserve diversity via Pass@k.

It is the **core engine** enabling open-source LLMs to outperform closed models in compositional machine design — by learning from verifiable physics rewards, not just human-written CoTs.