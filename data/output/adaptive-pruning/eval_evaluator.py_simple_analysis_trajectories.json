[{"role": "system", "content": "You are an expert researcher, strategic analyzer and software engineer with a deep understanding of experimental design and reproducibility in scientific research.\nYou will receive a research paper, an overview of the plan, a design in JSON format consisting of \"Implementation approach\", \"File list\", \"Data structures and interfaces\", and \"Program call flow\", followed by a task in JSON format that includes \"Required packages\", \"Required other language third-party packages\", \"Logic Analysis\", and \"Task list\", along with a configuration file named \"config.yaml\". \n\nYour task is to conduct a comprehensive logic analysis to accurately reproduce the experiments and methodologies described in the research paper. \nThis analysis must align precisely with the paper\u2019s methodology, experimental setup, and evaluation criteria.\n\n1. Align with the Paper: Your analysis must strictly follow the methods, datasets, model configurations, hyperparameters, and experimental setups described in the paper.\n2. Be Clear and Structured: Present your analysis in a logical, well-organized, and actionable format that is easy to follow and implement.\n3. Prioritize Efficiency: Optimize the analysis for clarity and practical implementation while ensuring fidelity to the original experiments.\n4. Follow design: YOU MUST FOLLOW \"Data structures and interfaces\". DONT CHANGE ANY DESIGN. Do not use public member functions that do not exist in your design.\n5. REFER TO CONFIGURATION: Always reference settings from the config.yaml file. Do not invent or assume any values\u2014only use configurations explicitly provided.\n     \n"}, {"role": "user", "content": "## Paper\n# APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference\n\nBowen Zhao 1 Hannaneh Hajishirzi 1 2 Qingqing Cao\u2217 3\n\n# Abstract\n\nFine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of finetuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to $98 \\%$ task performance when pruning $60 \\%$ of the parameters in RoBERTa and T5 models. APT also preserves $8 6 . 4 \\%$ of LLaMA models\u2019 performance with $70 \\%$ parameters remaining. Furthermore, APT speeds up LMs\u2019 fine-tuning by up to $8 \\times$ and reduces large LMs\u2019 memory training footprint by up to $70 \\%$ Our code and models are publicly available at https://github.com/ROIM1998/APT.\n\n![](images/97be0e120351b3cd832a1193313d541c1047d0304a4663136a4dc679cd1a963a.jpg)  \nFigure 1. APT provides both training and inference efficiency benefits by pruning and tuning pretrained LM parameters adaptively via the APT adapter. We dynamically adjust (add/reduce) APT adapter input/output dimensions and the rank $( r _ { \\mathrm { a p t } } )$ . Reducing adapter dimensions prunes frozen parameters, making training and inference faster and more memory-efficient. Adding adapter ranks helps recover the pruned LM\u2019s task performance. In contrast, existing adapters like LoRA allow efficient training but do not provide inference efficiency since the model size is not reduced.\n\na 13B LLaMA model (Touvron et al., 2023) costs about 100GB memory for fine-tuning and 30GB for inference with float16 datatype. It is important to improve the training and inference efficiency of LM for practical applications.\n\n# 1. Introduction\n\nFine-tuning language models (LMs) (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020) is an essential paradigm to adapt them to downstream tasks (Mishra et al., 2022; Wang et al., 2022b). Increasing the parameter scale of LMs improves model performance (Kaplan et al., 2020), but incurs significant training and inference costs. For instance,\n\nParameter-efficient fine-tuning methods (PEFT, summarized in Table 1) (Houlsby et al., 2019; Li & Liang, 2021) reduce the memory consumption of LM fine-tuning via updating a small number of parameters. However, PEFT models do not improve inference efficiency because the LM size remains the same or even increases after fine-tuning. For instance, LoRA (Hu et al., 2022) tunes low-rank decomposed linear layers parallel to frozen parameters to reduce training memory but takes longer to converge (Ding et al., 2023). On the other hand, structured pruning (Kwon et al., 2022; Xia et al., 2022; Ma et al., 2023) improves inference efficiency by removing blocks of parameters such as attention heads and feed-forward neurons in Transformer LMs, showing more inference speedup than sparse unstructured pruning methods (Han et al., 2016; 2015; Sanh et al., 2020). However, training pruned LMs takes extra time to converge and incurs high memory, substantially diminishing LMs\u2019 accessibility in usage scenarios with limited computational resources.\n\nIntegrating structured pruning and PEFT could increase both training and inference efficiency. However, existing research (Zhao et al., 2023) indicates that combining PEFT and structured pruning, such as applying structured pruning over LoRA-tuned models, causes noticeable performance loss and extra training costs. It remains challenging to prune LMs accurately using limited training resources.\n\n<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\"></td><td rowspan=\"2\">Ap</td><td rowspan=\"2\">AT</td><td colspan=\"2\">Training</td><td colspan=\"2\">Inference</td></tr><tr><td>T</td><td>M</td><td>T</td><td>M</td></tr><tr><td rowspan=\"3\">PEFT</td><td>Adapter (Pfeiffer et al., 2021)</td><td>\u00d7</td><td></td><td>\u4ecbHigh</td><td>\u2193Low</td><td>\u4ecbLow</td><td>\u4ecbLow</td></tr><tr><td>LoRA (Hu et al.,2022)</td><td>\u00d7</td><td>\u00d7</td><td>\u4ecbHigh</td><td>Low</td><td>=</td><td>=</td></tr><tr><td>AdaLoRA (Zhang et al., 2023b)</td><td>X</td><td>\u221a</td><td>\u4ecbHigh</td><td>\u2193Low</td><td>=</td><td>=</td></tr><tr><td rowspan=\"4\">Pruning</td><td>MvP (Sanh et al., 2020)</td><td>\u00d7</td><td>X</td><td>\u4ecbHigh</td><td>\u4ecbLow</td><td>\u221aLow</td><td>Low</td></tr><tr><td>BMP (Lagunas et al.,2021)</td><td>\u00d7</td><td>X</td><td>\u4ecbHigh</td><td>\u4ecbLow</td><td>High</td><td>\u2193Low</td></tr><tr><td>CoFi (Xia et al., 2022)</td><td>\u00d7</td><td></td><td>\u4ecbHigh</td><td>\u4ecbLow</td><td>High</td><td>Low</td></tr><tr><td>MT (Kwon et al., 2022)</td><td>X</td><td>\u00d7</td><td>=</td><td>=</td><td>High</td><td>Low</td></tr><tr><td rowspan=\"3\">Combined</td><td>SPA (Hedegaard etal., 2022)</td><td>X</td><td>X</td><td>\u4ecbHigh</td><td>\u4ecbLow</td><td>\u221aHigh</td><td>\u221aLow</td></tr><tr><td>LRP (Zhang et al., 2023a)</td><td>X</td><td>X</td><td>\u4ecbHigh</td><td>\u2193Low</td><td>High</td><td>\u221aLow</td></tr><tr><td>APT (ours)</td><td>\u221a</td><td>\u221a</td><td>\u4ecbLow</td><td>Low</td><td>High</td><td>\u221aLow</td></tr></table>\n\nIn this paper, we develop an efficient fine-tuning approach named APT that Adaptively selects model parameters for Pruning and fine-Tuning. APT combines the benefits of PEFT and structured pruning to make fine-tuning and inference more efficient. Our intuition is that pre-trained LM parameters contain general knowledge, but their importance to downstream tasks varies. Therefore, we can remove the parameters irrelevant to the fine-tuning task in the early training stage. Early-removing these parameters improves training and inference efficiency while not substantially hurting model accuracy (Frankle et al., 2021; Shen et al., 2022a; Zhang et al., 2023c). Meanwhile, continuously adding more parameters for fine-tuning can improve LM performance because task-specific skills live in a subset of LM parameters (Wang et al., 2022a; Panigrahi et al., 2023).\n\nMore specifically, APT learns the pruning masks via an outlier-aware salience scoring function to remove irrelevant LM parameter blocks and adds more tuning parameters during fine-tuning according to tuning layer importance. To make training more efficient, the salience scoring function is lightweight and causes little runtime and memory overhead. Combined with our self-distillation technique that shares teacher and student parameters, APT can accurately prune an LM with less training time and lower memory usage.\n\nExperimental results show that APT prunes RoBERTa and T5 base models $8 \\times$ faster than the LoRA plus pruning baseline while reaching $9 8 . 0 \\%$ performance with $2 . 4 \\times$ speedup and $7 8 . 1 \\%$ memory consumption during inference. When pruning large LMs like LLaMA, APT costs only $30 \\%$ memory compared to the state-of-the-art pruning method and still maintains $8 6 . 4 \\%$ performance with $70 \\%$ parameters. Our ablation study in Section 5.6 indicates the effectiveness of adaptive pruning and tuning. It also demonstrates that efficient distillation with APT adapter substantially recovers small LMs\u2019 performance while outlier-aware salience scoring prunes large LMs more accurately. Our analysis in Appendix H demonstrates that controlled adaptive tuning with early pruning during fine-tuning improves LM end-task accuracy better with less training time and memory costs.\n\n# 2. Related Works\n\n# 2.1. Parameter-efficient Fine-tuning (PEFT)\n\nPEFT methods aim to tune LMs with limited resources by updating a small number of parameters (Lialin et al., 2023), mainly falling into three categories: selective, additive, and dynamic. Selective methods focus on tuning a subset of parameters in LMs with pre-defined rules (Ben Zaken et al., 2022) or importance metrics (Sung et al., 2021; Guo et al., 2021). Additive methods tune injected layer modules (Houlsby et al., 2019; Pfeiffer et al., 2021) or embeddings (Lester et al., 2021; Li & Liang, 2021). For example, LoRA (Hu et al., 2022) tunes low-rank decomposed layers to avoid inference cost overhead. However, LoRA keeps the tuning layer shapes static without dynamic adjustments. Dynamic methods (He et al., 2022b) adjust tuning parameters during training. For instance, AdaLoRA (Zhang et al., 2023b) gradually reduces tuning parameters but does not benefit inference efficiency. Compared to these methods, APT adaptively adjusts the pruning and tuning parameters simultaneously, improving training and inference efficiency.\n\n# 2.2. Model Compression\n\nModel compression methods like quantization and pruning boost inference efficiency. Quantization aims to reduce LMs\u2019 memory consumption via converting parameters to low-bit data types (Frantar et al., 2023; Dettmers et al., 2022; Lin et al., 2023). However, despite reducing LM\u2019s memory consumption, the speedup benefits of quantization require specific framework support, which limits their adaptability. Pruning (LeCun et al., 1989; Han et al., 2016; Frankle & Carbin, 2019; Xu et al., 2021) aims to discard unimportant parameters in LMs for inference efficiency. Unstructured pruning (Sanh et al., 2020) prunes sparse parameters in LMs, which requires dedicated hardware support for efficiency improvements. Meanwhile, structured pruning (Lagunas et al., 2021; Xia et al., 2022) prunes consistent blocks in transformer layers (MHA heads, FFN neurons, and model dimensions) for ubiquitous inference efficiency gains. Such pruning often uses knowledge distillation (Hinton et al., 2015), which causes more training costs. Post-training pruning (Kwon et al., 2022; Frantar & Alistarh, 2023) aims to prune fine-tuned models with limited extra costs but requires initialization from fully fine-tuned models. Moreover, taskagnostic pruning (Sun et al., 2023; Ma et al., 2023) cannot achieve on-par performance with task-specific pruning.\n\n# 2.3. Combining Compression and PEFT\n\nCombining model compression and PEFT might achieve both training and inference efficiency improvements: QLoRA (Dettmers et al., 2023) and QA-LoRA (Xu et al., 2023) bring quantization and LoRA together for large LM tuning. SPA (Hedegaard et al., 2022) combines structured pruning and Compacter (Mahabadi et al., 2021), yet suffers substantial performance loss. CPET (Zhao et al., 2023) leverages different task-agnostic model compression methods together with LoRA and knowledge distillation, but the performance loss becomes notable specifically when structured pruning is applied. PST (Li et al., 2022) and LRP (Zhang et al., 2023a) also explored the combination of LoRA and pruning, yet their performance degradations are also substantial because their tuning parameters are static. In contrast, APT identifies tuning and pruning parameters based on their salience in fine-tuning, which can improve training and inference efficiency under a new paradigm with minimal performance loss.\n\n# 3. Problem Formulation\n\nOur goal is to improve the training and inference efficiency of pretrained LM while maintaining task performance. Intuitively, tuning fewer parameters leads to smaller training memory footprints and shorter time per training step; models with fewer parameters also run faster with less memory footprint during inference but come with task performance degradation. We aim to find the optimal parameters for training and inference without sacrificing task performance.\n\nWe formally define the problem objective as minimizing the task loss $\\mathcal { L }$ under the constraint that the total LM parameter size $\\Theta$ reaches a target sparsity (defined as the ratio of the number of parameters pruned to the original LM) $\\gamma _ { T }$ after $T$ training steps. For each training step $t$ , the sparsity of the LM remains above $\\gamma _ { t }$ while the number of tuning parameters is below $\\Delta _ { t }$ . We control the pruning masks $\\mathcal { M } _ { t }$ and tuning ranks $\\mathcal { R } _ { t }$ to satisfy these constraints. We describe the optimization process as:\n\n$$\n\\begin{array} { r l } { \\underset { \\boldsymbol { \\Theta } _ { T } , \\boldsymbol { \\mathcal { M } } _ { T } } { \\mathrm { a r g m i n } } } & { \\displaystyle \\frac { 1 } { | \\mathcal { D } | } \\displaystyle \\sum _ { \\boldsymbol { x } , \\boldsymbol { y } \\in \\mathcal { D } } \\mathcal { L } ( \\boldsymbol { x } , \\boldsymbol { y } | \\Theta _ { T } , \\mathcal { M } _ { T } ) } \\\\ { \\mathrm { s . t . } \\quad } & { 1 - \\frac { \\mathcal { C } ( \\Theta _ { t } , \\mathcal { M } _ { t } ) } { \\mathcal { C } ( \\Theta _ { 0 } , \\mathcal { M } _ { 0 } ) } \\geq \\gamma _ { t } , } \\\\ & { \\delta ( \\Theta _ { t } , \\mathcal { M } _ { t } , \\mathcal { R } _ { t } ) \\leq \\Delta _ { t } , } \\\\ & { \\forall t \\in \\{ 0 , 1 , \\ldots , T \\} . } \\end{array}\n$$\n\nwhere $x , y$ are inputs and labels sampled from the task dataset $\\mathcal { D }$ , while $\\mathcal { C }$ and $\\delta$ denotes total and tuning parameter numbers of the LM, respectively.\n\nBased on Equation (1), a higher target sparsity $\\gamma _ { T }$ improves inference efficiency with fewer FLOPs and memory usage but sacrifices performance. Increasing $\\gamma _ { t }$ when $t \\ll T$ also improves training efficiency. Besides, tuning more parameters with larger $\\Delta$ costs more training memory but makes the model converge faster with better task performance. Our formulation supports task performance improvements together with training and inference efficiency by dynamically adjusting the LM parameters during fine-tuning.\n\n# 4. Adaptive Pruning and Tuning\n\nWe design Adaptive Pruning and Tuning (APT) over LM parameters to allow efficient training and inference while maintaining task performance.\n\nSummarized in the left of Figure 2, existing pruning methods often neglect training costs where the number of tuning parameters is more than a parameter-efficient threshold with $\\Delta _ { t } \\geq \\mathcal { C } ( \\Theta _ { t } , \\mathcal { M } _ { t } )$ , resulting in long training time and high memory consumption. Instead, to improve training efficiency, we prune LM parameters (increase $\\gamma _ { t }$ ) during early training when $t \\ll T$ while keeping $\\Delta _ { t } \\ll \\mathcal { C } ( \\Theta _ { t } , \\mathcal { M } _ { t } )$ to reduce training costs. In addition, we add tuning parameters (increase $\\Delta _ { t }$ ) in early training to effectively mitigate the degradation of LM\u2019s performance due to pruning.\n\nOverview. Figure 2 shows the overview of our method that incorporates our new APT adapter for pruning and tuning. Our intuition is that pruning LMs during early fine-tuning will not hurt their task performance while reducing training and inference costs. Meanwhile, unlike existing adapters like LoRA (Hu et al., 2022) that use fixed tuning parameters, APT adapters dynamically add tuning parameters to accelerate LM convergence with superior task performance. We first introduce the architecture of APT adapters in Section 4.1. We then describe how we prune LM parameters at early fine-tuning with low cost in Section 4.2 and adaptively tune LMs to recover task performance efficiently in Section 4.3. Additionally, we explain our self-knowledge distillation technique that improves pruned LM\u2019s task performance with limited training expense in Section 4.4.\n\n![](images/7a784ac712151bf50d4c32a16f607ec7d880b2d6b2af934f5de79e54ad065f5d.jpg)  \nFigure 2. APT adaptively identifies pruning and tuning parameters via APT adapters during fine-tuning with little cost. APT gradually prunes LM parameters with binary pruning masks learned from our lightweight outlier-aware salience scoring function for training and inference efficiency. APT also adds tuning parameters in salient layers in LM fine-tuning through increasing dynamic ranks in APT adapters for performance recovery.\n\n# 4.1. APT adapter\n\nWe build the APT adapter architecture over LoRA, but the key difference is that APT adapter supports dynamic LM pruning and tuning. Assuming an APT adapter projects the input $X \\in \\mathbb { R } ^ { d _ { i } }$ to the output $\\bar { H } _ { \\mathrm { a p t } } ( X ) \\in \\mathbb { R } ^ { \\bar { d } _ { o } }$ , we design binary pruning masks $\\mathbf { \\bar { \\Phi } } _ { m _ { i } } \\in \\mathbb { R } ^ { d _ { i } }$ for input and $m _ { o } \\in \\mathbb { R } ^ { \\bar { d } _ { o } }$ for output) and dynamic ranks $r _ { \\mathrm { a p t } }$ in APT adapter to control the total and tuning LM parameters during fine-tuning, respectively. Specifically, with tuning parameters $W _ { A } \\in \\mathbb { R } ^ { r _ { \\mathrm { a p t } } \\times d _ { i } }$ and $W _ { B } \\in \\mathbb { R } ^ { d _ { o } \\times r _ { \\mathrm { a p t } } }$ , APT adapter $H _ { \\mathrm { a p t } }$ is denoted as:\n\n$$\nH _ { \\mathrm { a p t } } ( X ) = m _ { o } \\circ ( W + s \\cdot W _ { B } W _ { A } ) X \\circ m _ { i }\n$$\n\nwhere $s$ is the constant scaling factor following LoRA\u2019s implementation, and $\\circ$ denotes the Hadamard product between the masks and their corresponding matrices. The parameter block is pruned when the multiplying mask is set to 0 and retained when set to 1. In the meantime, during fine-tuning, we dynamically increase $r _ { \\mathrm { a p t } }$ for the weight matrices $W _ { B }$ and $W _ { A }$ . Compared to LoRA, APT adapters can be more efficient due to more adaptive pruning and tuning over LM parameters.\n\nIn transformer-based LM fine-tuning, we add APT adapters in queries and values of multi-head attention (MHA) layers. We also add APT adapter in feed-forward network (FFN) layers when fine-tuning smaller models like RoBERTa and T5 for fast training convergence. In these cases, $m _ { i }$ prunes transformers\u2019 hidden dimension and $m _ { o }$ prunes attention heads in MHA and internal neurons in FFN layers. By learning the pruning masks and adjusting the ranks dynamically in the APT adapter, we can achieve the goal defined in Section 3 where the tuning parameter number $\\delta ( \\Theta _ { t } , \\mathcal { M } _ { t } , \\mathcal { R } _ { t } )$ increases to maintain task performance and the LM parameter size $\\mathcal { C } ( \\Theta _ { t } , \\mathcal { M } _ { t } )$ decreases to support more efficient training and inference. Next, we describe the adaptive pruning and tuning procedures in detail.\n\n# 4.2. Low-cost Adaptive LM Pruning $\\left( \\mathcal { A } _ { \\mathbf { p } } \\right)$\n\nTo benefit the efficiency of LM training and inference, APT adaptively prunes LM parameters since the start of finetuning. The problem is finding the parameters to be pruned and discarding them without hurting training stability. Given a task, we compute the outlier-aware salience score of parameter blocks at each early-training step when $t \\ll T$ Afterward, we use a fast search algorithm to determine the parameters to be pruned, and then we update their binary pruning masks accordingly. The upper-right of Figure 2 shows this adaptive pruning procedure.\n\nOutlier-aware salience scoring of LM parameters. When determining the influence of pruning parameters on the LM performance for fine-tuning tasks, the key idea is to compute the outlier-aware salience scores of LM activations to consider both tuning and frozen parameters. In detail, salience is defined as the magnitude of parameters\u2019 weight-gradient production from previous works (Sanh et al., 2020), where\n\n$$\nS ( W _ { i , j } ) = | W _ { i , j } \\cdot \\frac { \\partial \\mathcal { L } } { \\partial W _ { i , j } } |\n$$\n\nHowever, since the frozen weights\u2019 gradients are unreachable in PEFT settings, we compute the salience as the magnitude of the product of activations and their gradients. Additionally, we compress the activation and gradients by summing along batches before production to further reduce the training memory consumption. On the other hand, block outlier parameters play a crucial role in task-specific capabilities, as previous quantization methods suggest (Dettmers et al., 2022; Lin et al., 2023). Such effects brought by outlier parameters will be averaged if salience is only measured on the block level. To keep more outlier parameters in the pruned LMs, we combine the salience score above and the kurtosis1 of the activation together. Therefore, given the supervised finetuning dataset $\\mathcal { D } _ { t }$ , the outlier-aware salience score $\\hat { S }$ is defined as:\n\n$$\n\\begin{array} { r l } & { \\widetilde { S } _ { t } ( W _ { : , j } ) = \\displaystyle \\sum _ { ( x , y ) \\in \\mathcal { D } _ { t } } \\sum _ { i } \\vert \\frac { \\partial \\mathcal { L } ( x , y \\vert \\Theta _ { t } , \\mathcal { M } _ { t } ) } { \\partial H _ { j , i } } \\vert \\cdot } \\\\ & { \\qquad \\displaystyle \\sum _ { ( x , y ) \\in \\mathcal { D } _ { t } } \\sum _ { i } \\vert H _ { j , i } \\vert } \\\\ & { \\qquad \\quad \\widehat { S } ( ( W _ { : , j } ) = \\widetilde { S } ( W _ { : , j } ) + ( \\mathrm { K u r t } ( O _ { j , : } ) ) ^ { \\frac { 1 } { 2 } } } \\end{array}\n$$\n\nwhere $H$ is the activations in the LM, $\\mathrm { K u r t } ( \\cdot )$ stands for kurtosis, and $O _ { : , j } = W _ { : , j } \\circ X _ { j , } ^ { \\intercal }$ represents the activation. We leave details of the salience scoring in Appendix B.\n\nEfficient search of LM block parameters. Given the salience calculated in Equation (5), the next step is to learn the binary pruning masks to increase the LM sparsity above $\\gamma _ { t }$ . Intuitively, we shall prune the blocks with less salience score, which formulates a latency-saliency knapsack (Shen et al., 2022b) task. For an LM with $n _ { L }$ transformer layers, where layer $i$ has $n _ { h } ^ { i }$ MHA heads and ${ n } _ { f } ^ { i }$ FFN neurons, and all transformer layers\u2019 hidden dimension sizes are $d _ { m }$ , the approximated2 number LM parameter is:\n\n$$\n\\mathcal { C } ( \\Theta _ { t } ; \\mathcal { M } _ { t } ) \\approx d _ { m } \\sum _ { i = 1 } ^ { n _ { L } } ( 4 n _ { h } ^ { i } \\cdot d _ { h } + 2 n _ { f } ^ { i } )\n$$\n\nwhere $d _ { h }$ is the dimension per MHA head. To keep the constraint in Equation (1), we prune MHA heads, FFN neurons, and the model hidden dimension simultaneously by reducing $n _ { h } ^ { i } , n _ { f } ^ { i }$ , and $d _ { m }$ . Hence, we first sort the blocks by their salience divided by the parameter number. As the parameter size monotonically increases with block quantity, we use binary search to identify the top salient blocks to be retained given the sparsity constraint $\\gamma _ { t }$ . We leave the implementation details in Appendix C for simplicity.\n\n# 4.3. Adaptive and Efficient LM Tuning $\\left( \\mathcal { A } _ { \\mathbf { T } } \\right)$\n\nAs using PEFT methods to fine-tune pruned LMs causes notable performance decrease (illustrated in Table 2 and Table 4), we aim to dynamically add tuning parameters in LM fine-tuning to improve the model\u2019s end-task performance. However, since more tuning parameters will consume extra training time and memory, we want to add parameters in a controlled way, where new parameters are only added to task-sensitive APT adapters. As a result, we can recover pruned LMs\u2019 performance with reasonable training costs. In detail, we first calculate the salience of each APT adapter to determine their importance. Next, we select the top-half APT adapters after sorting them with salience and add their parameters by increasing their $r _ { \\mathrm { a p t } }$ .\n\nSalience scoring of APT adapter. Since gradients of tuning parameters information are available when determining the layer salience, we can first calculate each tuning parameter\u2019s salience with Equation (3). Then, we define the salience of an APT adapter as the summation of the parameter salience scores in $W _ { B }$ , denoted as $\\begin{array} { r } { \\mathcal { T } ( H _ { \\mathrm { a p t } } ) = \\sum _ { i , j } S ( W _ { B i , j } ) } \\end{array}$ , to represent each tuning APT adapter\u2019s importance3. Given the calculated $\\mathcal { T } ( H _ { \\mathrm { a p t } } )$ for each APT adapter, we can then decide where to add new tuning parameters to efficiently improve the pruned LM\u2019s task accuracy.\n\nDynamically adding APT adapter parameters to recover task performance. With the importance of APT adapters $\\mathcal { T } ( H _ { \\mathrm { a p t } } )$ calculated, the next step of adaptive tuning is to add tuning parameters by increasing the salient tuning layers\u2019 ranks $r _ { \\mathrm { a p t } } \\in \\mathcal { R } _ { t }$ following budget $\\Delta _ { t }$ . Therefore, firstly, we sort all tuning layers according to their importance score $\\mathcal { T } ( H _ { \\mathrm { a p t } } )$ and linearly increase the ranks of the tophalf salient ones. More specifically, when increasing the tuning parameter from $\\Delta _ { t }$ to $\\Delta _ { t ^ { \\prime } }$ , the salient layer\u2019s rank is changed from rapt to r\u2032apt = \u230arapt \u00b7 \u2206 \u2206t where $\\lfloor \\cdot \\rfloor$ denotes the floor operation. For training stability, when adding parameters and converting $W _ { B } \\in \\mathbb { R } ^ { d _ { o } \\times r _ { \\mathrm { a p t } } }$ , $W _ { A } \\in \\mathbb { R } ^ { r _ { \\mathrm { a p t } } \\times d _ { i } }$ to $W _ { B } ^ { \\prime } \\in \\mathbb { R } ^ { d _ { o } \\times r _ { \\mathrm { a p t } } ^ { \\prime } } , W _ { A } ^ { \\prime } \\in \\mathbb { R } ^ { r _ { \\mathrm { a p t } } ^ { \\prime } \\times d _ { i } }$ , we concatenate random Gaussian initialized parameters ${ \\mathcal { N } } ( 0 , \\sigma ^ { 2 } )$ in $W _ { A }$ and zeros in $W _ { B }$ same as the LoRA initialization, so the layer\u2019s output remains unchanged before and after new parameters added.\n\n# 4.4. Efficient Self-Knowledge Distillation\n\nAs shown in Table 4, training pruned LM without knowledge distillation causes significant end-task performance drops. Therefore, we use knowledge distillation in APT to recover the pruned LM\u2019s performance. Still, existing strategies require a fully trained teacher model being put into the GPU with the student during distillation, causing high training time and memory. To avoid extra training costs, we keep duplicating the tuning student layers as teachers during finetuning to reduce total training time. Meanwhile, frozen parameters are shared between the student and teacher model during training to reduce memory consumption. We edit the distillation objective in CoFi (Xia et al., 2022) as\n\n$$\n\\begin{array} { c } { \\mathcal { L } = \\mu \\mathcal { L } _ { d i s t i l l } + ( 1 - \\mu ) \\mathcal { L } _ { f t } } \\\\ { \\mathcal { L } _ { l a y e r } = \\displaystyle \\sum _ { i = 1 } ^ { \\mathcal { T } } \\mathbf { M S E } ( \\mathrm { T r } ( H _ { s } ^ { \\phi ( i ) } ) , H _ { t } ^ { i } ) } \\end{array}\n$$\n\nwhere $\\mu$ is a moving term linearly scales from 0 to 1 during distillation to encourage the pre-pruned model vastly fit to the training data, $\\mathcal { L } _ { d i s t i l l }$ is the distillation objective from CoFi, and $\\mathcal { L } _ { f t }$ is the supervised fine-tuning objective. $\\tau$ is block-wise randomly sampled teacher layers following (Haidar et al., 2022), $\\phi ( \\cdot )$ is the teacher-student layer-mapping function that matches the teacher layer to its closest, non-pruned student layer. Tr denotes the tunable LoRA layer for layer transformation, initialized as an identical matrix $\\mathcal { T }$ . More implementation details of our self-distillation technique is introduced in Appendix A.\n\n# 5. Experiments\n\nTo evaluate the training and inference efficiency gains of APT, we compare it with the combined use of PEFT with pruning and distillation baselines. We first describe the natural language understanding and generation tasks targeting different LM backbones, then the setup of baselines and APT. We then report task performance, speed, and memory usage for training and inference costs.\n\n# 5.1. Tasks\n\nWe apply APT to BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), T5(Raffel et al., 2020)4, and LLaMA (Touvron et al., 2023). For BERT, RoBERTa, and T5 models, we train and evaluate on SST2 and MNLI datasets from the GLUE benchmark (Wang et al., 2019) and report the dev set accuracy. We also train and evaluate $\\mathrm { R o B E R T a _ { b a s e } }$ on $\\mathrm { S Q u A D }$ v2.0 (Rajpurkar et al., 2018) and report the dev set F1 score. For T5 models, we also fine-tune them on CNN/DM (Nallapati et al., 2016) and report the ROUGE 1/2/L scores. Meanwhile, We use the GPT-4 generated Alpaca dataset (Taori et al., 2023) to fine-tune large LLaMA models and evaluate them with the lm-eval-harness package (Gao et al., 2023) on four tasks from the Open LLM Leaderboard, namely 25- shot ARC (Clark et al., 2018), 10-shot HellaSwag (Zellers et al., 2019), 5-shot MMLU (Hendrycks et al., 2021), and zero-shot TruthfulQA (Lin et al., 2022).\n\n# 5.2. Baselines\n\nWe validate the efficiency benefits of APT for both training and inference by comparing with PEFT, pruning, and distillation methods, along with their combinations.\n\nLoRA+Prune: a post-training pruning method over on LoRA-tuned LMs. We use Mask Tuning (Kwon et al., 2022), a state-of-the-art post-training structured pruning method based on fisher information. Due to that post-training pruning performs poorly on high-sparsity settings, we retrain the pruned LM after pruning to recover its performance.\n\nPrune+Distill: knowledge distillation has been proved to be a key technique in recovering pruned LMs\u2019 task accuracy. In particular, we use the state-of-the-art pruning plus distillation method called CoFi (Xia et al., 2022) which uses $L _ { 0 }$ regularization for pruning plus dynamic layer-wise distillation objectives. We only compare APT to CoFi with RoBERTa models since the training memory usage of CoFi is too high for larger LMs.\n\nLoRA+Prune+Distill: to reduce the training memory consumption in pruning and distillation, a simple baseline is to conduct CoFi pruning and distillation but with LoRA parameters tuned only. More specifically, only the $L _ { 0 }$ module and LoRA parameters are tunable under this setting.\n\nLLMPruner (Ma et al., 2023): LLMPruner is the state-ofthe-art task-agnostic pruning method on LLaMA that prunes its blocks or channels based on salience metrics while using LoRA for fast performance recovery. We compare APT to LLMPruner with fine-tuning on the same GPT-4 generated Alpaca data for fair comparisons.\n\nWe also compare APT to PST (Li et al., 2022) and LRP (Zhang et al., 2023a), which are the state-of-theart parameter-efficient unstructured and structured pruning methods on BERT model. We leave these results in Appendix D.\n\n# 5.3. Evaluation Metrics\n\nWe evaluate APT and baselines on training and inference efficiency, measured in runtime memory and time consumption as follows:\n\nTraining Efficiency Metrics: we report relative training peak memory (Train. Mem.) and relative training speed measured by time to accuracy $( \\mathrm { T T A } ^ { 5 } )$ (Coleman et al., 2019) compared to full finetuning. For fair comparisons, we consider the training time of the teacher model plus the student for methods using knowledge distillation.\n\nInference Efficiency Metrics: we report the inference peak memory (Inf. Mem.) and the relative speedup (Inf. Speed)\n\n<table><tr><td>Model</td><td>Method</td><td>MNLI</td><td>SST2</td><td>SQuAD v2</td><td>CNN/DM</td><td>Train Time()</td><td>Train Mem(\u2193)</td><td>Inf Time()</td><td>Inf Mem()</td></tr><tr><td rowspan=\"6\">RoBERTabase</td><td>FT</td><td>87.6</td><td>94.8</td><td>82.9</td><td>-</td><td>100.0%</td><td>100.0%</td><td>100.0%</td><td>100.0%</td></tr><tr><td>LoRA</td><td>87.5</td><td>95.1</td><td>83.0</td><td>-</td><td>2137.0%</td><td>60.5%</td><td>100.0%</td><td>100.0%</td></tr><tr><td>LoRA+Prune</td><td>84.0</td><td>93.0</td><td>79.2</td><td></td><td>5128.3%</td><td>60.5%</td><td>38.0%</td><td>75.1%</td></tr><tr><td>Prune+Distill</td><td>87.3</td><td>94.5</td><td>-</td><td>-</td><td>1495.3%</td><td>168.5%</td><td>38.6%</td><td>79.2%</td></tr><tr><td>LoRA+Prune+Distill</td><td>84.2</td><td>91.9</td><td>-</td><td></td><td>6534.6%</td><td>141.4%</td><td>39.4%</td><td>82.3%</td></tr><tr><td>APT</td><td>86.4</td><td>94.5</td><td>81.8</td><td>-</td><td>592.1%</td><td>70.1%</td><td>41.3%</td><td>78.1%</td></tr><tr><td rowspan=\"4\">T5base</td><td>FT</td><td>87.1</td><td>95.2</td><td></td><td>42.1/20.3/39.4</td><td>100.0%</td><td>100.0%</td><td>100.0%</td><td>100.0%</td></tr><tr><td>LoRA</td><td>87.0</td><td>95.0</td><td></td><td>38.7/17.2/36.0</td><td>255.5%</td><td>62.0%</td><td>100.0%</td><td>100.0%</td></tr><tr><td>LoRA+Prune</td><td>80.9</td><td>92.3</td><td></td><td>36.7/15.7/33.9</td><td>4523.5%</td><td>62.0%</td><td>47.1%</td><td>73.4%</td></tr><tr><td>APT</td><td>87.0</td><td>95.0</td><td></td><td>38.6/17.0/35.8</td><td>484.7%</td><td>73.9%</td><td>74.6%</td><td>81.5%</td></tr></table>\n\nTable 2. RoBERTa and T5 pruning with APT compared to baselines under $60 \\%$ sparsity. We measure the training and inference efficiency with LMs pruned on the SST2 task. Training speed is measured via $9 7 \\%$ accuracy TTA. All efficiency metrics are normalized to FT. \u21d3 denotes smaller is better. The best-pruned results are bold. Raw efficiency results are reported in Table 11.   \n\n<table><tr><td>Method</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Avg.</td><td>Train Time(\u2193)</td><td>Train Mem ()</td><td>Inf Time(\u2193)</td><td>Inf Mem(\u2193)</td></tr><tr><td>LLaMA 2 7B</td><td>53.1</td><td>77.7</td><td>43.8</td><td>39.0</td><td>53.4</td><td>1</td><td></td><td>-</td><td>-</td></tr><tr><td>LoRA</td><td>55.6</td><td>79.3</td><td>46.9</td><td>49.9</td><td>57.9</td><td>100.0%</td><td>100.0%</td><td>100.0%</td><td>100.0%</td></tr><tr><td>LoRA+Prune</td><td>46.8</td><td>65.2</td><td>23.9</td><td>46.2</td><td>45.5</td><td>180.9%</td><td>100.0%</td><td>115.5%</td><td>68.9%</td></tr><tr><td>LLMPruner</td><td>39.2</td><td>67.0</td><td>24.9</td><td>40.6</td><td>42.9</td><td>86.9%</td><td>253.6%</td><td>114.8%</td><td>74.2%</td></tr><tr><td>APT</td><td>45.4</td><td>71.1</td><td>36.9</td><td>46.6</td><td>50.0</td><td>106.0%</td><td>75.8%</td><td>117.0%</td><td>67.2%</td></tr></table>\n\nTable 3. LLaMA 2 7B $30 \\%$ sparsity pruning results with GPT4-generated Alpaca dataset, evaluated on the Open LLM leaderboard few-shot tasks. Training speed is measured via training time per step. We do not compare to distillation baselines because the training cost of distillation is too large, and we also compare APT to LLMPruner since it is dedicated to large LM pruning. All efficiency metrics are normalized to LoRA. $\\Downarrow$ denotes smaller is better. The best-pruned results are bold. Raw efficiency results are reported in Table 12.\n\nbased on throughput (data processed per second) for inference efficiency.\n\nBoth training and evaluation are conducted on a single A100 GPU. The inference test batch size is 128 for small models while 32 and 4 for LLaMA 7B and 13B models, respectively. We demonstrate detailed training and evaluation setups/implementations in Appendix A.\n\n# 5.4. Main Results\n\nOverview We demonstrate the end-task performance of APT comparing to fine-tuning (FT), LoRA-tuning (LoRA), and pruning baselines in Table 2 and Table 3. Overall, up to $9 9 \\%$ of fine-tuned LM\u2019s task accuracy is maintained when pruning RoBERTa and T5 models leaving $40 \\%$ parameters, with only about $70 \\%$ training memory consumption than fine-tuning. When pruning LLaMA2-7B models with $70 \\%$ parameters remaining, APT recovers $8 6 . 4 \\%$ task performance on average, together with only $7 5 . 8 \\%$ training memory usage than LoRA-tuning. Furthermore, APT also significantly reduces end-task performance and training costs compared to the pruning and distillation baselines. The detailed comparisons are shown as follows.\n\nAPT speeds up RoBERTa and T5 training $\\mathbf { 8 \\times }$ and reduces training memory costs to $30 \\%$ in LLaMA pruning compared to LoRA $^ +$ Prune baseline. Shown in Table 2, when pruning RoBERTa models to $60 \\%$ sparsity, APT converges $8 . 4 \\times$ faster than the LoRA $^ +$ Prune baseline with consuming similar GPU memory. APT also prunes T5 models $8 . 2 \\times$ faster than the LoRA $^ +$ Prune baseline. The reason is that APT adaptively prunes task-irrelevant parameters during training, reducing memory and per-step training time. Adding parameters in salient tuning layers also accelerates LM convergence. Also, APT costs less than 24GB of memory when pruning $30 \\%$ parameters in LLaMA2-7B models before tuning, which can be easily adapted to the consumerlevel GPUs. In contrast, LLM-Pruner costs about 80GB memory when pruning the LLaMA 7B model6.\n\nAPT achieves $2 . 5 \\% . 9 . 9 \\%$ higher task performance than the LoRA+Prune baseline with the same pruning sparsities. Presented in Table 2 and Table 3 , when RoBERTa, T5, and LLaMA models, regardless of size, APT consistently reach higher task performance than the LoRA+Prune. With similar inference speedup and memory when pruning RoBERTa models, APT reaches $2 . 5 \\%$ more end-task performance on average. When pruning T5 models under the $60 \\%$ sparsity, the task performance achieved by APT is $5 . 1 \\%$ better than the LoRA $^ +$ Prune baseline. However, the inference efficiency reached by APT ( $1 . 3 \\times$ speedup and $8 1 . 5 \\%$ memory cost) is worse than the LoRA $+$ Prune baseline $( 2 . 1 \\times$ speedup and $7 3 . 4 \\%$ memory cost). This is because APT can adaptively prune more decoder parameters, which are also computationally cheaper than encoder parameters (due to shorter output sequence length) but relatively useless for classification tasks. For LLaMA2-7B model pruning with $70 \\%$ sparsity, APT outperforms LLMPruner with $1 6 . 5 \\%$ and the LoRA $^ +$ Prune baseline with $9 . 9 \\%$ , where the inference efficiency improvements of APT is slightly better than both LoRA+Prune and LLMPruner baselines.\n\nAPT reaches on-par performance with the Prune+Distill baseline given the same pruning sparsity but trains $2 . 5 \\times$ faster and costs only $4 1 . 6 \\%$ memory. Compared to the Prune+Distill baseline, APT results in comparable task accuracy (0.9 point drop in MNLI and same in SST2). At the same time, with similar inference efficiency achieved, APT costs only $4 1 . 6 \\%$ training memory and converges $2 . 5 \\times$ than the Prune+Distill baseline. This is because of the selfdistillation technique in APT where no separated teacher model is required in pruning LMs. Moreover, APT achieves better task performance than the LoRA+Prune+Distill baseline as well, with less training time and memory consumption. These results demonstrate that APT successfully tackles the problem where simply combining PEFT and pruning hurts pruned LM\u2019s task accuracy and training efficiency.\n\n![](images/b69b2f909dafdfc22ab1883d7fda78032988349319a7552876765648cc9c7785.jpg)  \nFigure 3. Task performance v.s. relative inference efficiency on RoBERTa, T5, and LLaMA-2 7B models with APT and baselines.\n\n# 5.5. Pruning Sparsity Analysis\n\nWe further show the task performance changing trajectory with different pruning sparsities in Figure 3. APT achieves superior inference speedup with less inference memory consumption than baselines targeting the same task performance. Compared to the LoRA+Prune baseline, when pruning RoBERTa models targeting similar task accuracy, APT is $2 1 . 8 \\%$ faster in inference and is $7 \\%$ more memoryefficient. For T5 model pruning with $9 7 \\%$ of dense model performance, APT results in $6 2 . 7 \\%$ more inference speedup with $2 4 . 8 \\%$ more inference memory reduction compared to the LoRA $^ +$ Prune baseline. When pruning large LLaMA2- 7B models, APT speedup is $6 . 7 \\%$ more and reduces $9 . 2 \\%$ more inference memory than the LoRA $^ +$ Prune baseline, maintaining over $85 \\%$ task performance of the dense model.\n\n# 5.6. Ablation Study\n\nWe evaluate the impact of different components in APT by removing the adaptive pruning $( { \\mathcal { A } } _ { \\mathrm { p } } )$ , adaptive tuning $( \\mathcal { A } _ { \\mathrm { T } } )$ , and self-distillation $( \\mathcal { D } _ { \\mathrm { { S } } } )$ . Besides end-task performance, we also report the training efficiency metrics for each ablation.\n\nAdaptive pruning $( { \\mathcal { A } } _ { \\mathbf { P } } )$ We demonstrate the ablation of adaptive pruning (w/o $\\mathcal { A } _ { \\mathrm { P } } \\big )$ ) for RoBERTa models in Table 4 and LLaMA models in Table 5. In these cases, we only train LMs with adaptive tuning strategies with supervised finetuning objectives without distillation. In such settings, APT w/o $\\mathcal { A } _ { \\mathrm { P } }$ can be recognized as a PEFT method with tuning parameters\u2019 sizes adaptively changing during fine-tuning. Hence, the inference efficiency of the trained LMs are the same as full fine-tuning and LoRA. Without pruning, the task performance of RoBERTa reaches 94.4 for SST2 and 87.5 for MNLI $( 9 9 . 8 \\%$ fine-tuned LM performance on average). The average performance of the LLaMA model also achieves $9 6 . 6 \\%$ to its LoRA-tuned counterpart. In addition, we surprisingly find that the RoBERTA training speed with APT w/o $\\mathcal { A } _ { \\mathrm { P } }$ is even $21 \\%$ faster than full fine-tuning while costing only $6 2 . 2 \\%$ memory. In the meantime, the training memory cost of APT w/o $\\mathcal { A } _ { \\mathrm { P } }$ in LLaMA tuning is higher than LoRA. The reason is that the tuning parameter number of APT will grow larger than static LoRA-tuning. This ablation demonstrates that adaptive pruning is essential in reducing the training memory consumption of LLaMA model fine-tuning, besides benefiting model inference efficiency.\n\nAdaptive tuning $\\left( \\mathcal { A } _ { \\mathbf { T } } \\right)$ In Table 4, we show results of ablating adaptive tuning (w/o $\\boldsymbol { \\mathcal { A } } _ { \\mathrm { T } } $ ) where the tuning parameters are static when pruning RoBERTa models. Without $\\mathcal { A } _ { \\mathrm { T } }$ , the model\u2019s performance decreases to 93.2/84.4, leading to a similar performance as the LoRA+Prune baseline (93.0/84.0). Moreover, equally increasing parameters across all layers instead of adding parameters based on salience notably hurts the task accuracy (84.4 on MNLI compared to 86.4). At the same time, $\\mathcal { A } _ { \\mathrm { T } }$ helps the model converge\n\n$16 \\%$ faster than static LoRA training. For ablation results in LLaMA models shown in Table 5, we observe that $\\mathcal { A } _ { \\mathrm { T } }$ recovers the model performance under $50 \\%$ pruning setting (38.2 compared to 35.8). However, the difference under $70 \\%$ pruning is insignificant. Meanwhile, if calculating the pruning parameter salience without using kurtosis to consider outliers parameters, the pruned LM\u2019s performance substantially drops from 50.0 to 38.1. We conclude that $\\mathcal { A } _ { \\mathrm { T } }$ substantially improves LM training speed and end-task performance. For large LLaMA-based LM pruning, and outlier parameters are essential to recovering the pruned large LLaMA-based models\u2019 capabilities.\n\nTable 4. Results of ablating salience-based allocation strategy and APT adapter with RoBERTa-base model, with relative training efficiency metrics to fine-tuning.   \n\n<table><tr><td>Method</td><td>SST2</td><td>MNLI</td><td>Train Time()</td><td>Train Mem()</td></tr><tr><td>APT</td><td>94.5</td><td>86.4</td><td>592.1%</td><td>70.1%</td></tr><tr><td>w/o Ap</td><td>94.4</td><td>87.5</td><td>82.6%</td><td>62.2%</td></tr><tr><td>w/o salience</td><td>94.3</td><td>84.7</td><td>609.8%</td><td>65.0%</td></tr><tr><td>w/o AT</td><td>93.2</td><td>84.5</td><td>684.9%</td><td>64.4%</td></tr><tr><td>w/o Ds</td><td>92.9</td><td>85.3</td><td>483.1%</td><td>61.9%</td></tr></table>\n\n<table><tr><td></td><td>Sparsity</td><td>T.M.</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Avg</td></tr><tr><td>APT</td><td>30%</td><td>75.8%</td><td>45.4</td><td>71.1</td><td>36.9</td><td>46.6</td><td>50.0</td></tr><tr><td>w/oAp</td><td>100%</td><td>102.4%</td><td>53.8</td><td>79.1</td><td>46.9</td><td>48.4</td><td>57.1</td></tr><tr><td>w/o kurtosis</td><td>30%</td><td>75.9%</td><td>47.2</td><td>39.7</td><td>23.0</td><td>42.3</td><td>38.1</td></tr><tr><td>w/oAT</td><td>30%</td><td>76.1%</td><td>44.2</td><td>70.1</td><td>40.8</td><td>45.1</td><td>50.0</td></tr><tr><td>APT</td><td>50%</td><td>60.2%</td><td>29.8</td><td>48.9</td><td>26.7</td><td>47.6</td><td>38.2</td></tr><tr><td>w/o AT</td><td>50%</td><td>60.1%</td><td>27.9</td><td>46.2</td><td>24.5</td><td>44.7</td><td>35.8</td></tr></table>\n\nTable 5. LLaMA 2 7B model ablation results under $30 \\%$ and $50 \\%$ sparsity settings. T.M. denotes relative training memory compare to LoRA-tuning.\n\nSelf-distillation $\\mathbf { ( } \\mathcal { D } \\mathbf { s } )$ Shown in Table 4, tuning APT adapters dynamically without distillation objectives gets 1.35 worse task accuracy on average. However, pruning RoBERTa models without self-distillation is $2 2 . 5 \\%$ faster and costs $1 1 . 7 \\%$ less training memory. This result indicates the effectiveness of leveraging knowledge distillation to recover pruned LM performance, but conducting distillation will result in extra training costs regarding both time and memory. Detailed comparisons of self-distillation and traditional, static distillation strategies are shown in Appendix G.\n\nBesides the ablation study results demonstrated above, we show the detailed analysis of adaptive pruning and tuning\u2019s effect on LMs\u2019 end-task performance, training, and inference efficiency in Appendix H.\n\n# 6. Limitation and Discussion\n\nTowards better performance gain and inference speedup of large LM in limited resource settings. By comparing Table 2 to Table 3, we notice the performance gap in pruned LLaMA models is larger than smaller LMs because we use distillation-free settings in large LM pruning to reduce training memory consumption. One can improve performance-efficiency trade-offs with better memoryefficient distillation, parameter sharing, and re-allocation strategies. Furthermore, because of the hardware features of Ampere-architecture GPUs, layer dimensions divisible by 8 for FP16 and divisible by 16 for Int8 would reach more realistic speedups. One possible direction is to explore a higher level of structured pruning, for example, grouped neurons and dimensions, in LLMs.\n\nTraining could be unstable because of parameter shape changes. Since we adjust tuning parameters dynamically during training, newly initialized parameters are added to the model while existing parameters are pruned. We reset the optimizer every time after each parameter size changes to avoid stability issues, but this strategy might cause unstable training. Meanwhile, the time of selecting the teacher checkpoints during training highly affects the pruned model\u2019s performance, whereas non-converged or sparse teachers do not help in performance recovery. The pruned LMs\u2019 end-task accuracy could benefit from better and more stable strategies in adaptive pruning and tuning.\n\nCould non-linear adapters perform better for performance recovery? To avoid inference time and memory overhead, we specifically adapt APT adapter to LoRA since the added tuning parameters can be merged after LMs\u2019 training. However, low-rank decomposition does not add more complexity to a LM, whereas the model\u2019s overall representation capacity doesn\u2019t increase. The adaptation with a wider range of adapters, such as Prefix-tuning (Li & Liang, 2021), HAdapters (Houlsby et al., 2019), and Parallel-adapters (He et al., 2022a), could be better explored.\n\n# 7. Conclusion\n\nWe design APT to adaptively identify LMs\u2019 pruning and tuning parameters during fine-tuning, improving both training and inference efficiency. APT prunes small LMs faster while pruning large LMs with less memory consumption. With using similar memory costs as LoRA, APT prunes small LMs $8 \\times$ faster than the LoRA plus pruning baseline. In large LM pruning, APT maintains $87 \\%$ performance with only $30 \\%$ pruning memory usage when $70 \\%$ LM parameter retained. APT opens new directions to pruning LMs in fine-tuning for resource-limited settings, allowing wider usage of LMs in practical applications. In the future, we could adapt APT to more PEFT architectures and target better performance-efficiency trade-offs for billion-level large LMs. Meanwhile, we hope future research will continue to find efficient and accurate techniques to identify salient structures in LMs based on our formulated setting.\n\n# Acknowledgements\n\nThis research was supported partly by NSF IIS-2044660, an Allen Investigator Distinguished award. We thank the members of the UW NLP group for their comments and feedback on this paper.\n\n# Impact Statement\n\nThis paper introduces APT, a paradigm for improving the efficiency of training and inference in pre-trained LMs. While our primary goal is to advance machine learning, particularly in the efficiency of LMs and their applications, we recognize potential broader societal impacts. APT significantly reduces training and inference costs and contributes to lower resource consumption for a wide range of applications. This could have a positive environmental impact but might lead to potential model misuse due to lower resource requirements. Additionally, while APT does not introduce new ethical concerns, it might inherit existing issues in language models, for example, biases in training data. We explicitly ask users of APT to be aware of these risks and follow best practices in data selection and model monitoring to mitigate potential harms.\n\n# References\n\nBen Zaken, E., Goldberg, Y., and Ravfogel, S. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 1\u20139, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.1.\n\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv preprint, abs/1803.05457, 2018.\n\nColeman, C., Kang, D., Narayanan, D., Nardi, L., Zhao, T., Zhang, J., Bailis, P., Olukotun, K., R\u00e9, C., and Zaharia, M. Analysis of dawnbench, a time-to-accuracy machine learning performance benchmark. SIGOPS Oper. Syst. Rev., 53(1):14\u201325, 2019. ISSN 0163-5980. doi: 10.1145/ 3352020.3352024.\n\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 30318\u201330332. Curran Associates, Inc., 2022.\n\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,\n\nL. Qlora: Efficient finetuning of quantized llms. ArXiv preprint, abs/2305.14314, 2023.\n\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\n\nDing, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S., Chen, Y., Chan, C.-M., Chen, W., et al. Parameterefficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence, 5(3):220\u2013235, 2023.\n\nFrankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nFrankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Pruning neural networks at initialization: Why are we missing the mark? In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n\nFrantar, E. and Alistarh, D. SparseGPT: Massive language models can be accurately pruned in one-shot. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 10323\u2013 10337. PMLR, 2023.\n\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023.\n\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac\u2019h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot language model evaluation, 2023.\n\nGuo, D., Rush, A., and Kim, Y. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4884\u20134896, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.378.\n\nHaidar, M. A., Anchuri, N., Rezagholizadeh, M., Ghaddar, A., Langlais, P., and Poupart, P. RAIL-KD: RAndom intermediate layer mapping for knowledge distillation. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 1389\u20131400, Seattle, United States, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.103.\n\nHan, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural network. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 1135\u20131143, 2015.\n\nHan, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. In Bengio, Y. and LeCun, Y. (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.\n\nHe, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a.\n\nHe, S., Ding, L., Dong, D., Zhang, J., and Tao, D. SparseAdapter: An easy approach for improving the parameter-efficiency of adapters. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2184\u20132190, Abu Dhabi, United Arab Emirates, 2022b. Association for Computational Linguistics.\n\nHedegaard, L., Alok, A., Jose, J., and Iosifidis, A. Structured Pruning Adapters, 2022.\n\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. ArXiv preprint, abs/1503.02531, 2015.\n\nHoulsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for NLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2790\u20132799. PMLR, 2019.\n\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\n\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. ArXiv preprint, abs/2001.08361, 2020.\n\nKwon, W., Kim, S., Mahoney, M. W., Hassoun, J., Keutzer, K., and Gholami, A. A fast post-training pruning framework for transformers. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 24101\u201324116. Curran Associates, Inc., 2022.\n\nLagunas, F., Charlaix, E., Sanh, V., and Rush, A. Block pruning for faster transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10619\u201310629, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 829.\n\nLeCun, Y., Denker, J. S., and Solla, S. A. Optimal brain damage. In NIPS, 1989.\n\nLester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045\u20133059, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.243.\n\nLi, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582\u20134597, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353.\n\nLi, Y., Luo, F., Tan, C., Wang, M., Huang, S., Li, S., and Bai, J. Parameter-efficient sparsity for large language models fine-tuning. In Raedt, L. D. (ed.), Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pp. 4223\u20134229. International Joint Conferences on Artificial Intelligence Organization, 2022. doi: 10.24963/ijcai.2022/586. Main Track.\n\nLialin, V., Deshpande, V., and Rumshisky, A. Scaling down to scale up: A guide to parameter-efficient fine-tuning. ArXiv preprint, abs/2303.15647, 2023.\n\nLin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. Awq: Activation-aware weight quantization for llm compression and acceleration. ArXiv preprint, abs/2306.00978, 2023.\n\nLin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214\u2013 3252, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229.\n\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. ArXiv preprint, abs/1907.11692, 2019.\n\nMa, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models. ArXiv preprint, abs/2305.11627, 2023.\n\nMahabadi, R. K., Henderson, J., and Ruder, S. Compacter: Efficient low-rank hypercomplex adapter layers. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 1022\u20131035, 2021.\n\nMishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Crosstask generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.244.\n\nNallapati, R., Zhou, B., dos Santos, C., Gulcehre, C., and Xiang, B. Abstractive text summarization using sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, pp. 280\u2013290, Berlin, Germany, 2016. Association for Computational Linguistics. doi: 10.18653/v1/K16-1028.\n\nPanigrahi, A., Saunshi, N., Zhao, H., and Arora, S. Taskspecific skill localization in fine-tuned language models. ArXiv preprint, abs/2302.06600, 2023.\n\nPfeiffer, J., Kamath, A., R\u00fcckl\u00e9, A., Cho, K., and Gurevych, I. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 487\u2013503, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.39.\n\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67, 2020.\n\nRajpurkar, P., Jia, R., and Liang, P. Know what you don\u2019t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 784\u2013789, Melbourne, Australia, 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124.\n\nSanh, V., Wolf, T., and Rush, A. M. Movement pruning: Adaptive sparsity by fine-tuning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n\nShen, M., Molchanov, P., Yin, H., and Alvarez, J. M. When to prune? a policy towards early structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 12247\u2013 12256, 2022a.\n\nShen, M., Yin, H., Molchanov, P., Mao, L., Liu, J., and Alvarez, J. M. Structural pruning via latency-saliency knapsack. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 12894\u201312908. Curran Associates, Inc., 2022b.\n\nSun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective pruning approach for large language models. ArXiv preprint, abs/2306.11695, 2023.\n\nSung, Y., Nair, V., and Raffel, C. Training neural networks with fixed sparse masks. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 24193\u201324205, 2021.\n\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca, 2023.\n\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023.\n\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\n\nWang, X., Wen, K., Zhang, Z., Hou, L., Liu, Z., and Li, J. Finding skill neurons in pre-trained transformer-based language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11132\u201311152, Abu Dhabi, United Arab Emirates, 2022a. Association for Computational Linguistics.\n\nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Pal, K. K., Patel, M., Moradshahi, M., Parmar, M., Purohit, M., Varshney, N., Kaza, P. R., Verma, P., Puri, R. S., Karia, R., Doshi, S., Sampat, S. K., Mishra, S., Reddy A, S., Patro, S., Dixit, T., and Shen, X. Super-NaturalInstructions: Generalization via declarative instructions on $1 6 0 0 +$ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085\u20135109, Abu Dhabi, United Arab Emirates, 2022b. Association for Computational Linguistics.\n\nXia, M., Zhong, Z., and Chen, D. Structured pruning learns compact and accurate models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1513\u20131528, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.107.\n\nXu, D., Yen, I. E.-H., Zhao, J., and Xiao, Z. Rethinking network pruning \u2013 under the pre-train and fine-tune paradigm. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2376\u20132382, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.188.\n\nXu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X., and Tian, Q. Qa-lora: Quantizationaware low-rank adaptation of large language models. ArXiv preprint, abs/2309.14717, 2023.\n\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791\u20134800, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472.\n\nZhang, M., Shen, C., Yang, Z., Ou, L., Yu, X., Zhuang, B., et al. Pruning meets low-rank parameter-efficient fine-tuning. ArXiv preprint, abs/2305.18403, 2023a.\n\nZhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023b.\n\nZhang, Z., Zeng, Z., Lin, Y., Xiao, C., Wang, X., Han, X., Liu, Z., Xie, R., Sun, M., and Zhou, J. Emergent modularity in pre-trained transformers. ArXiv preprint, abs/2305.18390, 2023c.\n\nZhao, W., Huang, Y., Han, X., Liu, Z., Zhang, Z., and Sun, M. Cpet: Effective parameter-efficient tuning for compressed large language models. ArXiv preprint, abs/2307.07705, 2023.\n\n# A. Hyperparameter and Training Details\n\nOur hyper-parameter settings are shown in Table 6. For GLUE task fine-tuning, we follow the hyper-parameter setting of CoFi (Xia et al., 2022), separating the tasks into big (MNLI, SST2, QNLI, QQP) and small (MRPC, CoLA, RTE, STSB) based on the dataset size. For instruction tuning on the Alpaca dataset, we train the pruned model for 15 epochs after the pre-tuning pruning process to make sure they converge. However, in practice, such training epochs can be reduced. To adaptively increase the tuning parameters in the LM, at the start of fine-tuning, we initialize adapter ranks to 8, with salient layers\u2019 ranks linearly increased. The scaling factors are set as 2 statically. Since evaluating billion-level LLaMA models during instruction tuning with all evaluation tasks would be time-consuming, we did not do the TTA evaluation as small models. We do not conduct any hyper-parameters search for any training for fair comparison.\n\nTable 6. Hyperparameters used in APT experiments   \n\n<table><tr><td>Hypeparameter</td><td>GLUE-small</td><td>GLUE-big</td><td>SQuAD</td><td>CNN/DM</td><td>Alpaca</td></tr><tr><td>Learning rate</td><td>2e-4</td><td>2e-4</td><td>2e-4</td><td>1e-4</td><td>1e-4</td></tr><tr><td>Batch size</td><td>32</td><td>32</td><td>32</td><td>16</td><td>32</td></tr><tr><td>Epochs</td><td>40</td><td>40</td><td>40</td><td>16</td><td>15</td></tr><tr><td>Distill epochs</td><td>20</td><td>20</td><td>20</td><td>6</td><td>-</td></tr></table>\n\nWhen pruning LMs with APT, following (Xia et al., 2022), we first prune and train the LM with the self-distillation objective, and then fine-tune the pruned LM to recover its end-task performance. Given $T$ pruning training steps in total, we set a pre-determined target sparsity $\\gamma _ { T }$ (defined as the ratio of pruned parameter size to the total parameter size) and use cubic scheduling to control the LM parameter size, where $\\begin{array} { r } { \\gamma _ { t } = \\gamma _ { T } + ( 1 - \\gamma _ { T } ) ( 1 - \\frac { t } { T } ) ^ { 3 } } \\end{array}$ . We adaptively increase the tuning parameters in the pruning stage but restrict them to a specific limit $\\Delta _ { t }$ at each training step $t$ . Towards better training stability in LM pruning, we gradually decrease the pruning masks of pruned blocks by $\\alpha < 1$ instead of instantly setting them from ones to zeros. We also use the exponential moving-averaged salience in (Zhang et al., 2023b) when calculating the salience score during fine-tuning.\n\n# B. Block salience calculation and correlations\n\nAs addressed in Section 4.1, we use the compressed weight-gradient production as the salience metric for identifying the tuning and pruning parameter blocks in LMs. Previous works (Sanh et al., 2020) use salience score defined as the magnitude of the parameters\u2019 weight-gradient production, where given a linear layer $H = W X$ (we omit the bias term here for simplicity) in model parameters $\\Theta$ trained on the objective $\\mathcal { L }$ , the salience scoring function $S$ is defined as:\n\n$$\n\\begin{array} { l } { { S ( W _ { i , j } ) = \\displaystyle \\sum _ { ( x , y ) \\in { \\cal D } \\atop ( x , y ) \\in { \\cal D } } s ( W _ { i , j } , x , y ) \\nonumber } } \\\\ { { \\displaystyle = \\sum _ { ( x , y ) \\in { \\cal D } } \\left. \\frac { \\partial \\mathscr { L } ( x , y | \\Theta ) } { \\partial W _ { i , j } } \\cdot W _ { i , j } \\right. } } \\\\ { { S ( W _ { : , j } ) = \\displaystyle \\sum _ { ( x , y ) \\in { \\cal D } } \\displaystyle \\sum _ { i } \\sum _ { j } \\left. \\frac { \\partial \\mathscr { L } ( x , y | \\Theta ) } { \\partial W _ { i , j } } \\cdot W _ { i , j } \\right. } } \\\\ { { \\displaystyle = \\sum _ { ( x , y ) \\in { \\cal D } } \\displaystyle ( \\sum _ { i } \\left. \\frac { \\partial \\mathscr { L } ( x , y | \\Theta ) } { \\partial X _ { j , i } } \\cdot X _ { j , i } \\right. ) } } \\end{array}\n$$\n\nwhere $x , y$ are the inputs and labels sampled from the training batch $\\mathcal { D }$ . $S ( W _ { i , j } )$ denotes the unstructured, sparse parameter\u2019s salience, and $S ( W _ { : , j } )$ denotes the salience score of a block in the weight $W$ (for example, rows, columns, attention heads, etc.).\n\nWhen applying this equation to APT adapter layers as defined in Equation (2), there are three different consistent dimensions, namely input dimension $j$ , output dimension $i$ , and tuning rank dimension $k$ . Therefore, the combined salience (including\n\n# Algorithm 1 Adaptive Pruning and Tuning\n\n1: Input: Model $f$ ; Training dataset $\\mathcal { D }$ ; total training steps $T$ ; Adjustment step set $\\tau$ ; Training target $\\mathcal { L }$ ; Initial par   \nand masks $\\Theta _ { 0 } , M _ { 0 }$ , training memory budget $\\Delta$ ; Parameter number constraint $\\gamma$ ; Hyperparameters $\\alpha \\beta$ .   \n2: for $t = 1 , \\dots , T$ do   \n3: Forward pass: $L \\gets \\mathcal { L } ( f ( \\Theta _ { t } , D _ { t } ) )$   \n4: Cache the batch-sequence summed hidden states: $\\begin{array} { r } { \\widetilde { H }  \\sum _ { i , j } ( | H | ) _ { i j } } \\end{array}$   \n5: Backward pass: $\\begin{array} { r } { \\nabla _ { \\Theta _ { t } } L \\gets \\frac { \\partial \\mathcal { L } ( f ( \\Theta _ { t } , D _ { t } ) ) } { \\partial \\Theta _ { t } } } \\end{array}$   \n6: Calculate approximated salience: $\\begin{array} { r } { \\tilde { \\boldsymbol { S } } ( m _ { i } ) \\gets \\widetilde { \\boldsymbol { H } } \\cdot \\sum _ { i , j } ( | \\boldsymbol { \\nabla } _ { \\boldsymbol { H } } \\boldsymbol { L } | ) _ { i j } } \\end{array}$   \n7: Update global scores: $\\overline { { S } } ^ { ( t ) } ( m ) \\gets \\beta \\overline { { S } } ^ { ( t - 1 ) } ( m ) + ( 1 - \\beta ) \\widetilde { S } ( m )$ ;   \n8: Select blocks: $M _ { 1 }$ , $M _ { 0 } \\gets$ Binary search against constraint Equation (6), with scores $\\overline { { S } } ^ { ( t ) } ( m )$ ;   \n9: Update masks: $M _ { 1 } ^ { ( t ) } \\gets m i n ( 1 , M _ { 1 } ^ { ( t - 1 ) } + \\alpha )$ , $\\boldsymbol { M _ { 0 } ^ { ( t ) } } \\gets m a x ( 0 , M _ { 0 } ^ { ( t - 1 ) } - \\alpha )$ ;   \n10: Update parameters: $\\Theta _ { t + 1 } \\gets \\Theta _ { t } - \\alpha \\nabla _ { \\Theta _ { t } } L$   \n11: end for\n\n12: Output: Parameters and masks $\\Theta ^ { ( T ) } , M ^ { ( T ) }$ .\n\ntuning low-rank weights and the frozen weight) of the parameter block shall be calculated as follows:\n\n$$\n\\begin{array} { r l } { \\bar { S } ( \\bar { H } , \\bar { i } ) = \\sum _ { j } \\frac { \\Delta \\bar { F } ( \\bar { Z } , 0 ) \\bar { H } ( 1 , 1 ) } { \\partial W ( k , 1 ) } , } & { \\bar { H } ( \\bar { N } , j ) , } \\\\ & { \\quad - \\sum _ { j } ^ { \\infty } \\frac { \\Delta \\bar { F } ( \\bar { Z } , 0 ) \\bar { H } ( 1 , 1 ) } { \\partial W ( k , 1 ) } , } \\\\ & { \\quad \\quad - \\kappa \\cdot \\frac { \\bar { N } ( \\bar { Z } , 0 ) \\bar { H } ( 0 , 1 ) } { \\partial W ( k , 1 ) } , } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ { S ( \\bar { H } , \\bar { i } ) = \\sum _ { j } \\frac { \\Delta \\bar { F } ( \\bar { Z } , 0 ) \\bar { H } ( 1 , 1 ) } { \\partial W ( k , 1 ) } , } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad - \\sum _ { j } ^ { \\infty } \\frac { \\Delta \\bar { F } ( \\bar { Z } , 0 ) \\bar { H } ( 1 , 1 ) } { \\partial W ( k , 1 ) } , } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ & { \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad } \\\\ &  \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\end{array}\n$$\n\nTherefore, we can notice that the real block-wise salience of the LoRA layer shall be the sum of the block-wise frozen weight salience and the corresponding tuning weight. Hence, the existing work (Zhang et al., 2023a) that only uses the tuning block salience as layer salience leads to sub-optimal pruning results. Meanwhile, we shall also notice the correlation between the input-, output-dimension, and tuning rank dimensions, which are the summation of the weight-gradient production of parameters on different dimensions.\n\n# C. Adaptive Pruning and Tuning Details\n\nWe show the detailed algorithm description of our Lightweight Parameter Adjustment as described in Section 4.1 in Algorithm 1. For the details of the algorithm, we first sort all blocks by the salience density, defined as the block salience divided by the number of parameters in the block. For instance, given a RoBERTa-base model with the hidden dimension $d _ { m } = 7 6 8$ , the number of transformer layers $n _ { L } = 1 2$ , the number of attention heads $n _ { h } = 1 2$ , and the number of FFN\n\nneurons $n _ { f } = 3 0 7 2$ , we have:\n\n$$\n\\begin{array} { c } { { \\mathcal { C } _ { \\mathrm { h e a d } } = 4 \\times d _ { m } \\times d _ { m } / n _ { h } = 1 9 6 6 0 8 } } \\\\ { { \\mathcal { C } _ { \\mathrm { n e u r o n } } = 2 \\times d _ { m } = 1 5 3 6 } } \\\\ { { \\mathcal { C } _ { \\mathrm { d i m e n s i o n } } = n _ { L } \\times ( 4 d _ { m } + 2 n _ { f } ) = 1 1 0 5 9 2 } } \\end{array}\n$$\n\nWe also omit the bias term for density calculation since it takes up less than $1 \\%$ of LM\u2019s parameters. Since the number of heads, neurons, and hidden dimensions is ever-changing during pruning, we re-calculate the density after executing each parameter size change. Meanwhile, for T5 and LLaMA-like models, the FFN layers are gated, consisting of up-, gate-, and down-projection linear layers. Therefore, the number of layers in FFN shall be three instead of two in these LMs. Furthermore, for encoder-decoder LMs like T5, the cross-attention layers in the decoder shall also be counted.\n\nAfter sorting the blocks by salience density, as LM\u2019s parameter size monotonically increases with the number of MHA heads, FFN neurons, and hidden dimensions, we conduct a binary search algorithm to identify the blocks shall be retained as LM\u2019s parameter size monotonically increases with the number of MHA heads, FFN neurons, and hidden dimensions. Specifically, given a sorted list of $N$ blocks $B = \\{ b _ { 1 } , b _ { 2 } , . . . , b _ { N } \\}$ and function $f$ for identifying the block\u2019s category where\n\n$$\nf ( b _ { i } ) = { \\left\\{ \\begin{array} { l l } { 0 } & { { \\mathrm { i f ~ } } b _ { i } { \\mathrm { ~ i s ~ a ~ h e a d } } } \\\\ { 1 } & { { \\mathrm { i f ~ } } b _ { i } { \\mathrm { ~ i s ~ a ~ n e u r o n } } } \\\\ { 2 } & { { \\mathrm { i f ~ } } b _ { i } { \\mathrm { ~ i s ~ a ~ d i m e n s i o n } } } \\end{array} \\right. }\n$$\n\ngiven any index $i$ , we can calculate the parameter number of the LM consisting of the top- $i$ blocks by:\n\n$$\n\\begin{array} { l } { { \\displaystyle { \\mathrm {  ~ \\tilde { \\ r } { \\ o p . } ~ } } = \\left( 4 d _ { h } ^ { \\prime } \\cdot n _ { h } ^ { \\prime } + 2 n _ { f } ^ { \\prime } \\right) \\cdot d _ { m } ^ { \\prime } } } \\\\ { { \\displaystyle n _ { h } ^ { \\prime } = \\sum _ { j = 0 } ^ { i - 1 } \\delta ( 0 , f ( b _ { j } ) ) } } \\\\ { { \\displaystyle n _ { f } ^ { \\prime } = \\sum _ { j = 0 } ^ { i - 1 } \\delta ( 1 , f ( b _ { j } ) ) } } \\\\ { { \\displaystyle d _ { m } ^ { \\prime } = \\sum _ { j = 0 } ^ { i - 1 } \\delta ( 2 , f ( b _ { j } ) ) } } \\end{array}\n$$\n\nwhere $\\delta ( i , j )$ is the Kronecker delta function that valued 1 if $i = j$ and otherwise 0. Hence, we can use binary search to get the top- $i$ salient blocks, which shall be retained given a parameter constraint, and the rest of the block shall be pruned. In our implementation, for training stability, we do not set the pruned blocks\u2019 corresponding masks to 0 directly but gradually decrease their values by $\\alpha = 0 . 0 1$ .\n\n# D. Additional Baseline Comparisons\n\nIn this section, we further compare APT to existing parameter-efficient pruning methods, such as PST and LRP. In the meantime, we also show detailed results of APT pruning compared to the LoRA $+$ Distill baseline with more tasks in the GLUE benchmark and LLaMA-2 13B model pruning results.\n\n# D.1. Comparison to PST and LRP\n\nWe compare APT with the state-of-the-art joint use of unstructured pruning (Li et al., 2022) and structured pruning (Zhang et al., 2023a) with PEFT on $\\mathbf { B E R T _ { b a s e } }$ model, showing in Table 7. We can see that APT outperforms existing baselines in both $50 \\%$ and $10 \\%$ pruning density settings with a notable margin. The performance gain is credited to our more accurate pruning strategy considering frozen and tuning parameters. At the same time, our efficient self-distillation technique used in conjunction with salient parameters added in training also boosts performance recovery.\n\n# D.2. Further Comparison to LoRA $^ +$ Distill\n\nWe show the detailed comparison between APT and the LoRA+Distill baseline in Table 8. APT reaches superior task performance compared to the baseline in all seven GLUE tasks listed in the table, with on average $9 3 . 5 \\%$ fine-tuned LM performance maintained, notably outperforming the joint use of LoRA and knowledge distillation. In particular, the results of STS-B cannot be reproduced when conducting CoFi distillation with LoRA parameters tuned only, so we exclude the comparison on STS-B. Among the other seven tasks in the GLUE benchmark, we find that tasks with relatively smaller dataset sizes, namely CoLA, MRPC, and RTE, reach superior performance gain when using APT. We conclude that this is because, compared to simple fine-tuning, knowledge distillation with salient parameters added in training is more robust and not prone to overfitting the training data.\n\n<table><tr><td>Density</td><td>Method</td><td>MNLI</td><td>QQP</td><td>QNLI</td><td>SST2</td><td>CoLA</td><td>STS-B</td><td>MRPC</td><td>RTE</td><td>GLUE Avg.</td></tr><tr><td rowspan=\"5\">50%</td><td>MaP</td><td>83.6</td><td>87.8</td><td>91.5</td><td>91.0</td><td>60.1</td><td>89.8</td><td>90.7</td><td>67.2</td><td>82.7</td></tr><tr><td>MvP</td><td>82.3</td><td>87.3</td><td>90.8</td><td>90.8</td><td>57.7</td><td>89.4</td><td>91.1</td><td>67.2</td><td>82.1</td></tr><tr><td>PST</td><td>81.0</td><td>85.8</td><td>89.8</td><td>91.3</td><td>57.6</td><td>84.6</td><td>90.7</td><td>67.9</td><td>81.0</td></tr><tr><td>LRP</td><td>82.4</td><td>87.2</td><td>89.6</td><td>90.9</td><td>54.1</td><td>88.7</td><td>89.8</td><td>69.3</td><td>82.2</td></tr><tr><td>APT</td><td>82.8</td><td>90.1</td><td>90.1</td><td>92.7</td><td>59.6</td><td>88.3</td><td>91.8</td><td>70.4</td><td>83.2</td></tr><tr><td rowspan=\"5\">10%</td><td>MaP</td><td>78.2</td><td>83.2</td><td>84.1</td><td>85.4</td><td>27.9</td><td>82.3</td><td>80.5</td><td>50.1</td><td>71.4</td></tr><tr><td>MvP</td><td>80.1</td><td>84.4</td><td>87.2</td><td>87.2</td><td>28.6</td><td>84.3</td><td>84.1</td><td>57.6</td><td>74.2</td></tr><tr><td>PST</td><td>79.6</td><td>86.1</td><td>86.6</td><td>89.0</td><td>38.0</td><td>81.3</td><td>83.6</td><td>63.2</td><td>75.9</td></tr><tr><td>LRP</td><td>79.4</td><td>86.0</td><td>85.3</td><td>89.1</td><td>35.6</td><td>83.3</td><td>84.4</td><td>62.8</td><td>75.7</td></tr><tr><td>APT</td><td>78.8</td><td>89.4</td><td>85.5</td><td>90.0</td><td>30.9</td><td>86.3</td><td>88.2</td><td>65.3</td><td>76.8</td></tr></table>\n\nTable 7. Comparison of APT to existing unstructured pruning baseline with using PEFT in conjunction. The best results are bold while the second-best ones are underlined.   \n\n<table><tr><td>Sparsity</td><td>Method</td><td>MNLI</td><td>QQP</td><td>QNLI</td><td>SST2</td><td>CoLA</td><td>MRPC</td><td>RTE</td><td>GLUE Avg.</td></tr><tr><td rowspan=\"2\">0%</td><td>FT</td><td>87.6</td><td>91.9</td><td>92.8</td><td>95.2</td><td>91.2</td><td>90.2</td><td>78.7</td><td>89.7</td></tr><tr><td>LoRA</td><td>87.5</td><td>90.8</td><td>93.3</td><td>95.0</td><td>63.4</td><td>89.7</td><td>72.1</td><td>84.5</td></tr><tr><td rowspan=\"2\">40%</td><td>LoRA+Distill</td><td>84.2</td><td>88.3</td><td>90.1</td><td>91.9</td><td>49.9</td><td>86.8</td><td>68.6</td><td>80.0</td></tr><tr><td>APT</td><td>86.4</td><td>90.9</td><td>92.3</td><td>94.5</td><td>56.5</td><td>92.3</td><td>74.4</td><td>83.9</td></tr></table>\n\nTable 8. Detailed results of RoBERTa pruning with APT compared to the LoRA $+$ Distill baseline. We ignore the evaluation results of the STS-B task since it cannot be successfully reproduced with CoFi (the distillation backbone).\n\n# D.3. LLaMA-2 13B Pruning Results\n\nAs shown in Table 9, when pruning LLaMA-2 13B models, APT maintains $9 0 . 0 \\%$ performance of the unpruned LoRA-tuned baseline. Compared to the pruning result on 7B models that maintain $8 6 . 4 \\%$ dense model performance, better accuracies can be recovered in larger models (13B). At the same time, under the same pre-tuning pruning settings, APT performs better than the LLMPruner baseline on all four evaluation tasks, indicating the effectiveness of considering outlier parameters in large LM pruning. Nonetheless, the LoRA $^ +$ Prune baseline reaches slightly better results than APT when pruning 13B models, illustrating that there is still room for improving pre-tuning pruning methods in future works. More specifically, among the four tasks we use for evaluating large LMs, TruthfulQA benefits the most from Alpaca fine-tuning. We can see that APT reaches superior results on TruthfulQA than existing baselines regardless of model size. The LM\u2019s capabilities on ARC and HellaSawg downgrade the most when pruning large LM before fine-tuning, implying possibilities of catastrophic forgetting in this paradigm.\n\n# E. Efficiency and Performance Tradeoff Analysis\n\nWe use Figure 4 to clearly show the LMs\u2019 end-task performance and efficiency tradeoffs between different tuning, pruning, and distillation baselines. We add several extra baselines to conduct more detailed comparisons between APT with existing PEFT, pruning, and distillation methods:\n\nLoRA+Prune w/distill: we first use LoRA to fully converge a model on the task dataset, and then use Mask-Tuning (Kwon et al., 2022) to prune the LM. Afterward, we utilize the converged model before pruning as the teacher model and distill its knowledge to the pruned student model with static knowledge distillation objectives.\n\n<table><tr><td>Method</td><td>ARC</td><td>HellaSwag</td><td>MMLU</td><td>TruthfulQA</td><td>Avg</td></tr><tr><td>LLaMA2 7B</td><td>53.1</td><td>77.7</td><td>43.8</td><td>39.0</td><td>53.4</td></tr><tr><td>LoRA</td><td>55.6</td><td>79.3</td><td>46.9</td><td>49.9</td><td>57.9</td></tr><tr><td>LoRA+Prune LLMPruner</td><td>46.8</td><td>65.2</td><td>23.9</td><td>46.2</td><td>45.5</td></tr><tr><td></td><td>39.2</td><td>67.0</td><td>24.9</td><td>40.6</td><td>42.9</td></tr><tr><td>APT</td><td>45.4</td><td>71.1</td><td>36.9</td><td>46.6</td><td>50.0</td></tr><tr><td>LLaMA2 13B</td><td>59.4</td><td>82.1</td><td>55.8</td><td>37.4</td><td>58.7</td></tr><tr><td>LoRA</td><td>60.8</td><td>82.8</td><td>56.0</td><td>46.5</td><td>61.5</td></tr><tr><td>LoRA+Prune</td><td>56.4</td><td>79.1</td><td>50.7</td><td>42.1</td><td>57.1</td></tr><tr><td>LLMPruner</td><td>46.8</td><td>74.0</td><td>24.7</td><td>34.8</td><td>45.1</td></tr><tr><td>APT</td><td>49.5</td><td>75.8</td><td>52.5</td><td>44.7</td><td>55.6</td></tr></table>\n\n![](images/66c92a147242de4eb085c6ad39b2613a7ea37961c867f8ad6ab8022640377e31.jpg)  \nTable 9. LLaMA2 7B and 13B $30 \\%$ sparsity pruning results with GPT4-generated Alpaca dataset, evaluated on the Open LLM leaderboard few-shot tasks.   \nFigure 4. The performance-efficiency tradeoff of APT compared to baseline methods. All metrics are normalized using LoRA tuning w/o pruning as the baseline. The circular dots with vertical axes on the left indicate training speed v.s. performance, with their sizes denoting the peak training memory usage. The squared dots with axes on the right indicate inference speedup v.s. performance, with sizes denoting inference memory usage.\n\nLoRA+Prune w/o retrain: we use Mask-Tuning to prune a LoRA-tuned converged model but do not conduct any retraining to recover the pruned models\u2019 performance. Therefore, the LM\u2019s training time will be reduced, yet its performance is lower than the LoRA $^ +$ Prune baseline.\n\nWith the same target sparsity in RoBERTa and LLaMA pruning setups, APT achieves on-par end-task performance with full fine-tuning and LoRA tuning baselines. Meanwhile, APT-tuned models reach similar or even better inference time and memory efficiency than existing baselines. APT-pruned T5 LMs\u2019 inference efficiency is slightly worse because more decoder parameters (with less computations happening) are pruned than the baselines. Moreover, when pruning RoBERTa and T5 models, APT achieves faster training time than all pruning and distillation baselines. Specifically, the training speed of APT in RoBERTa models is even higher than LoRA tuning without pruning. In LLaMA model pruning, APT costs significantly less training memory than both LLMPruner and LoRA $^ +$ Prune baselines.\n\n# F. Pruning Sparsity Analysis\n\nWe further show the task performance changing trajectory with different pruning sparsities in Figure 3. APT achieves superior inference speedup and less inference memory consumption than baselines targeting the same task performance. Compared to the LoRA $^ { + }$ Prune baseline, when pruning RoBERTa models targeting similar task accuracy, APT gains $2 1 . 8 \\%$ more inference speedup and $7 \\%$ more memory reduction. For T5 model pruning with $9 7 \\%$ dense model performance maintained, APT results in $6 2 . 7 \\%$ more inference speedup with $2 4 . 8 \\%$ more inference memory reduced compared to the\n\n![](images/2ca53ae85f9c0f03c956ece16770c1e7e04ab6c3a2b01e22ac6f987d4e91b83b.jpg)\n\n(a) Comparison of different initial ranks of LoRA layers pruning with APT on RoBERTa with SST2 task accuracy, relative training peak memory and speed to $9 7 \\%$ fine-tuning accuracy to the fine-tuning model.\n\n![](images/89705bc133ab7066fb806d5f344a7557e152352653cc72ffffa78c7ae65d36cd.jpg)  \n(b) Training initial sparsity trade-off with $30 \\%$ target sparsity model\u2019s relative performances to the LoRA-tuned LLaMA2-7B and 13B models.\n\nFigure 5. Detailed analysis in APT with different initial, target sparsities, and adaptive tuning schedules.\n\nLoRA $^ +$ Prune baseline. When pruning large LLaMA2-7B models, APT prunes gets $6 . 7 \\%$ more speedup and $9 . 2 \\%$ more inference memory reduction than the LoRA $^ +$ Prune baseline, with about $8 5 \\%$ dense model performance maintained.\n\n# G. Distillation Strategy Comparison\n\n<table><tr><td></td><td>SST2</td><td>Train. Speed(\u4ecb)</td><td>Train.Mem.(\u2193)</td></tr><tr><td>APT</td><td>94.5</td><td>16.9%</td><td>70.1%</td></tr><tr><td>w/o Llayer</td><td>93.7</td><td>17.4%</td><td>69.8%</td></tr><tr><td>w/o self-distillation</td><td>92.9</td><td>20.7%</td><td>69.2%</td></tr><tr><td>FT teacher</td><td>94.3</td><td>7.9%</td><td>111.8%</td></tr><tr><td>LoRA teacher</td><td>93.7</td><td>1.7%</td><td>96.1%</td></tr></table>\n\nTable 10. Ablation study of distillation strategies and comparison to non-efficient distillation techniques. The training speed and memory are relative metrics compared to fine-tuning the dense model.\n\nWe show the further analysis in Table 10 to compare the self-distillation technique we use in APT and traditional knowledge distillation methods. When ablating the dynamic layer mapping strategy in our self-distillation approach, the LM performance decreased by $0 . 8 \\%$ with similar training time and memory consumption. When training without distillation objectives (w/o self-distillation), the LM performance drops by $1 . 7 \\%$ . Nonetheless, the training is slightly faster with less memory costs. These results present that using distillation objectives for better LM task performance will sacrifice training efficiency as a tradeoff. Furthermore, we also demonstrate the comparisons with existing static knowledge distillation strategies, using the converged full-parameter fine-tuned LM (FT teacher) and LoRA-tuned LM (LoRA teacher) as the teacher model. We calculate the time consumption for both teacher and student training when using these distillation baselines. As shown in Table 10, using fully fine-tuned models as the teacher will incur more memory cost than dense model fine-tuning, while APT only consumes $70 \\%$ . In the meantime, the training convergence speed of APT training is two times faster than the traditional knowledge distillation method with a fine-tuned teacher. Furthermore, using a LoRA-tuned model as the teacher will result in extremely slow training speed. In addition, simply tuning the LoRA layers with knowledge distillation objectives doesn\u2019t help reduce the training memory consumption, as the memory consumption is still $9 6 . 1 \\%$ than full fine-tuning.\n\n# H. Adaptive Pruning and Tuning Analysis\n\nEffects of adaptive tuning strategies on end-task performance and training efficiency. As the trajectories shown in Figure 5a, simply enlarging the initial tuning parameter number in APT will not improve or even hurt the model\u2019s final performance. Moreover, the training memory consumption grows even higher than fine-tuning when the tuning layer ranks become extremely large (initial ranks set as 256). Therefore, this result proves that adding tuning parameters according to layer salience is better than uniformly increasing them before tuning.\n\nEffects of early pruning on task accuracy and training memory in LLaMA pruning. Figure 5b shows the effect of the initial density on LLaMA models\u2019 task performance under the $30 \\%$ sparsity pruning setting. We find that densely-trained models only perform better in TruthfulQA with fewer parameters pruned before tuning. The accuracy reaches 48.6 and 47.4 when not pruning before tuning, compared to 46.6 and 44.7 when directly pruning to the target sparsity for both 7B and 13B models. Training the LM densely harms the model performance while costing extra memory for all other tasks. These results demonstrate that pruning during training hurts large LM performance under distillation-free settings, and we hypothesize this is due to the training instability issue when parameters are set to zeros during fine-tuning.\n\n# I. Absolute Efficiency Metrics\n\nWe report the raw efficiency evaluation results in Table 11 and Table 12, including training and inference time and memory consumption. The training times are measured in seconds, and the inference times are measured in milliseconds. All memory footprints are measured in MB. We report the time-to-accuracy for RoBERTa and T5 model training to measure the training time. For LLaMA model training, we measure the training time per epoch to represent training time consumption.   \n\n<table><tr><td>Model</td><td>Method</td><td>Sparsity</td><td>97% TTA (s)</td><td>Train Mem. (MB)</td><td>Inf. Time (ms)</td><td>Inf. Mem (MB)</td></tr><tr><td rowspan=\"6\">RoBERTabase</td><td>FT</td><td>0%</td><td>127</td><td>2.696</td><td>220.8</td><td>1,157</td></tr><tr><td>LoRA</td><td>0%</td><td>2,714</td><td>1,630</td><td>181.8</td><td>1,157</td></tr><tr><td>LoRA+Prune</td><td>60%</td><td>6,513</td><td>1,630</td><td>84.0</td><td>869</td></tr><tr><td>Prune+Distill</td><td>60%</td><td>1,899</td><td>4,544</td><td>85.2</td><td>917</td></tr><tr><td>LoRA+Prune+Distill</td><td>60%</td><td>8,299</td><td>3,813</td><td>87.0</td><td>952</td></tr><tr><td>APT</td><td>60%</td><td>752</td><td>1,890</td><td>91.3</td><td>904</td></tr><tr><td rowspan=\"4\">T5base</td><td>FT</td><td>0%</td><td>366</td><td>7,217</td><td>248.1</td><td>2,347</td></tr><tr><td>LoRA</td><td>0%</td><td>935</td><td>4,476</td><td>254.2</td><td>2,347</td></tr><tr><td>LoRA+Prune</td><td>60%</td><td>14,417</td><td>4,476</td><td>116.8</td><td>1,724</td></tr><tr><td>APT</td><td>60%</td><td>1,774</td><td>5,332</td><td>185.0</td><td>1,913</td></tr></table>\n\nTable 11. Raw efficiency metrics, including time to accuracy, training peak memory, inference time and memory footprints, when using different methods to fine-tune $\\mathrm { R o B E R T a _ { b a s e } }$ and $\\mathrm { T 5 _ { b a s e } }$ models on SST2.   \n\n<table><tr><td>Method</td><td>Train Time (s)</td><td>Train Mem. (MB)</td><td>Inf. Time (ms)</td><td>Inf. Mem (MB)</td></tr><tr><td>LoRA</td><td>980</td><td>32,185</td><td>2457.5</td><td>45,311</td></tr><tr><td>LoRA+MT</td><td>980</td><td>32,185</td><td>2127.5</td><td>31,207</td></tr><tr><td>LoRA+MT+retrain</td><td>1,773</td><td>32,185</td><td>2127.5</td><td>31,207</td></tr><tr><td>LLMPruner</td><td>852</td><td>23,425</td><td>2140.6</td><td>33,625</td></tr><tr><td>APT</td><td>1,039</td><td>24,408</td><td>2099.7</td><td>30,469</td></tr></table>\n\nTable 12. Raw efficiency metrics, including time to accuracy, training peak memory, inference time, and memory footprints, when using different methods to fine-tune LLaMA2 7B models on Alpaca.\n\n-----\n\n## Overview of the plan\n# Reproduction Plan for APT: Adaptive Pruning and Tuning Pretrained Language Models\n\n## 1. Core Methodology Extraction\n\n### 1.1 Overall Framework\nAPT (Adaptive Pruning and Tuning) is a method that simultaneously prunes and tunes pretrained language models during fine-tuning to improve both training and inference efficiency. The key innovation is the dynamic adjustment of:\n- Pruning masks (removing unimportant parameters)\n- Tuning ranks (adding salient tuning parameters)\n\nThe framework operates by:\n1. Starting with an initial sparse configuration\n2. Dynamically pruning less important parameter blocks based on salience scores\n3. Dynamically increasing tuning capacity in more important layers\n4. Using self-distillation to maintain performance\n\n### 1.2 APT Adapter Architecture\nThe APT adapter builds upon LoRA but adds dynamic capabilities:\n\n**Mathematical Formulation:**\n```\nH_apt(X) = m_o \u2218 (W + s \u00b7 W_B W_A) X \u2218 m_i\n```\n\nWhere:\n- `m_i` \u2208 \u211d^d_i: binary input pruning mask\n- `m_o` \u2208 \u211d^d_o: binary output pruning mask  \n- `W_A` \u2208 \u211d^(r_apt \u00d7 d_i): low-rank decomposition matrix\n- `W_B` \u2208 \u211d^(d_o \u00d7 r_apt): low-rank decomposition matrix\n- `s`: scaling factor (constant)\n- `\u2218`: Hadamard product\n- `r_apt`: dynamic rank that can be increased during training\n\n**Implementation Details:**\n- Applied to queries and values in Multi-Head Attention (MHA) layers\n- Also applied to Feed-Forward Network (FFN) layers for smaller models (RoBERTa, T5)\n- Input mask `m_i` prunes transformer hidden dimensions\n- Output mask `m_o` prunes attention heads in MHA and neurons in FFN layers\n\n### 1.3 Adaptive Pruning (\ud835\udc9c\u209a)\n\n**Outlier-aware Salience Scoring:**\n```\n\u015c(W_:j) = \u03a3_(x,y\u2208\ud835\udc9f_t) \u03a3_i |\u2202\u2112/\u2202H_j,i| \u00b7 \u03a3_(x,y\u2208\ud835\udc9f_t) \u03a3_i |H_j,i| + (Kurt(O_j,:))^(1/2)\n```\n\nWhere:\n- First term: activation-gradient magnitude product (compressed across batches)\n- Second term: square root of kurtosis of activation O_j,: = W_:j \u2218 X_j,^T\n- Kurtosis captures outlier parameters important for task-specific capabilities\n\n**Efficient Search Algorithm:**\n1. Calculate salience score for each parameter block\n2. Sort blocks by salience density (salience / parameter count)\n3. Use binary search to identify top-salient blocks to retain given sparsity constraint \u03b3_t\n4. Gradually decrease pruning masks by \u03b1=0.01 instead of instant zeroing for stability\n\n### 1.4 Adaptive Tuning (\ud835\udc9c\u209c)\n\n**APT Adapter Salience Scoring:**\n```\n\ud835\udcaf(H_apt) = \u03a3_(i,j) S(W_Bi,j)\n```\nWhere S(W_Bi,j) = |W_Bi,j \u00b7 \u2202\u2112/\u2202W_Bi,j| is the standard salience score\n\n**Dynamic Rank Adjustment:**\n1. Calculate importance score \ud835\udcaf(H_apt) for each APT adapter\n2. Sort adapters by importance\n3. Increase ranks of top-half most salient adapters\n4. When increasing from \u0394_t to \u0394_t':\n   - New rank r'_apt = \u230ar_apt \u00b7 (\u0394_t' / \u0394_t)\u230b\n   - Initialize new parameters: concatenate random Gaussian N(0,\u03c3\u00b2) to W_A and zeros to W_B\n   - This preserves layer output before/after expansion\n\n### 1.5 Efficient Self-Knowledge Distillation\n\n**Distillation Objective:**\n```\n\u2112 = \u03bc\u2112_distill + (1-\u03bc)\u2112_ft\n\u2112_layer = \u03a3_(i=1)^\u03c4 MSE(Tr(H_s^\u03c6(i)), H_t^i)\n```\n\nKey features:\n- \u03bc linearly scales from 0 to 1 during training\n- Teacher model uses same parameters as student (self-distillation)\n- Only tuning layers are duplicated as teachers\n- Frozen parameters shared between teacher and student\n- \u03c4: number of randomly sampled teacher layers\n- \u03c6(\u00b7): teacher-student layer mapping function\n- Tr: tunable LoRA layer initialized as identity matrix\n\n## 2. Experimental Requirements\n\n### 2.1 Datasets\n\n**For BERT/RoBERTa/T5 Models:**\n- **GLUE Benchmark:**\n  - SST-2: Binary sentiment classification\n  - MNLI: Multi-genre Natural Language Inference\n  - QNLI, QQP, MRPC, CoLA, RTE, STS-B (mentioned in Appendix D)\n- **SQuAD v2.0:** Question answering with unanswerable questions\n- **CNN/DM:** Summarization dataset (for T5 models)\n\n**For LLaMA Models:**\n- **Alpaca Dataset:** GPT-4 generated instruction-following data\n- Evaluation on Open LLM Leaderboard tasks:\n  - 25-shot ARC (AI2 Reasoning Challenge)\n  - 10-shot HellaSwag\n  - 5-shot MMLU (Massive Multitask Language Understanding)\n  - Zero-shot TruthfulQA\n\n### 2.2 Model Architectures\n\n**Small-to-Medium Models:**\n- RoBERTa-base (12 layers, 768 hidden dim, 12 heads)\n- T5-base (encoder-decoder architecture)\n\n**Large Models:**\n- LLaMA-2 7B\n- LLaMA-2 13B (mentioned in Appendix D.3)\n\n### 2.3 Baseline Methods for Comparison\n\n**PEFT Methods:**\n- Full Fine-tuning (FT)\n- LoRA (Hu et al., 2022)\n\n**Pruning Methods:**\n- LoRA+Prune: Post-training pruning using Mask Tuning (Kwon et al., 2022)\n- Prune+Distill: CoFi (Xia et al., 2022) with L0 regularization\n- LoRA+Prune+Distill: CoFi pruning with only LoRA parameters tuned\n\n**State-of-the-Art Comparisons:**\n- LLMPruner (Ma et al., 2023): Task-agnostic pruning for LLaMA\n- PST (Li et al., 2022) and LRP (Zhang et al., 2023a): Additional baselines in Appendix D\n\n### 2.4 Hyperparameters and Training Settings\n\n**General Hyperparameters (from Table 6):**\n\n| Hyperparameter | GLUE-small | GLUE-big | SQuAD | CNN/DM | Alpaca |\n|----------------|------------|---------|-------|--------|--------|\n| Learning rate | 2e-4 | 2e-4 | 2e-4 | 1e-4 | 1e-4 |\n| Batch size | 32 | 32 | 32 | 16 | 32 |\n| Epochs | 40 | 40 | 40 | 16 | 15 |\n| Distill epochs | 20 | 20 | 20 | 6 | - |\n\n**APT-Specific Parameters:**\n- Initial adapter rank: 8\n- Scaling factor (s): 2 (static)\n- Target sparsity (\u03b3_T): Varies by experiment (60% for RoBERTa/T5, 30% for LLaMA)\n- Sparsity scheduling: Cubic schedule \u03b3_t = \u03b3_T + (1-\u03b3_T)(1-t/T)\u00b3\n- Momentum for salience averaging (\u03b2): Not specified \"\"\n- Pruning mask decay rate (\u03b1): 0.01\n- Distillation coefficient (\u03bc): Linearly increases from 0 to 1\n\n**Optimizer Settings:**\n- Standard Adam optimizer (implied, not explicitly stated) \"\"\n- Reset optimizer every time parameter sizes change (for stability)\n\n### 2.5 Implementation Architecture\n\n**Model Integration:**\n- Add APT adapters to:\n  - Query and value projections in MHA layers (all models)\n  - FFN layers (RoBERTa and T5 only, not mentioned for LLaMA)\n- For encoder-decoder models (T5), include cross-attention layers in decoder\n\n**Memory Management:**\n- Single A100 GPU for all experiments\n- Inference batch sizes:\n  - Small models: 128\n  - LLaMA 7B: 32\n  - LLaMA 13B: 4\n\n### 2.6 Evaluation Metrics\n\n**Training Efficiency Metrics:**\n- Relative training peak memory (Train Mem \u2193): Compared to full fine-tuning\n- Time to Accuracy (TTA): For small models, time to reach 97% of final accuracy\n- Training time per step: For large models (LLaMA)\n\n**Inference Efficiency Metrics:**\n- Inference peak memory (Inf Mem \u2193)\n- Inference speedup: Based on throughput (data processed per second)\n- Absolute metrics reported in Tables 11 and 12\n\n**Performance Metrics:**\n- Classification tasks: Accuracy\n- SQuAD v2.0: F1 score\n- CNN/DM: ROUGE-1/2/L scores\n- Large model evaluation: Average of four task scores\n\n## 3. Step-by-Step Implementation Plan\n\n### Step 1: Environment Setup\n1. Install PyTorch and Hugging Face Transformers library\n2. Set up single GPU environment (A100 as specified)\n3. Prepare dataset loading pipelines for all required datasets\n\n### Step 2: Base Model and Adapter Implementation\n1. Implement base transformer model classes with support for:\n   - Loading pretrained weights (RoBERTa, T5, LLaMA)\n   - Standard forward pass\n2. Implement APT adapter module with:\n   - Dynamic rank capability\n   - Input/output pruning masks\n   - Proper initialization (Gaussian for W_A, zero for W_B)\n   - Merge functionality for inference\n\n### Step 3: APT Adapter Integration\n1. Modify transformer layers to include APT adapters in:\n   - MHA query and value projections\n   - FFN layers (for RoBERTa and T5)\n2. Implement proper masking operations that respect pruning masks\n3. Ensure gradient flow only through unpruned parameters\n\n### Step 4: Adaptive Pruning Module\n1. Implement outlier-aware salience scoring function:\n   - Activation-gradient product calculation\n   - Kurtosis computation for outlier detection\n   - Batch compression to reduce memory\n2. Implement efficient search algorithm:\n   - Block sorting by salience density\n   - Binary search for sparsity-constrained selection\n   - Gradual mask updating mechanism\n\n### Step 5: Adaptive Tuning Module\n1. Implement APT adapter salience scoring:\n   - Per-parameter salience calculation\n   - Adapter-level importance aggregation\n2. Implement dynamic rank adjustment:\n   - Top-half adapter selection\n   - Rank interpolation formula\n   - Parameter initialization for new ranks\n   - Output-preserving expansion\n\n### Step 6: Self-Distillation Framework\n1. Implement teacher-student architecture with shared parameters:\n   - Duplicate tuning layers as teachers\n   - Shared frozen parameters\n2. Implement distillation objectives:\n   - Layer-wise MSE loss\n   - Dynamic layer mapping function \u03c6(\u00b7)\n   - Moving average coefficient \u03bc scheduling\n\n### Step 7: Training Loop Implementation\n1. Implement main training loop following Algorithm 1:\n   ```python\n   for t in range(T):\n       # Forward pass and loss computation\n       # Cache summed hidden states\n       # Backward pass\n       # Calculate approximated salience\n       # Update global scores with exponential moving average\n       # Select blocks via binary search against constraints\n       # Update masks gradually\n       # Apply parameter updates\n       \n       # Periodically adjust ranks based on adapter importance\n       # Reset optimizer when parameter counts change\n   ```\n\n### Step 8: Sparsity Scheduling\n1. Implement cubic sparsity scheduling:\n   ```\n   \u03b3_t = \u03b3_T + (1-\u03b3_T)(1-t/T)\u00b3\n   ```\n2. Schedule distillation coefficient \u03bc to increase linearly from 0 to 1\n\n### Step 9: Baseline Implementations\n1. Implement all comparison methods:\n   - LoRA (standard implementation)\n   - LoRA+Prune with Mask Tuning\n   - Prune+Distill (CoFi)\n   - LoRA+Prune+Distill\n   - LLMPruner (for LLaMA comparisons)\n\n### Step 10: Evaluation Framework\n1. Implement comprehensive evaluation suite:\n   - Training metrics collection (memory, time)\n   - Inference metrics collection (throughput, memory)\n   - Task-specific performance evaluation\n2. Create reporting system that matches paper's tables and figures\n\n### Step 11: Hyperparameter Configuration\n1. Set up configuration files matching Table 6 specifications\n2. Implement automatic logging of all hyperparameters\n3. Ensure reproducibility through fixed random seeds \"\"\n\n### Step 12: Experiment Execution\n1. Run experiments on specified datasets:\n   - RoBERTa on SST-2 and MNLI\n   - T5 on CNN/DM\n   - LLaMA on Alpaca\n2. Collect results for all baseline comparisons\n3. Generate all figures and tables presented in the paper\n\n## 4. Critical Ambiguities and Uncertainties\n\n1. **Optimizer Type**: The specific optimizer (Adam, AdamW, etc.) is not specified in the paper \"\"\n\n2. **Random Seeds**: The paper does not specify random seed settings for reproducibility \"\"\n\n3. **Gradient Accumulation**: Whether gradient accumulation is used for larger batch sizes is not mentioned \"\"\n\n4. **Weight Decay**: The value of weight decay, if any, is not specified \"\"\n\n5. **Salience Averaging Parameter**: The momentum parameter \u03b2 for exponential moving average of salience scores is not specified \"\"\n\n6. **Layer Mapping Function \u03c6(\u00b7)**: The exact implementation of the teacher-student layer mapping function is not detailed beyond \"closest non-pruned student layer\" \"\"\n\n7. **Block Definition**: The precise definition of parameter blocks for pruning (especially for gated FFNs in T5/LLaMA) could use more detail \"\"\n\n8. **Rank Initialization Variance**: The variance \u03c3\u00b2 for Gaussian initialization of new rank parameters is not specified \"\"\n\n9. **Multiple APT Adapters per Layer**: It's unclear if multiple APT adapters can exist in a single layer and how they're coordinated \"\"\n\n10. **Cross-Layer Dependencies**: How pruning decisions in one layer affect others isn't fully explained \"\"\n\nThis reproduction plan provides a comprehensive, step-by-step guide to implementing APT exactly as described in the paper, while clearly identifying areas where the original description lacks sufficient detail for perfect reproduction.\n\n-----\n\n## Design\n[EXAMPLE]\n{\n    \"file_list\": [\n        \"main.py\",\n        \"model/apt_adapter.py\",\n        \"model/base_model.py\",\n        \"pruning/adaptive_pruner.py\",\n        \"tuning/adaptive_tuner.py\",\n        \"distillation/self_distiller.py\",\n        \"train/trainer.py\",\n        \"eval/evaluator.py\",\n        \"data/dataloader.py\",\n        \"utils/salience.py\",\n        \"utils/mask_scheduler.py\",\n        \"config/hparams.py\"\n    ],\n    \"plan\": \"Step 1: Implement the APT adapter module with dynamic rank and pruning mask support. Step 2: Build base model integration for RoBERTa, T5, and LLaMA with APT adapters in MHA and FFN layers. Step 3: Implement outlier-aware salience scoring function using activation-gradient product and kurtosis. Step 4: Develop adaptive pruner that uses binary search over salience density to meet sparsity constraints. Step 5: Implement adaptive tuner that increases ranks in top-half salient adapters. Step 6: Build self-distillation framework with shared frozen parameters and tunable LoRA layers. Step 7: Create training loop with cubic sparsity scheduling and optimizer reset on shape change. Step 8: Implement data loaders for GLUE, SQuAD, CNN/DM, and Alpaca datasets. Step 9: Develop evaluation pipeline for all tasks and efficiency metrics. Step 10: Integrate all components in main execution script with proper configuration handling.\",\n    \"Code structure\": [\n        {\n            \"name\": \"model/apt_adapter\",\n            \"classes\": [\n                {\n                    \"name\": \"APTAdapter\",\n                    \"base_class\": \"nn.Module\",\n                    \"methods\": [\n                        {\n                            \"name\": \"__init__\",\n                            \"args\": [\"in_features: int\", \"out_features: int\", \"rank: int = 8\", \"scaling: float = 2.0\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"forward\",\n                            \"args\": [\"x: Tensor\", \"mask_in: Tensor = None\", \"mask_out: Tensor = None\"],\n                            \"returns\": \"Tensor\"\n                        },\n                        {\n                            \"name\": \"increase_rank\",\n                            \"args\": [\"new_rank: int\", \"init_type: str = 'lora'\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"merge_weights\",\n                            \"args\": [],\n                            \"returns\": \"None\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"pruning/adaptive_pruner\",\n            \"classes\": [\n                {\n                    \"name\": \"AdaptivePruner\",\n                    \"methods\": [\n                        {\n                            \"name\": \"__init__\",\n                            \"args\": [\"model: nn.Module\", \"sparsity_scheduler: SparsityScheduler\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"compute_salience\",\n                            \"args\": [\"activations: Dict\", \"gradients: Dict\"],\n                            \"returns\": \"Dict[str, Tensor]\"\n                        },\n                        {\n                            \"name\": \"compute_outlier_score\",\n                            \"args\": [\"activations: Tensor\"],\n                            \"returns\": \"Tensor\"\n                        },\n                        {\n                            \"name\": \"generate_mask\",\n                            \"args\": [\"salience_scores: Dict\", \"current_sparsity: float\"],\n                            \"returns\": \"Dict[str, Tensor]\"\n                        },\n                        {\n                            \"name\": \"apply_mask\",\n                            \"args\": [\"masks: Dict\"],\n                            \"returns\": \"None\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"tuning/adaptive_tuner\",\n            \"classes\": [\n                {\n                    \"name\": \"AdaptiveTuner\",\n                    \"methods\": [\n                        {\n                            \"name\": \"__init__\",\n                            \"args\": [\"model: nn.Module\", \"max_params: int\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"compute_adapter_salience\",\n                            \"args\": [\"named_parameters: Iterator\"],\n                            \"returns\": \"Dict[str, float]\"\n                        },\n                        {\n                            \"name\": \"adjust_ranks\",\n                            \"args\": [\"salience_dict: Dict\", \"target_params: int\"],\n                            \"returns\": \"int\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"distillation/self_distiller\",\n            \"classes\": [\n                {\n                    \"name\": \"SelfDistiller\",\n                    \"methods\": [\n                        {\n                            \"name\": \"__init__\",\n                            \"args\": [\"student_model: nn.Module\", \"layer_mapping: Callable\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"compute_distill_loss\",\n                            \"args\": [\"student_outputs: List[Tensor]\", \"teacher_outputs: List[Tensor]\"],\n                            \"returns\": \"Tensor\"\n                        },\n                        {\n                            \"name\": \"forward_teacher\",\n                            \"args\": [\"x: Tensor\"],\n                            \"returns\": \"List[Tensor]\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"train/trainer\",\n            \"classes\": [\n                {\n                    \"name\": \"APTrainer\",\n                    \"methods\": [\n                        {\n                            \"name\": \"__init__\",\n                            \"args\": [\"model: nn.Module\", \"train_dataloader: DataLoader\", \"val_dataloader: DataLoader\", \"hparams: dict\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"train_step\",\n                            \"args\": [\"batch: Tuple\"],\n                            \"returns\": \"Dict[str, float]\"\n                        },\n                        {\n                            \"name\": \"pruning_step\",\n                            \"args\": [],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"tuning_step\",\n                            \"args\": [],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"reset_optimizer\",\n                            \"args\": [],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"train\",\n                            \"args\": [],\n                            \"returns\": \"None\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"eval/evaluator\",\n            \"classes\": [\n                {\n                    \"name\": \"Evaluator\",\n                    \"methods\": [\n                        {\n                            \"name\": \"__init__\",\n                            \"args\": [\"model: nn.Module\", \"test_dataloader: DataLoader\", \"task: str\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"evaluate\",\n                            \"args\": [],\n                            \"returns\": \"Dict[str, float]\"\n                        },\n                        {\n                            \"name\": \"measure_efficiency\",\n                            \"args\": [],\n                            \"returns\": \"Dict[str, float]\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"data/dataloader\",\n            \"classes\": [\n                {\n                    \"name\": \"DatasetLoader\",\n                    \"methods\": [\n                        {\n                            \"name\": \"__init__\",\n                            \"args\": [\"dataset_name: str\", \"tokenizer: AutoTokenizer\", \"max_length: int\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"load_glue_dataset\",\n                            \"args\": [\"task_name: str\", \"split: str\"],\n                            \"returns\": \"Dataset\"\n                        },\n                        {\n                            \"name\": \"load_squad_dataset\",\n                            \"args\": [\"split: str\"],\n                            \"returns\": \"Dataset\"\n                        },\n                        {\n                            \"name\": \"load_cnndm_dataset\",\n                            \"args\": [\"split: str\"],\n                            \"returns\": \"Dataset\"\n                        },\n                        {\n                            \"name\": \"load_alpaca_dataset\",\n                            \"args\": [\"split: str\"],\n                            \"returns\": \"Dataset\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"utils/salience\",\n            \"functions\": [\n                {\n                    \"name\": \"compute_activation_gradient_product\",\n                    \"args\": [\"activations: Tensor\", \"gradients: Tensor\"],\n                    \"returns\": \"Tensor\"\n                },\n                {\n                    \"name\": \"compute_kurtosis\",\n                    \"args\": [\"tensor: Tensor\"],\n                    \"returns\": \"Tensor\"\n                },\n                {\n                    \"name\": \"normalize_scores\",\n                    \"args\": [\"scores: Dict\", \"param_counts: Dict\"],\n                    \"returns\": \"Dict\"\n                }\n            ]\n        },\n        {\n            \"name\": \"utils/mask_scheduler\",\n            \"classes\": [\n                {\n                    \"name\": \"SparsityScheduler\",\n                    \"methods\": [\n                        {\n                            \"name\": \"__init__\",\n                            \"args\": [\"total_steps: int\", \"target_sparsity: float\", \"schedule_type: str = 'cubic'\"],\n                            \"returns\": \"None\"\n                        },\n                        {\n                            \"name\": \"get_current_sparsity\",\n                            \"args\": [\"step: int\"],\n                            \"returns\": \"float\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"config/hparams\",\n            \"variables\": [\n                {\n                    \"name\": \"GLUE_HPARAMS\",\n                    \"type\": \"Dict\",\n                    \"description\": \"Hyperparameters for GLUE tasks\"\n                },\n                {\n                    \"name\": \"SQUAD_HPARAMS\",\n                    \"type\": \"Dict\",\n                    \"description\": \"Hyperparameters for SQuAD task\"\n                },\n                {\n                    \"name\": \"CNN_DM_HPARAMS\",\n                    \"type\": \"Dict\",\n                    \"description\": \"Hyperparameters for CNN/DM task\"\n                },\n                {\n                    \"name\": \"ALPACA_HPARAMS\",\n                    \"type\": \"Dict\",\n                    \"description\": \"Hyperparameters for Alpaca task\"\n                }\n            ]\n        }\n    ],\n    \"relationships\": [\n        {\"from\": \"APTrainer\", \"to\": \"APTAdapter\", \"type\": \"uses\"},\n        {\"from\": \"APTrainer\", \"to\": \"AdaptivePruner\", \"type\": \"uses\"},\n        {\"from\": \"APTrainer\", \"to\": \"AdaptiveTuner\", \"type\": \"uses\"},\n        {\"from\": \"APTrainer\", \"to\": \"SelfDistiller\", \"type\": \"uses\"},\n        {\"from\": \"AdaptivePruner\", \"to\": \"SparsityScheduler\", \"type\": \"uses\"},\n        {\"from\": \"AdaptivePruner\", \"to\": \"compute_activation_gradient_product\", \"type\": \"uses\"},\n        {\"from\": \"AdaptivePruner\", \"to\": \"compute_kurtosis\", \"type\": \"uses\"},\n        {\"from\": \"AdaptiveTuner\", \"to\": \"APTAdapter\", \"type\": \"uses\"},\n        {\"from\": \"SelfDistiller\", \"to\": \"APTAdapter\", \"type\": \"uses\"},\n        {\"from\": \"Evaluator\", \"to\": \"APTAdapter\", \"type\": \"uses\"},\n        {\"from\": \"DatasetLoader\", \"to\": \"APTAdapter\", \"type\": \"provides data to\"}\n    ],\n    \"pipeline or workflow\": \"The pipeline begins with loading pretrained models (RoBERTa, T5, LLaMA) and integrating APT adapters into designated layers. During training, the AdaptivePruner computes outlier-aware salience scores from activations and gradients, then generates pruning masks via binary search to meet scheduled sparsity constraints. Simultaneously, the AdaptiveTuner calculates adapter importance and increases ranks in top-salient layers. The SelfDistiller performs knowledge distillation using shared parameters between teacher and student. The APTrainer orchestrates these components with a main loop that includes forward pass, salience computation, pruning, tuning, and parameter updates, resetting the optimizer when shapes change. Evaluation measures both task performance and efficiency metrics (memory, speed). All experiments run on single A100 GPU with specified batch sizes and hyperparameters.\"\n}\n[/EXAMPLE]\n\n-----\n\n## Task\n[CONTENT]\n{\n    \"Required packages\": [\n        \"torch==2.0.1\",\n        \"transformers==4.35.0\",\n        \"datasets==2.14.6\",\n        \"accelerate==0.27.2\",\n        \"numpy==1.24.3\",\n        \"tqdm==4.66.1\"\n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"config/hparams.py\",\n            \"Contains global hyperparameters for all experiments as specified in Table 6 of the paper: learning rates, batch sizes, epochs, distillation epochs for GLUE-small, GLUE-big, SQuAD, CNN/DM, and Alpaca tasks. This file is foundational and imported by nearly all other modules to ensure consistent training settings across components.\"\n        ],\n        [\n            \"model/base_model.py\",\n            \"Implements base model loading using Hugging Face Transformers. Loads RoBERTa, T5, and LLaMA architectures with frozen pretrained weights. Exposes interfaces to inject APT adapters into specific layers (MHA query/value projections and FFN). Depends on hparams for model-specific configurations and serves as the backbone for all downstream modifications.\"\n        ],\n        [\n            \"model/apt_adapter.py\",\n            \"Defines the core APTAdapter class that extends LoRA with dynamic pruning and tuning capabilities. Implements forward pass with input/output masks (m_i, m_o), low-rank update via Wa and Wb, and rank adjustment logic. Must support merging weights post-training for inference efficiency. Imports from base_model to integrate with transformer layers. Critical dependency for pruner, tuner, and trainer modules.\"\n        ],\n        [\n            \"utils/salience.py\",\n            \"Implements outlier-aware salience scoring functions: compute_activation_gradient_product computes |\u2202L/\u2202H|\u00b7|H| summed over batches; compute_kurtosis calculates kurtosis of activation tensors O=W\u2218X^T to capture outlier importance. normalize_scores adjusts salience by parameter count for density-based ranking. These utilities are used by both AdaptivePruner and AdaptiveTuner for decision-making.\"\n        ],\n        [\n            \"pruning/adaptive_pruner.py\",\n            \"Implements AdaptivePruner class that uses salience scores to generate binary pruning masks. Key methods: compute_salience aggregates activation-gradient products and kurtosis; generate_mask sorts blocks by salience density and applies binary search to meet sparsity constraint \u03b3_t; apply_mask gradually updates masks (with \u03b1=0.01 decay) for stability. Depends on apt_adapter for mask application and salience utils for scoring. Outputs W_B pruning matrices.\"\n        ],\n        [\n            \"tuning/adaptive_tuner.py\",\n            \"Implements AdaptiveTuner class that dynamically increases adapter ranks. compute_adapter_salience sums parameter-level salience S(W_Bi,j) within each adapter to get layer importance \ud835\udcaf(H_apt); adjust_ranks identifies top-half most important adapters and scales their r_apt proportionally to budget \u0394_t. On expansion, initializes new Wa rows with N(0,\u03c3\u00b2) and new Wb columns with zeros to preserve output. Depends on apt_adapter and salience utils.\"\n        ],\n        [\n            \"utils/mask_scheduler.py\",\n            \"Implements SparsityScheduler with cubic scheduling: \u03b3_t = \u03b3_T + (1\u2212\u03b3_T)(1\u2212t/T)^3. Provides get_current_sparsity(step) method called by AdaptivePruner at each step. Configured via hparams (target_sparsity, total_steps). Ensures smooth transition to target sparsity without abrupt performance drops.\"\n        ],\n        [\n            \"distillation/self_distiller.py\",\n            \"Implements SelfDistiller using shared parameters between teacher and student. Uses tunable LoRA layers (Tr) initialized as identity for transformation. Computes MSE loss between teacher Tr(H_s^\u03c6(i)) and student H_t^i outputs across \u03c4 sampled layers. \u03c6(\u00b7) maps teacher to closest non-pruned student layer. \u03bc coefficient linearly increases from 0 to 1 during training. Shares frozen parameters with student model to reduce memory overhead.\"\n        ],\n        [\n            \"data/dataloader.py\",\n            \"Implements DatasetLoader supporting multiple datasets: GLUE tasks (SST-2, MNLI), SQuAD v2.0, CNN/DM, and Alpaca instruction-following data. Handles tokenization, batching (sizes defined in paper: 128 for small models, 32/4 for LLaMA), and formatting. Returns PyTorch-compatible datasets used by trainer and evaluator. Requires tokenizer from base_model.\"\n        ],\n        [\n            \"train/trainer.py\",\n            \"Implements APTrainer orchestrating full training loop (Algorithm 1). At each step: performs forward/backward passes, caches activations/gradients, calls AdaptivePruner.pruning_step() and AdaptiveTuner.tuning_step(), applies changes, updates parameters. Resets optimizer when parameter shapes change. Integrates SelfDistiller for distillation loss (\u03bc scheduling). Manages SparsityScheduler for \u03b3_t control. Main driver module depending on all others.\"\n        ],\n        [\n            \"eval/evaluator.py\",\n            \"Implements Evaluator measuring both task performance and efficiency metrics. evaluate() computes accuracy/F1/ROUGE/MMLU etc. measure_efficiency() records peak memory (via torch.cuda.max_memory_allocated), inference time (ms), and throughput. Metrics normalized to baselines (FT, LoRA) as in Tables 2\u20133. Used post-training to generate results matching paper tables.\"\n        ],\n        [\n            \"main.py\",\n            \"Entry point executing full pipeline: loads config \u2192 initializes model with APT adapters \u2192 sets up dataloaders \u2192 instantiates pruner/tuner/distiller \u2192 launches APTrainer.train() \u2192 runs final evaluation. Coordinates experiment flow across all components. Accepts CLI args for model type, dataset, sparsity level to reproduce different ablations.\"\n        ]\n    ],\n    \"Task list\": [\n        \"config/hparams.py\",\n        \"model/base_model.py\",\n        \"model/apt_adapter.py\",\n        \"utils/salience.py\",\n        \"utils/mask_scheduler.py\",\n        \"pruning/adaptive_pruner.py\",\n        \"tuning/adaptive_tuner.py\",\n        \"distillation/self_distiller.py\",\n        \"data/dataloader.py\",\n        \"train/trainer.py\",\n        \"eval/evaluator.py\",\n        \"main.py\"\n    ],\n    \"Full API spec\": \"\",\n    \"Shared Knowledge\": \"All modules share access to hparams.py for consistent hyperparameter usage. The APTAdapter class is central\u2014both AdaptivePruner and AdaptiveTuner modify its state (masks and ranks). Salience computation functions are reused across pruning and tuning decisions. Trainer manages shared state including current step t, which drives sparsity scheduling and distillation coefficient \u03bc. Memory efficiency is maintained through shared frozen parameters in self-distillation and compressed gradient/activation storage.\",\n    \"Anything UNCLEAR\": \"Clarification needed on: (1) exact variance \u03c3\u00b2 for Gaussian initialization of new adapter weights; (2) whether gradient accumulation is used for larger effective batch sizes; (3) specific Adam optimizer variants (AdamW?) and weight decay values; (4) random seed settings for reproducibility; (5) precise implementation of layer mapping function \u03c6(\u00b7) when layers are pruned; (6) handling of gated FFNs in T5/LLaMA during block-wise pruning; (7) whether APT adapters are applied to all MHA layers uniformly or selectively.\"\n}\n[/CONTENT]\n\n-----\n\n## Configuration file\n```yaml\n## config.yaml\ntraining:\n  learning_rate:\n    glue_small: 0.0002\n    glue_big: 0.0002\n    squad: 0.0002\n    cnn_dm: 0.0001\n    alpaca: 0.0001\n  batch_size:\n    glue_small: 32\n    glue_big: 32\n    squad: 32\n    cnn_dm: 16\n    alpaca: 32\n  epochs:\n    glue_small: 40\n    glue_big: 40\n    squad: 40\n    cnn_dm: 16\n    alpaca: 15\n  distill_epochs:\n    glue_small: 20\n    glue_big: 20\n    squad: 20\n    cnn_dm: 6\n    alpaca: null\n  target_sparsity: 0.6\n  initial_rank: 8\n  scaling_factor: 2.0\n  mask_decay_rate: 0.01\n  momentum_salience: 0.9\n  warmup_steps: null\n  optimizer: \"Adam\"\n  weight_decay: 0.0\n  max_grad_norm: 1.0\n\nmodel:\n  architectures:\n    - roberta-base\n    - t5-base\n    - llama-2-7b\n    - llama-2-13b\n  adapter:\n    apply_to_mha: true\n    apply_to_ffn: true\n    mha_components:\n      - q_proj\n      - v_proj\n    ffn_components:\n      - intermediate_dense\n      - output_dense\n\ndata:\n  datasets:\n    - sst2\n    - mnli\n    - qnli\n    - qqp\n    - mrpc\n    - cola\n    - rte\n    - stsb\n    - squad_v2\n    - cnn_dm\n    - alpaca\n  task_type:\n    classification: [sst2, mnli, qnli, qqp, mrpc, cola, rte]\n    qa: [squad_v2]\n    summarization: [cnn_dm]\n    instruction_following: [alpaca]\n  num_labels:\n    sst2: 2\n    mnli: 3\n    qnli: 2\n    qqp: 2\n    mrpc: 2\n    cola: 2\n    rte: 2\n    stsb: 1\n  max_length:\n    text: 512\n    question: 64\n    answer: 192\n    summary: 128\n    instruction: 2048\n\nevaluation:\n  inference_batch_size:\n    small_model: 128\n    llama_7b: 32\n    llama_13b: 4\n  metrics:\n    classification: accuracy\n    qa: f1\n    summarization: rouge\n    instruction_following: lm_eval_harness\n  eval_tasks:\n    open_llm_leaderboard:\n      - arc: 25-shot\n      - hellaswag: 10-shot\n      - mmlu: 5-shot\n      - truthfulqa: 0-shot\n  normalize_metrics: true\n  baseline_ft: true\n  baseline_lora: true\n\npruning:\n  scheduler: cubic\n  salience_type: outlier-aware\n  use_kurtosis: true\n  block_types:\n    - mha_head\n    - ffn_neuron\n    - hidden_dimension\n  binary_search: true\n  update_frequency: 100\n  min_sparsity: 0.0\n  target_sparsity: 0.6\n  gradual_mask_update: true\n\ntuning:\n  adaptive_rank_update: true\n  rank_increase_strategy: top_half\n  max_tuning_params_ratio: 0.1\n  init_std: 0.02\n\ndistillation:\n  use_self_distillation: true\n  share_frozen_params: true\n  teacher_layer_mapping: closest_non_pruned\n  num_teacher_layers: 6\n  distill_loss_weight_start: 0.0\n  distill_loss_weight_end: 1.0\n  distill_layers_sampled: 6\n  tr_layer_init: identity\n\nhardware:\n  gpu_type: A100\n  num_gpus: 1\n  distributed_training: false\n  precision: fp16\n```\n-----\n\n## Instruction\nConduct a Logic Analysis to assist in writing the code, based on the paper, the plan, the design, the task and the previously specified configuration file (config.yaml). \nYou DON'T need to provide the actual code yet; focus on a thorough, clear analysis.\n\nWrite the logic analysis in 'eval/evaluator.py', which is intended for 'Implements Evaluator measuring both task performance and efficiency metrics. evaluate() computes accuracy/F1/ROUGE/MMLU etc. measure_efficiency() records peak memory (via torch.cuda.max_memory_allocated), inference time (ms), and throughput. Metrics normalized to baselines (FT, LoRA) as in Tables 2\u20133. Used post-training to generate results matching paper tables.'.\n\n-----\n\n## Logic Analysis: eval/evaluator.py"}, {"role": "assistant", "content": "# Logic Analysis: eval/evaluator.py\n\nThe `Evaluator` class in `eval/evaluator.py` is responsible for measuring both task-specific performance and efficiency metrics after model training. This component plays a critical role in generating the results that directly compare APT against baselines as presented in Tables 2, 3, and other evaluation tables in the paper.\n\n## Core Responsibilities\n\n### 1. Task Performance Evaluation\nThe evaluator must compute appropriate metrics based on the task type, consistent with the paper's experimental setup:\n\n- **Classification tasks** (SST-2, MNLI, etc.): Accuracy\n- **Question Answering** (SQuAD v2.0): F1 score  \n- **Summarization** (CNN/DM): ROUGE-1, ROUGE-2, ROUGE-L scores\n- **Large Model Evaluation** (LLaMA on Alpaca): Average of four Open LLM Leaderboard tasks:\n  - 25-shot ARC\n  - 10-shot HellaSwag  \n  - 5-shot MMLU\n  - Zero-shot TruthfulQA\n\nThese metrics must be computed using the same methodology as described in Section 5.1 and evaluated through the lm-eval-harness package mentioned in the paper.\n\n### 2. Efficiency Metrics Measurement\nThe evaluator must measure both training and inference efficiency metrics as defined in Section 5.3:\n\n#### Training Efficiency Metrics:\n- **Relative training peak memory**: Compare to full fine-tuning baseline\n- **Time to Accuracy (TTA)**: For small models, time to reach 97% of final accuracy\n- **Training time per step**: For large models like LLaMA\n\n#### Inference Efficiency Metrics:\n- **Inference peak memory**: Measured via `torch.cuda.max_memory_allocated()`\n- **Inference speedup**: Based on throughput (data processed per second)\n- **Inference time**: Reported in milliseconds\n\nAll efficiency metrics should be normalized relative to baselines (FT and LoRA) as shown in Tables 2-3, with \"\u2193\" indicating smaller is better.\n\n## Implementation Details from config.yaml\n\nThe configuration file provides specific parameters that must be respected:\n\n```yaml\nevaluation:\n  inference_batch_size:\n    small_model: 128\n    llama_7b: 32\n    llama_13b: 4\n  metrics:\n    classification: accuracy\n    qa: f1\n    summarization: rouge\n    instruction_following: lm_eval_harness\n  eval_tasks:\n    open_llm_leaderboard:\n      - arc: 25-shot\n      - hellaswag: 10-shot\n      - mmlu: 5-shot\n      - truthfulqa: 0-shot\n  normalize_metrics: true\n  baseline_ft: true\n  baseline_lora: true\n```\n\nThis indicates:\n- Different batch sizes for different model sizes during inference testing\n- Specific metric types mapped to task categories\n- Use of lm-eval-harness for instruction-following evaluation\n- Normalization of metrics to baselines (FT and LoRA)\n\n## Method Breakdown\n\n### evaluate()\nThis method will:\n1. Iterate through the test dataset using the configured batch size\n2. Perform forward passes to generate predictions\n3. Compute task-specific metrics based on the task type\n4. Return a dictionary containing all relevant performance metrics\n5. For instruction-following tasks, interface with lm-eval-harness as specified\n\n### measure_efficiency()\nThis method will:\n1. Record GPU memory usage before and after inference using PyTorch's memory tracking\n2. Measure inference time by timing multiple forward passes and calculating average\n3. Calculate throughput (samples processed per second)\n4. Normalize all efficiency metrics according to the specified baselines\n5. Return efficiency metrics in the format required for comparison tables\n\n## Integration Requirements\n\nThe Evaluator must work seamlessly with:\n- The trained model containing APT adapters\n- The APTAdapter's merge_weights() functionality for inference optimization\n- DatasetLoader for proper data formatting\n- Configuration settings from config.yaml regarding batch sizes and evaluation parameters\n\n## Output Format\n\nThe output must match the structure of the paper's evaluation tables:\n- Performance metrics at the precision reported (e.g., 86.4 for MNLI)\n- Efficiency metrics normalized to baselines with appropriate directional indicators\n- Raw values available for reproduction (as referenced in Appendix I)\n\nThis evaluator will be crucial for demonstrating APT's claimed advantages: maintaining up to 98% task performance when pruning 60% of RoBERTa/T5 parameters while reducing training memory by up to 70% and speeding up fine-tuning by up to 8\u00d7."}]