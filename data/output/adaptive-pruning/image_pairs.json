{"0": {"image": "/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output/adaptive-pruning/2401.12200v2/auto/images/97be0e120351b3cd832a1193313d541c1047d0304a4663136a4dc679cd1a963a.jpg", "caption": "Figure 1. APT provides both training and inference efficiency benefits by pruning and tuning pretrained LM parameters adaptively via the APT adapter. We dynamically adjust (add/reduce) APT adapter input/output dimensions and the rank $( r _ { \\mathrm { a p t } } )$ . Reducing adapter dimensions prunes frozen parameters, making training and inference faster and more memory-efficient. Adding adapter ranks helps recover the pruned LM\u2019s task performance. In contrast, existing adapters like LoRA allow efficient training but do not provide inference efficiency since the model size is not reduced."}, "1": {"image": "/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output/adaptive-pruning/2401.12200v2/auto/images/7a784ac712151bf50d4c32a16f607ec7d880b2d6b2af934f5de79e54ad065f5d.jpg", "caption": "Figure 2. APT adaptively identifies pruning and tuning parameters via APT adapters during fine-tuning with little cost. APT gradually prunes LM parameters with binary pruning masks learned from our lightweight outlier-aware salience scoring function for training and inference efficiency. APT also adds tuning parameters in salient layers in LM fine-tuning through increasing dynamic ranks in APT adapters for performance recovery."}, "2": {"image": "/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output/adaptive-pruning/2401.12200v2/auto/images/b69b2f909dafdfc22ab1883d7fda78032988349319a7552876765648cc9c7785.jpg", "caption": "Figure 3. Task performance v.s. relative inference efficiency on RoBERTa, T5, and LLaMA-2 7B models with APT and baselines."}, "3": {"image": "/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output/adaptive-pruning/2401.12200v2/auto/images/66c92a147242de4eb085c6ad39b2613a7ea37961c867f8ad6ab8022640377e31.jpg", "caption": "Table 9. LLaMA2 7B and 13B $30 \\%$ sparsity pruning results with GPT4-generated Alpaca dataset, evaluated on the Open LLM leaderboard few-shot tasks.  Figure 4. The performance-efficiency tradeoff of APT compared to baseline methods. All metrics are normalized using LoRA tuning w/o pruning as the baseline. The circular dots with vertical axes on the left indicate training speed v.s. performance, with their sizes denoting the peak training memory usage. The squared dots with axes on the right indicate inference speedup v.s. performance, with sizes denoting inference memory usage."}, "4": {"image": "/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output/adaptive-pruning/2401.12200v2/auto/images/2ca53ae85f9c0f03c956ece16770c1e7e04ab6c3a2b01e22ac6f987d4e91b83b.jpg", "caption": ""}, "5": {"image": "/Users/v1p0r/Documents/DDL_Fall25/term_project/protopy_agent/data/output/adaptive-pruning/2401.12200v2/auto/images/89705bc133ab7066fb806d5f344a7557e152352653cc72ffffa78c7ae65d36cd.jpg", "caption": "(b) Training initial sparsity trade-off with $30 \\%$ target sparsity model\u2019s relative performances to the LoRA-tuned LLaMA2-7B and 13B models."}}