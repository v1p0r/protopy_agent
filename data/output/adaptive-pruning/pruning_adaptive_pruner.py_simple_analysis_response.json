[{"id": "chatcmpl-dcda07f0-d9a8-4797-a91f-fc39f8cb1806", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "# Logic Analysis: `pruning/adaptive_pruner.py`\n\n## Overview\nThe `AdaptivePruner` class is responsible for dynamically identifying and removing unimportant parameter blocks from the language model during fine-tuning. It implements the core pruning functionality of APT by computing outlier-aware salience scores, determining which blocks to prune based on sparsity constraints, and gradually applying pruning masks for training stability.\n\n## Dependencies and Configuration\n- Depends on `config.yaml` for key parameters:\n  - `pruning.scheduler`: Specifies cubic scheduling for sparsity progression\n  - `pruning.salience_type`: Confirms use of \"outlier-aware\" salience scoring\n  - `pruning.use_kurtosis`: Enables kurtosis calculation for outlier detection\n  - `pruning.target_sparsity`: Target sparsity level (0.6 per config)\n  - `pruning.update_frequency`: How often to update masks (every 100 steps)\n  - `model.adapter.mha_components` and `ffn_components`: Identifies where adapters are applied\n- Imports from `utils.salience` for activation-gradient product and kurtosis calculations\n- Interfaces with `APTAdapter` to apply masks to input/output dimensions\n\n## Core Components\n\n### 1. Salience Computation (`compute_salience`)\nThis method calculates the outlier-aware salience score as defined in Equation (5) of the paper:\n\n```\n\u015c(W_:j) = \u03a3_(x,y\u2208\ud835\udc9f_t) \u03a3_i |\u2202\u2112/\u2202H_j,i| \u00b7 \u03a3_(x,y\u2208\ud835\udc9f_t) \u03a3_i |H_j,i| + (Kurt(O_j,:))^(1/2)\n```\n\n**Implementation Steps:**\n1. **Activation-Gradient Product**: For each parameter block, compute the magnitude of the product between activations and their gradients, summed across batches to reduce memory overhead.\n   - Extract activations H from forward pass\n   - Obtain gradients \u2202\u2112/\u2202H from backward pass\n   - Compute element-wise product |\u2202\u2112/\u2202H|\u00b7|H| and sum over batch and sequence dimensions\n\n2. **Kurtosis Calculation**: Compute kurtosis of the activation tensor O_j,: = W_:j \u2218 X_j,^T to capture outlier importance.\n   - Use `utils.salience.compute_kurtosis()` function\n   - Take square root of kurtosis value as in paper equation\n   - This preserves more outlier parameters that are crucial for task-specific capabilities\n\n3. **Score Aggregation**: Combine both components into final salience score.\n   - Sum activation-gradient product term\n   - Add square root of kurtosis term\n   - Store scores for each block type (MHA heads, FFN neurons, hidden dimensions)\n\n4. **Block Identification**: Identify parameter blocks based on configuration:\n   - MHA heads in query and value projections (per `mha_components`)\n   - FFN neurons in intermediate and output dense layers (per `ffn_components`)\n   - Hidden dimension size affecting all layers\n\n### 2. Mask Generation (`generate_mask`)\nThis method determines which blocks to retain/prune given current sparsity requirements.\n\n**Implementation Steps:**\n1. **Retrieve Current Sparsity**: Get \u03b3_t from scheduler using cubic schedule:\n   ```\n   \u03b3_t = \u03b3_T + (1\u2212\u03b3_T)(1\u2212t/T)^3\n   ```\n   - Access via external scheduler object initialized with parameters from config\n\n2. **Calculate Salience Density**: For each block, compute salience density = salience / parameter_count.\n   - Parameter count varies by block type:\n     - MHA head: 4 \u00d7 d_model \u00d7 d_head\n     - FFN neuron: 2 \u00d7 d_model  \n     - Hidden dimension: n_layers \u00d7 (4\u00d7d_model + 2\u00d7ffn_size)\n\n3. **Sort Blocks**: Sort all blocks by salience density in descending order.\n   - Higher density indicates more important parameters per parameter cost\n\n4. **Binary Search for Constraint Satisfaction**: \n   - Goal: Find minimum number of top-salient blocks to retain while meeting \u03b3_t constraint\n   - Use binary search over sorted list to efficiently identify cutoff point\n   - Calculate total parameter count of retained blocks\n   - Ensure sparsity \u2265 \u03b3_t as required by Equation (1)\n\n5. **Generate Binary Masks**: Create m_i (input) and m_o (output) masks.\n   - Set mask values to 1 for retained blocks, 0 for pruned blocks\n   - Structure masks to align with adapter architecture\n\n### 3. Mask Application (`apply_mask`)\nHandles gradual application of pruning masks for training stability.\n\n**Implementation Steps:**\n1. **Gradual Update**: Instead of instant zeroing, update masks incrementally.\n   - Decrease pruned block masks by \u03b1=0.01 per update (from config.mask_decay_rate)\n   - Increase retained block masks toward 1.0 similarly\n   - Ensures smooth transition and prevents abrupt performance drops\n\n2. **Mask Propagation**: Apply masks to appropriate adapter components.\n   - Input mask m_i affects hidden dimension pruning\n   - Output mask m_o affects attention heads (MHA) and neurons (FFN)\n   - Coordinate with APTAdapter's forward method for proper masking\n\n3. **Shape Maintenance**: Ensure mask shapes remain compatible with model architecture.\n   - Handle dynamic changes in layer sizes due to previous pruning rounds\n   - Validate mask dimensions before application\n\n## Integration Points\n\n### With Training Loop\n- Called periodically during training (every `update_frequency` steps)\n- Receives activations and gradients from trainer after backward pass\n- Returns updated masks to be applied before next forward pass\n- Interacts with optimizer through trainer (optimizer reset when shapes change)\n\n### With Other Components\n- **APTAdapter**: Applies generated masks to control effective model size\n- **SparsityScheduler**: Gets target sparsity level for current training step\n- **SelfDistiller**: Pruning affects teacher-student alignment; non-pruned layers guide mapping function \u03c6(\u00b7)\n- **AdaptiveTuner**: Pruning decisions influence which layers receive additional tuning capacity\n\n## Special Considerations\n\n### Block Definition for Different Architectures\n- **RoBERTa/T5**: Standard transformer blocks with specified MHA and FFN components\n- **LLaMA**: May require adjustments for rotary embeddings and RMSNorm, but adapter placement remains consistent\n- **T5 Encoder-Decoder**: Must handle both encoder and decoder sides, including cross-attention layers\n\n### Memory Efficiency\n- Compress activation/gradient data by summing over batches\n- Reuse cached values when possible\n- Avoid storing full Jacobian matrices\n- Leverage mixed precision (fp16) as specified in hardware config\n\n### Stability Mechanisms\n- Gradual mask updates prevent sudden performance degradation\n- Exponential moving average for salience scores (using momentum_salience=0.9 from config)\n- Reset optimizer state when significant shape changes occur\n- Maintain minimum sparsity threshold to avoid over-pruning\n\n## Edge Cases and Validation\n- Handle cases where no blocks meet salience threshold\n- Validate that final sparsity meets or exceeds \u03b3_t requirement\n- Ensure at least one head/neuron remains in each layer\n- Check for numerical stability in kurtosis calculation\n- Verify mask compatibility with merged adapter weights post-training", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": null}}], "created": 1764811068, "model": "qwen-plus", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 1511, "prompt_tokens": 38976, "total_tokens": 40487, "completion_tokens_details": null, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0}}}]