[{"id": "chatcmpl-6d58858b-540f-4617-807e-7f9ef47a09ef", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "# Logic Analysis: data/dataloader.py\n\n## Core Responsibilities\nThe `DatasetLoader` class in `data/dataloader.py` is responsible for:\n1. Loading and preprocessing multiple datasets (GLUE, SQuAD, CNN/DM, Alpaca)\n2. Applying appropriate tokenization using the model's tokenizer\n3. Formatting data according to task-specific requirements\n4. Creating PyTorch-compatible datasets with proper batching\n5. Handling different maximum sequence lengths based on data type\n6. Supporting both training and evaluation modes\n\n## Dataset-Specific Processing Logic\n\n### GLUE Tasks\n- **Supported tasks**: SST-2, MNLI, QNLI, QQP, MRPC, CoLA, RTE, STS-B\n- **Processing flow**:\n  1. Load dataset using Hugging Face `datasets` library\n  2. For single-sentence tasks (SST-2, CoLA): tokenize input text only\n  3. For sentence-pair tasks (MNLI, QNLI, QQP, MRPC, RTE): tokenize both sentences with proper separator tokens\n  4. Apply truncation/padding to `max_length.text` (512) from config\n  5. Convert labels to appropriate format (integer for classification, float for regression in STS-B)\n\n### SQuAD v2.0\n- **Processing flow**:\n  1. Load dataset with both context/question pairs and answer spans\n  2. Tokenize using special handling for question answering:\n     - Concatenate question and context with proper separators\n     - Use `max_length.question` (64) for questions\n     - Use `max_length.text` (512) for combined question+context\n  3. Handle unanswerable questions by setting start/end positions to [CLS] token\n  4. Create multiple features per example if context exceeds max length (sliding window approach)\n  5. Include is_impossible flag for determining answerability\n\n### CNN/DM Summarization\n- **Processing flow**:\n  1. Load article-summary pairs\n  2. Tokenize articles using `max_length.text` (512)\n  3. Tokenize summaries using `max_length.summary` (128)\n  4. Format as encoder-decoder task for T5 models:\n     - Input: \"summarize: {article}\"\n     - Target: \"{summary}\"\n  5. Apply padding/truncation appropriately for both input and target sequences\n\n### Alpaca Instruction-Following\n- **Processing flow**:\n  1. Load instruction, input (optional), and output triples\n  2. Format examples using instruction-following template:\n     ```\n     Below is an instruction that describes a task. Write a response that appropriately completes the request.\n     \n     ### Instruction:\n     {instruction}\n     \n     ### Input:\n     {input}\n     \n     ### Response:\n     {output}\n     ```\n  3. Tokenize using `max_length.instruction` (2048) from config\n  4. Handle cases where input field is empty or null\n  5. Ensure proper formatting for LLaMA tokenizer (no extra whitespace issues)\n\n## Batching Strategy\n- **Batch size determination**:\n  - Training: Use values from `training.batch_size` in config.yaml\n  - Inference: Use values from `evaluation.inference_batch_size` based on model size:\n    - Small models (RoBERTa, T5): 128\n    - LLaMA-2 7B: 32\n    - LLaMA-2 13B: 4\n- **Dynamic batching considerations**:\n  - Implement padding within batches to minimize wasted computation\n  - Consider using `DataCollatorWithPadding` from Transformers library\n  - For variable-length sequences, sort examples by length within each batch when possible\n\n## Integration Points\n- **Tokenizer dependency**: Requires tokenizer from `model/base_model.py`\n- **Configuration dependency**: Uses hyperparameters from `config/hparams.py` and `config.yaml`\n- **Output compatibility**: Must produce datasets compatible with `train/trainer.py` and `eval/evaluator.py`\n\n## Implementation Details\n- **Class structure**: Single `DatasetLoader` class with multiple loader methods\n- **Error handling**: Robust error handling for missing fields, encoding issues, etc.\n- **Memory efficiency**: Stream data when possible rather than loading entire datasets into memory\n- **Caching**: Implement caching mechanism to avoid reprocessing datasets on subsequent runs\n- **Flexibility**: Support easy addition of new datasets through modular design\n\n## Special Considerations from Paper\n- The paper specifies single A100 GPU usage, so dataloading should be optimized for single-GPU performance\n- Inference batch sizes are specifically called out in Section 5.3, so these must be strictly followed\n- For Alpaca dataset, ensure compatibility with lm-eval-harness package used for evaluation\n- Maintain consistency with hyperparameter settings in Table 6 of the paper", "refusal": null, "role": "assistant", "annotations": null, "audio": null, "function_call": null, "tool_calls": null}}], "created": 1764811264, "model": "qwen-plus", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 1054, "prompt_tokens": 38972, "total_tokens": 40026, "completion_tokens_details": null, "prompt_tokens_details": {"audio_tokens": null, "cached_tokens": 0}}}]