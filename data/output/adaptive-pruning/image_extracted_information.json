["{\n  \"category\": \"model_architecture\",\n  \"extracted\": {\n    \"components\": [\n      {\n        \"name\": \"Frozen params\",\n        \"type\": \"model\",\n        \"role\": \"Pretrained language model parameters that are not updated during training\"\n      },\n      {\n        \"name\": \"LoRA adapter\",\n        \"type\": \"adapter\",\n        \"role\": \"Low-Rank Adaptation module with weight matrices Wa and Wb, used for efficient fine-tuning\"\n      },\n      {\n        \"name\": \"APT adapter\",\n        \"type\": \"adapter\",\n        \"role\": \"Adaptive Pruning and Tuning adapter with weight matrices Wa and Wb, dynamically adjustable rank r_apt and input/output dimensions\"\n      },\n      {\n        \"name\": \"Wa\",\n        \"type\": \"weight matrix\",\n        \"role\": \"Adapter weight matrix (low-rank update component)\"\n      },\n      {\n        \"name\": \"Wb\",\n        \"type\": \"weight matrix\",\n        \"role\": \"Adapter weight matrix (low-rank update component)\"\n      }\n    ],\n    \"flows\": [\n      {\n        \"source\": \"x\",\n        \"target\": \"LoRA adapter\",\n        \"data\": \"input token embeddings or hidden states\"\n      },\n      {\n        \"source\": \"LoRA adapter\",\n        \"target\": \"Frozen params\",\n        \"data\": \"fused output: Frozen params + LoRA update (Wa * H * Wb)\"\n      },\n      {\n        \"source\": \"x\",\n        \"target\": \"APT adapter\",\n        \"data\": \"input token embeddings or hidden states\"\n      },\n      {\n        \"source\": \"APT adapter\",\n        \"target\": \"Frozen params\",\n        \"data\": \"fused output: Frozen params + APT update (Wa * H * Wb) with dynamic rank r_apt\"\n      }\n    ],\n    \"stages\": [\n      {\n        \"name\": \"Training/Inference\",\n        \"description\": \"Input x is processed through either LoRA or APT adapter, which injects low-rank updates into the frozen pretrained model. APT allows dynamic adjustment of adapter dimensions and rank to prune or recover performance.\"\n      }\n    ],\n    \"inputs\": [\n      \"x\"\n    ],\n    \"outputs\": [\n      \"output from Frozen params + adapter update\"\n    ]\n  }\n}", "{\n  \"category\": \"model_architecture\",\n  \"extracted\": {\n    \"components\": [\n      {\n        \"name\": \"Frozen params\",\n        \"type\": \"layer block\",\n        \"role\": \"Fixed parameters from teacher model, not updated during training\"\n      },\n      {\n        \"name\": \"APT adapter\",\n        \"type\": \"adapter module\",\n        \"role\": \"Dynamically adjusts pruning and tuning parameters via binary masks and rank adaptation\"\n      },\n      {\n        \"name\": \"W_B\",\n        \"type\": \"pruning weight matrix\",\n        \"role\": \"Binary mask weights for pruning, derived from salience scoring\"\n      },\n      {\n        \"name\": \"W_A\",\n        \"type\": \"tuning weight matrix\",\n        \"role\": \"Adaptive tuning weights for salient layers, with dynamic rank r_apt\"\n      },\n      {\n        \"name\": \"Outlier-aware salience scoring\",\n        \"type\": \"scoring function\",\n        \"role\": \"Computes importance scores of hidden states to guide pruning and tuning\"\n      },\n      {\n        \"name\": \"Adaptive Pruning\",\n        \"type\": \"module\",\n        \"role\": \"Generates pruning masks using fast block search based on salience scores\"\n      },\n      {\n        \"name\": \"Adaptive Tuning\",\n        \"type\": \"module\",\n        \"role\": \"Increases dynamic rank in adapters for performance recovery in salient layers\"\n      }\n    ],\n    \"flows\": [\n      {\n        \"source\": \"Hidden states\",\n        \"target\": \"Outlier-aware salience scoring\",\n        \"data\": \"Activation values from intermediate layers\"\n      },\n      {\n        \"source\": \"Outlier-aware salience scoring\",\n        \"target\": \"Adaptive Pruning\",\n        \"data\": \"Salience scores \u2192 Pruning mask\"\n      },\n      {\n        \"source\": \"Adaptive Pruning\",\n        \"target\": \"W_B\",\n        \"data\": \"Binary pruning mask\"\n      },\n      {\n        \"source\": \"Hidden states\",\n        \"target\": \"Salience calculation\",\n        \"data\": \"Activation values\"\n      },\n      {\n        \"source\": \"Salience calculation\",\n        \"target\": \"Adaptive Tuning\",\n        \"data\": \"Salience scores \u2192 Layer rank r\"\n      },\n      {\n        \"source\": \"Adaptive Tuning\",\n        \"target\": \"W_A\",\n        \"data\": \"Tuning parameters (increased rank)\"\n      },\n      {\n        \"source\": \"Frozen params\",\n        \"target\": \"APT adapter\",\n        \"data\": \"Input X + pruning mask + tuning parameters\"\n      },\n      {\n        \"source\": \"APT adapter\",\n        \"target\": \"Final output\",\n        \"data\": \"Pruned + tuned model output\"\n      }\n    ],\n    \"stages\": [\n      {\n        \"name\": \"Pruning Phase\",\n        \"description\": \"Compute salience scores \u2192 generate pruning mask \u2192 apply binary mask to freeze non-salient parameters\"\n      },\n      {\n        \"name\": \"Tuning Phase\",\n        \"description\": \"Calculate salience per layer \u2192 increase adapter rank in salient layers \u2192 add tuning parameters for performance recovery\"\n      },\n      {\n        \"name\": \"Integration\",\n        \"description\": \"Merge pruned frozen parameters with adaptive tuning adapters to form final APT-enhanced model\"\n      }\n    ],\n    \"inputs\": [\n      \"Teacher model parameters\",\n      \"Input data X\",\n      \"Hidden states from teacher model\"\n    ],\n    \"outputs\": [\n      \"Pruned and tuned language model with reduced parameter count\",\n      \"Dynamic rank-adapted adapter weights W_A\",\n      \"Binary pruning mask W_B\"\n    ]\n  }\n}"]