# Logic Analysis: train/trainer.py

The `APTrainer` class is the central orchestrator of the APT framework, responsible for coordinating all components during training. It implements Algorithm 1 from the paper and manages the complex interaction between adaptive pruning, tuning, and self-distillation.

## Core Responsibilities

### 1. Training Loop Orchestration
The trainer follows a structured loop that integrates multiple dynamic processes:
- Forward and backward passes for gradient computation
- Salience score calculation and accumulation
- Adaptive pruning decisions based on current sparsity target
- Adaptive tuning adjustments based on layer importance
- Parameter updates with proper optimizer handling
- Distillation loss integration with scheduled weighting

### 2. Component Integration
The trainer serves as the primary interface between all major components:
- **Model**: Uses base model with integrated APT adapters
- **Pruner**: Calls `AdaptivePruner` to generate and apply pruning masks
- **Tuner**: Invokes `AdaptiveTuner` to adjust adapter ranks
- **Distiller**: Coordinates self-distillation objectives
- **Scheduler**: Manages sparsity progression via `SparsityScheduler`
- **Optimizer**: Handles parameter updates and resets when shapes change

## Detailed Workflow

### Initialization Phase
```python
def __init__(self, model, train_dataloader, val_dataloader, config):
    # Load configuration from config.yaml
    self.config = config
    
    # Initialize core components
    self.model = model
    self.train_loader = train_dataloader
    self.val_loader = val_dataloader
    
    # Initialize pruner with sparsity scheduler
    self.sparsity_scheduler = SparsityScheduler(
        total_steps=config.training.epochs * len(train_loader),
        target_sparsity=config.pruning.target_sparsity,
        schedule_type=config.pruning.scheduler
    )
    self.pruner = AdaptivePruner(self.model, self.sparsity_scheduler)
    
    # Initialize tuner with parameter budget
    self.tuner = AdaptiveTuner(
        self.model, 
        max_params=int(sum(p.numel() for p in model.parameters()) * 
                      config.tuning.max_tuning_params_ratio)
    )
    
    # Initialize distiller if enabled
    if config.distillation.use_self_distillation:
        self.distiller = SelfDistiller(
            self.model,
            layer_mapping_strategy=config.distillation.teacher_layer_mapping
        )
    
    # Setup optimizer (Adam as specified in config)
    self.optimizer = torch.optim.Adam(
        [p for n, p in model.named_parameters() if p.requires_grad],
        lr=config.training.learning_rate,
        weight_decay=config.training.weight_decay
    )
    
    # Track state variables
    self.global_step = 0
    self.current_param_count = sum(p.numel() for p in model.parameters())
```

### Main Training Step
The `train_step()` method executes one complete training iteration:

#### Forward Pass & Activation Caching
```python
# Perform forward pass
outputs = self.model(batch)
loss_ft = outputs.loss

# Cache summed hidden states (compressed across batches)
# As described in Algorithm 1, line 4
activations = {}
for name, act in self.model.get_cached_activations():
    if name not in activations:
        activations[name] = torch.zeros_like(act.sum((0,1)))
    activations[name] += act.sum((0,1))
```

#### Backward Pass & Gradient Collection
```python
# Compute total loss (including distillation if applicable)
if self.config.distillation.use_self_distillation:
    # Generate teacher outputs using shared parameters
    with torch.no_grad():
        teacher_outputs = self.distiller.forward_teacher(batch)
    
    # Compute distillation loss
    distill_loss = self.distiller.compute_distill_loss(
        student_outputs=outputs.hidden_states,
        teacher_outputs=teacher_outputs
    )
    
    # Combine losses with linearly increasing μ
    mu = min(1.0, self.global_step / (self.total_steps * 0.8))  # Linear ramp
    total_loss = mu * distill_loss + (1 - mu) * loss_ft
else:
    total_loss = loss_ft

# Backward pass
total_loss.backward()

# Collect gradients for salience computation
gradients = {}
for name, param in self.model.named_parameters():
    if param.grad is not None:
        gradients[name] = param.grad.detach().clone()
```

#### Adaptive Pruning Step
```python
# Execute pruning step every update_frequency steps
if self.global_step % self.config.pruning.update_frequency == 0:
    # Compute current sparsity target
    current_sparsity = self.sparsity_scheduler.get_current_sparsity(
        self.global_step
    )
    
    # Calculate outlier-aware salience scores
    salience_scores = self.pruner.compute_salience(activations, gradients)
    
    # Generate new pruning masks
    new_masks = self.pruner.generate_mask(salience_scores, current_sparsity)
    
    # Apply masks gradually for stability (α=0.01)
    self.pruner.apply_mask(new_masks)
    
    # Update activation cache reference
    self.model.clear_cached_activations()
```

#### Adaptive Tuning Step
```python
# Execute tuning adjustment periodically
if self.global_step % self.config.tuning.update_frequency == 0:
    # Calculate adapter importance scores
    adapter_salience = self.tuner.compute_adapter_salience(
        self.model.named_parameters()
    )
    
    # Determine target parameter count
    target_params = self.config.tuning.max_tuning_params_ratio * \
                   self.current_param_count
    
    # Adjust ranks in top-half most salient adapters
    param_change = self.tuner.adjust_ranks(adapter_salience, target_params)
    
    # Reset optimizer if parameter shapes changed
    if param_change != 0:
        self.reset_optimizer()
        self.current_param_count += param_change
```

#### Parameter Update & State Management
```python
# Clip gradients
torch.nn.utils.clip_grad_norm_(
    self.model.parameters(), 
    self.config.training.max_grad_norm
)

# Update parameters
self.optimizer.step()
self.optimizer.zero_grad()

# Increment step counter
self.global_step += 1
```

## Critical Design Considerations

### Optimizer Reset Mechanism
When adapter ranks are increased or pruning masks applied, the parameter shapes change. The trainer must handle this gracefully:
```python
def reset_optimizer(self):
    """Reset optimizer state when parameter shapes change"""
    # Store current parameter references
    param_groups = []
    for group in self.optimizer.param_groups:
        new_group = {k: v for k, v in group.items() if k != 'params'}
        new_group['params'] = []
        param_groups.append(new_group)
    
    # Recreate optimizer with updated parameters
    self.optimizer = type(self.optimizer)(
        [p for n, p in self.model.named_parameters() if p.requires_grad],
        **{k: v for group in param_groups for k, v in group.items()}
    )
```

### Memory Efficiency Strategies
The trainer implements several memory-saving techniques:
- **Compressed activation storage**: Summing activations across batch and sequence dimensions rather than storing full tensors
- **Gradient checkpointing**: Potentially enabled for large models like LLaMA
- **Mixed precision training**: Using FP16 as specified in hardware configuration
- **Efficient salience computation**: Computing scores incrementally rather than storing full historical data

### Synchronization Points
The trainer coordinates timing between different adaptive processes:
- **Pruning frequency**: Controlled by `pruning.update_frequency` (default 100 steps)
- **Tuning frequency**: Independent adjustment schedule
- **Sparsity progression**: Smooth cubic scheduling ensures gradual structural changes
- **Distillation ramp-up**: μ coefficient increases linearly from 0 to 1 over training

### Error Handling & Stability
Key stability measures include:
- Gradual mask updating (α=0.01) instead of abrupt zeroing
- Output-preserving initialization when expanding adapter ranks
- Gradient clipping to prevent explosion
- Proper handling of shape changes through optimizer reset
- Validation checks after major structural modifications

This comprehensive orchestration enables APT to simultaneously prune unimportant parameters while enhancing important ones, achieving both training and inference efficiency improvements as demonstrated in the paper's results.