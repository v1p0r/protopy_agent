# Logic Analysis: model/base_model.py

## Core Responsibilities
The `base_model.py` module serves as the foundational component for loading and configuring pretrained language models (RoBERTa, T5, LLaMA) with APT adapter integration capabilities. Its primary responsibilities are:

1. **Model Loading**: Load pretrained transformer models from Hugging Face Transformers library
2. **Parameter Freezing**: Freeze all original model parameters to implement parameter-efficient fine-tuning
3. **Adapter Integration**: Provide interfaces to inject APT adapters into specified layers
4. **Configuration Management**: Apply settings from config.yaml regarding which components receive adapters

## Key Design Decisions Based on Paper and Configuration

### 1. Model Architecture Support
Based on Section 5.1 of the paper and the config.yaml file, the module must support three distinct architectures:
- RoBERTa-base (encoder-only)
- T5-base (encoder-decoder)
- LLaMA-2 7B/13B (decoder-only)

Each architecture has different layer structures that affect how APT adapters are applied.

### 2. Adapter Placement Strategy
According to Section 4.1 of the paper:
- APT adapters should be added to queries and values in Multi-Head Attention (MHA) layers
- For smaller models like RoBERTa and T5, adapters should also be added to Feed-Forward Network (FFN) layers
- The config.yaml specifies this through `apply_to_mha: true` and `apply_to_ffn: true`

### 3. Component-Specific Application
The configuration file explicitly defines which components within each layer type should receive adapters:
```yaml
mha_components: [q_proj, v_proj]
ffn_components: [intermediate_dense, output_dense]
```
This means only query and value projections in attention layers get adapters, not key or output projections.

## Implementation Logic Flow

### Step 1: Model Initialization
1. Accept a model name identifier (e.g., "roberta-base")
2. Use Hugging Face's `AutoModel.from_pretrained()` to load the base model
3. Freeze all parameters by setting `requires_grad=False`
4. Store the model architecture type for later conditional logic

### Step 2: Architecture Detection and Processing
Implement detection logic to identify whether the loaded model is:
- Encoder-only (RoBERTa)
- Encoder-decoder (T5) 
- Decoder-only (LLaMA)

This affects how we traverse the model hierarchy to find target layers.

### Step 3: Layer Traversal and Adapter Injection
For each supported architecture, traverse the appropriate module hierarchy:

**For RoBERTa/T5 encoder layers:**
- Navigate to attention submodules
- Identify q_proj and v_proj layers based on naming conventions
- Inject APT adapters after these projection layers
- Similarly process FFN intermediate and output dense layers

**For T5 decoder layers:**
- Same pattern as encoder but also include cross-attention layers
- As noted in Appendix C, cross-attention in decoder should be counted

**For LLaMA layers:**
- Process self-attention q_proj and v_proj
- Process MLP up_proj, gate_proj, down_proj (gated FFN structure)
- Note: The paper mentions FFN adapters only for "smaller models" so may need conditional logic

### Step 4: Adapter Integration Mechanism
When injecting an APT adapter:
1. Create an instance of APTAdapter with parameters from config:
   - initial_rank: 8 (from config.training.initial_rank)
   - scaling_factor: 2.0 (from config.training.scaling_factor)
2. Wrap the target layer with the adapter using residual connection pattern
3. Ensure gradients flow only through the adapter parameters
4. Maintain proper tensor shape compatibility between layers

### Step 5: State Management
Provide methods to:
- Access all adapter parameters for optimizer setup
- Retrieve current model sparsity statistics
- Support merging adapter weights into base model for inference
- Handle optimizer reset when adapter shapes change during training

## Dependencies and Interactions

### From Configuration File
- `model.architectures`: List of supported models
- `model.adapter.apply_to_*`: Boolean flags for where to apply adapters  
- `model.adapter.mha_components`: Specific MHA components to modify
- `training.initial_rank`: Initial rank for adapter matrices
- `training.scaling_factor`: Scaling factor for LoRA updates

### To Other Modules
- Provides modified model to `train/trainer.py`
- Supplies adapter references to `pruning/adaptive_pruner.py` for mask application
- Exposes adapter parameters to `tuning/adaptive_tuner.py` for rank adjustment
- Shares frozen parameter access with `distillation/self_distiller.py`

## Special Considerations

### Gated FFNs in T5/LLaMA
As mentioned in Appendix C, T5 and LLaMA use gated feed-forward networks with three linear layers instead of two. The implementation must handle this difference while maintaining consistent adapter application logic.

### Cross-Layer Consistency
Ensure that adapter injection follows the same principles across all architectures despite their structural differences. This includes consistent naming, positioning, and initialization schemes.

### Memory Efficiency
Since the goal is efficiency, avoid creating unnecessary copies of model weights. Use view operations and shared storage where possible, especially important for large LLaMA models.

### Dynamic Shape Handling
Design the interface to accommodate dynamic changes to adapter ranks during training, ensuring downstream modules can detect and respond to these changes appropriately.